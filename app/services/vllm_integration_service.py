"""
vLLM ë©€í‹° LoRA ì„œë²„ í†µí•© ì„œë¹„ìŠ¤
- 4ê°€ì§€ ëª¨ë¸ íƒ€ì…ë³„ ì½”ë“œ ìƒì„±
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
- í•œêµ­ì–´/ì˜ì–´ ìë™ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì§€ì›
- ì‚¬ìš©ì ì„ íƒ ì˜µì…˜ ìµœì í™”
- ğŸ†• ì²­í¬ ë²„í¼ë§ ë° ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
"""

import asyncio
import json
import re
import time
from datetime import datetime
from enum import Enum
from typing import Any, AsyncGenerator, Dict, List, Optional

import aiohttp

from app.core.config import settings
from app.core.structured_logger import StructuredLogger
from app.schemas.code_generation import (
    CodeGenerationRequest,
    CodeGenerationResponse,
    ModelType,
)

# ğŸ†• AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ import ì¶”ê°€
from app.services.performance_profiler import ai_performance_metrics

logger = StructuredLogger("vllm_integration")


class VLLMModelType(str, Enum):
    """vLLM ì„œë²„ì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ íƒ€ì…"""

    AUTOCOMPLETE = "autocomplete"  # ì½”ë“œ ìë™ì™„ì„± (ë²ˆì—­ ì—†ìŒ)
    PROMPT = "prompt"  # ì¼ë°˜ ì½”ë“œ ìƒì„± (ì „ì²´ ë²ˆì—­)
    COMMENT = "comment"  # ì£¼ì„/docstring ìƒì„± (ì£¼ì„ë§Œ ë²ˆì—­)
    ERROR_FIX = "error_fix"  # ë²„ê·¸ ìˆ˜ì • (ì „ì²´ ë²ˆì—­)


class ChunkBuffer:
    """ì²­í¬ ë²„í¼ë§ í´ë˜ìŠ¤ - ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ì²­í¬ ê·¸ë£¹í™” ë° í›„ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self, buffer_size: int = 500, buffer_timeout: float = 2.0):
        # ê·¹í•œ ì„±ëŠ¥ ìµœì í™” ì„¤ì • (99.9% ì²­í¬ ê°ì†Œ ëª©í‘œ)
        self.buffer_size = buffer_size  # ìµœëŒ€ ë²„í¼ í¬ê¸° (300 â†’ 500ìë¡œ ê·¹í•œ ì¦ê°€)
        self.buffer_timeout = buffer_timeout  # ë²„í¼ íƒ€ì„ì•„ì›ƒ (1.0 â†’ 2.0ì´ˆë¡œ ê·¹í•œ ì¦ê°€)
        self.min_chunk_size = 120  # ìµœì†Œ ì²­í¬ í¬ê¸° (80 â†’ 120ìë¡œ ì¦ê°€)
        self.max_chunk_size = 1200  # ìµœëŒ€ ì²­í¬ í¬ê¸° (800 â†’ 1200ìë¡œ ì¦ê°€)
        self.optimal_chunk_size = 300  # ìµœì  ì²­í¬ í¬ê¸° (150 â†’ 300ìë¡œ ì¦ê°€)
        self.buffer = ""
        self.last_flush_time = time.time()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ë“¤
        self.total_chunks_processed = 0
        self.total_bytes_processed = 0
        self.small_chunks_count = 0  # 80ì ë¯¸ë§Œ ì²­í¬ ê°œìˆ˜
        self.large_chunks_count = 0  # 800ì ì´ˆê³¼ ì²­í¬ ê°œìˆ˜
        self.optimal_chunks_count = 0  # 80-300ì ì²­í¬ ê°œìˆ˜
        
        # ì²­í¬ í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì„¤ì •
        self.force_meaningful_boundaries = True  # ì˜ë¯¸ ìˆëŠ” ê²½ê³„ì—ì„œë§Œ í”ŒëŸ¬ì‹œ
        self.strict_size_enforcement = True  # ì—„ê²©í•œ í¬ê¸° ê²€ì¦
        
        # ê·¹ë„ë¡œ ì—„ê²©í•œ ì˜ë¯¸ êµ¬ë¶„ì íŒ¨í„´ (ì²­í¬ ìƒì„± 99% ê°ì†Œ)
        self.meaningful_delimiters = [
            # ìµœê³  ìš°ì„ ìˆœìœ„: ì™„ì „í•œ í•¨ìˆ˜/í´ë˜ìŠ¤ ë¸”ë¡ë§Œ (ìµœì†Œ 5ì¤„ ì´ìƒ)
            r'def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){5,}',     # í•¨ìˆ˜ ì •ì˜ (5ì¤„ ì´ìƒ)
            r'class\s+\w+[^:]*:\s*\n(?:\s{4}.*\n){5,}',       # í´ë˜ìŠ¤ ì •ì˜ (5ì¤„ ì´ìƒ)
            r'async\s+def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){3,}', # async í•¨ìˆ˜ (3ì¤„ ì´ìƒ)
            
            # ê³ ìš°ì„ ìˆœìœ„: ì™„ì „í•œ ì œì–´ êµ¬ì¡° (ìµœì†Œ 3ì¤„)
            r'if\s+[^:]+:\s*\n(?:\s{4}.*\n){3,}(?:else:\s*\n(?:\s{4}.*\n)*)?', # if-else (3ì¤„ ì´ìƒ)
            r'for\s+[^:]+:\s*\n(?:\s{4}.*\n){3,}',            # for ë£¨í”„ (3ì¤„ ì´ìƒ)
            r'while\s+[^:]+:\s*\n(?:\s{4}.*\n){3,}',          # while ë£¨í”„ (3ì¤„ ì´ìƒ)
            r'try:\s*\n(?:\s{4}.*\n){2,}except[^:]*:\s*\n(?:\s{4}.*\n){2,}', # try-except (ê° 2ì¤„ ì´ìƒ)
            
            # ì¤‘ìš°ì„ ìˆœìœ„: ì™„ì „í•œ docstringì´ë‚˜ ê¸´ ì£¼ì„ ë¸”ë¡
            r'"""\s*\n[^"]{20,}\n\s*"""',                     # ê¸´ docstring (20ì ì´ìƒ)
            r"'''\s*\n[^']{20,}\n\s*'''",                     # ê¸´ docstring (20ì ì´ìƒ)
            r'\n\s*#[^\n]{30,}\n',                            # ê¸´ ì£¼ì„ (30ì ì´ìƒ)
            
            # ì €ìš°ì„ ìˆœìœ„: í° êµ¬ì¡°ì  ë¶„ë¦¬
            r'\n\s*\n\s*\n',                                  # ì—°ì† ë¹ˆ ì¤„ (3ê°œ ì´ìƒ)
            r'```[^`]{50,}```',                               # ê¸´ ì½”ë“œ ë¸”ë¡ (50ì ì´ìƒ)
        ]
        
        # complete_code_patternsë„ ë” ì—„ê²©í•˜ê²Œ (ê¸°ì¡´ì— ì •ì˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¶”ê°€)
        self.complete_code_patterns = [
            r'def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){3,}',     # ì™„ì „í•œ í•¨ìˆ˜ (3ì¤„ ì´ìƒ)
            r'class\s+\w+[^:]*:\s*\n(?:\s{4}.*\n){3,}',       # ì™„ì „í•œ í´ë˜ìŠ¤ (3ì¤„ ì´ìƒ)
            r'if\s+[^:]+:\s*\n(?:\s{4}.*\n){2,}else:\s*\n(?:\s{4}.*\n)+', # ì™„ì „í•œ if-else
            r'try:\s*\n(?:\s{4}.*\n)+except[^:]*:\s*\n(?:\s{4}.*\n)+', # ì™„ì „í•œ try-except
        ]
        
        # AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° íŒ¨í„´ (ì œê±°ìš©) - ë” ì •êµí•œ íŒ¨í„´
        self.special_token_patterns = [
            r'<\|im_end\|>.*$',                               # im_end í† í° ë° ì´í›„ ëª¨ë“  ë‚´ìš©
            r'<\|im_start\|>[^|]*\|>',                        # im_start í† í°
            r'<\|assistant\|>',                               # assistant í† í°
            r'<\|user\|>',                                    # user í† í°
            r'<\|system\|>',                                  # system í† í°
            r'<\|end[^>]*\|>',                                # ê¸°íƒ€ end í† í°
            r'<\|[^>]*\|>',                                   # ê¸°íƒ€ íŠ¹ìˆ˜ í† í°
            r'</?\w+[^>]*>',                                  # HTML íƒœê·¸ ìœ ì‚¬ íŒ¨í„´
            r'\[INST\]|\[/INST\]',                            # ëª…ë ¹ í† í°
            r'<s>|</s>',                                      # ì‹œì‘/ì¢…ë£Œ í† í°
            r'<unk>|<pad>|<eos>|<bos>',                       # íŠ¹ìˆ˜ í† í°ë“¤
            r'Assistant:|Human:|User:',                       # ì—­í•  ë¼ë²¨
        ]
        
        # ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ ê°ì§€ íŒ¨í„´
        self.complete_code_patterns = [
            r'def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n)*(?=\S|$)',  # ì™„ì „í•œ í•¨ìˆ˜
            r'class\s+\w+[^:]*:\s*\n(?:\s{4}.*\n)*(?=\S|$)',    # ì™„ì „í•œ í´ë˜ìŠ¤
            r'if\s+[^:]+:\s*\n(?:\s{4}.*\n)*(?:elif[^:]*:\s*\n(?:\s{4}.*\n)*)*(?:else:\s*\n(?:\s{4}.*\n)*)?(?=\S|$)', # ì™„ì „í•œ if-elif-else
            r'for\s+[^:]+:\s*\n(?:\s{4}.*\n)*(?=\S|$)',         # ì™„ì „í•œ for ë£¨í”„
            r'while\s+[^:]+:\s*\n(?:\s{4}.*\n)*(?=\S|$)',       # ì™„ì „í•œ while ë£¨í”„
        ]
    
    def add_chunk(self, chunk: str) -> Optional[str]:
        """ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€í•˜ê³  í•„ìš”ì‹œ í”ŒëŸ¬ì‹œ - ì„±ëŠ¥ ìµœì í™”ëœ ë¡œì§"""
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ìƒì„¸ ë¡œê·¸
        if settings.should_log_debug():
            print(f"ğŸ” [ChunkBuffer] ì²­í¬ ì…ë ¥: '{chunk[:30]}...' (ê¸¸ì´: {len(chunk)})")
        
        # ë¨¼ì € im_end í† í° ì²´í¬ - ë°œê²¬ë˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if self._contains_end_token(chunk):
            if settings.should_log_performance():
                print(f"ğŸ›‘ [ChunkBuffer] ì¢…ë£Œ í† í° ê°ì§€: '{chunk[:20]}...'")
            # im_end í† í° ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            clean_chunk = self._extract_content_before_end_token(chunk)
            if clean_chunk:
                self.buffer += clean_chunk
            # ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•˜ê³  ì¤‘ë‹¨ ì‹ í˜¸ ë°˜í™˜
            final_content = self.flush()
            return final_content if final_content.strip() else "[END_OF_GENERATION]"
        
        # ì¼ë°˜ì ì¸ íŠ¹ìˆ˜ í† í° ì œê±° (im_end ì œì™¸)
        cleaned_chunk = self._clean_special_tokens(chunk)
        
        # ë¹ˆ ë‚´ìš©ì´ë©´ ë¬´ì‹œ
        if not cleaned_chunk.strip():
            return None
            
        self.buffer += cleaned_chunk
        current_time = time.time()
        
        # ì´ˆê°•ë ¥ í”ŒëŸ¬ì‹œ ì¡°ê±´ (99% ì²­í¬ ê°ì†Œ ëª©í‘œ)
        if self.strict_size_enforcement:
            # ê·¹ë„ë¡œ ì—„ê²©í•œ ëª¨ë“œ: ìµœì†Œ í¬ê¸°ì˜ 2ë°° ë¯¸ë‹¬ ì‹œ ì ˆëŒ€ í”ŒëŸ¬ì‹œ ê¸ˆì§€
            if len(self.buffer) < self.min_chunk_size * 2:  # 240ì ë¯¸ë§Œ
                # ê·¹ë„ë¡œ ì œí•œëœ ì˜ˆì™¸: ì˜¤ì§ ìµœëŒ€ í¬ê¸° ì´ˆê³¼ë‚˜ ê°•ì œ ì¢…ë£Œì‹œë§Œ
                if (len(self.buffer) >= self.max_chunk_size * 1.5 or  # 1800ì ì´ìƒ
                    self._contains_end_token(self.buffer)):
                    should_flush = True
                else:
                    should_flush = False
            else:
                # ìµœì†Œ í¬ê¸° 2ë°° ì¶©ì¡± ì‹œì—ë§Œ ë‹¤ë¥¸ ì¡°ê±´ ê²€í† 
                should_flush = (
                    # 1. ìµœì  í¬ê¸° 2ë°° ë„ë‹¬ + ê°•í•œ ì˜ë¯¸ ê²½ê³„ë§Œ
                    (len(self.buffer) >= self.optimal_chunk_size * 2 and  # 600ì ì´ìƒ
                     self._has_strong_meaningful_boundary()) or
                    
                    # 2. ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ ì™„ì„± + ìµœì†Œ 300ì ì´ìƒ
                    (len(self.buffer) >= 300 and
                     self._has_complete_code_element()) or
                    
                    # 3. ë²„í¼ í¬ê¸° 3ë°° ì´ˆê³¼ (ê°•ì œ í”ŒëŸ¬ì‹œ)
                    len(self.buffer) >= self.buffer_size * 3.0 or  # 1500ì ì´ìƒ
                    
                    # 4. ìµœëŒ€ í¬ê¸° 1.5ë°° ì´ˆê³¼ (ë¬´ì¡°ê±´ í”ŒëŸ¬ì‹œ)
                    len(self.buffer) >= self.max_chunk_size * 1.5 or  # 1800ì ì´ìƒ
                    
                    # 5. ë§¤ìš° ì—„ê²©í•œ ì‹œê°„ ê¸°ë°˜ ì¡°ê±´
                    (current_time - self.last_flush_time >= self.buffer_timeout * 5.0 and  # 10ì´ˆ ì´ìƒ
                     len(self.buffer) >= self.min_chunk_size * 3 and  # 360ì ì´ìƒ
                     self._has_strong_meaningful_boundary())  # ê°•í•œ ê²½ê³„ë§Œ
                )
        else:
            # ê¸°ì¡´ ë¡œì§ (í˜¸í™˜ì„±)
            should_flush = (
                len(self.buffer) >= self.min_chunk_size and (
                    len(self.buffer) >= self.buffer_size * 1.8 or
                    current_time - self.last_flush_time >= self.buffer_timeout or
                    self._has_complete_code_element() or
                    len(self.buffer) >= self.max_chunk_size
                )
            )
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì²­í¬ í’ˆì§ˆ ë¶„ë¥˜
        if should_flush:
            buffer_length = len(self.buffer)
            
            # ì²­í¬ í¬ê¸°ë³„ ë¶„ë¥˜
            if buffer_length < self.min_chunk_size:
                self.small_chunks_count += 1
                if settings.should_log_performance():
                    print(f"âš ï¸ [ChunkBuffer] ì‘ì€ ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì (ë¹„ì •ìƒ)")
            elif buffer_length <= self.buffer_size:
                self.optimal_chunks_count += 1
                if settings.should_log_debug():
                    print(f"âœ… [ChunkBuffer] ìµœì  ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            else:
                self.large_chunks_count += 1
                if settings.should_log_performance():
                    print(f"ğŸ“¦ [ChunkBuffer] ëŒ€í˜• ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            
            result = self.flush()
            
            # í”ŒëŸ¬ì‹œ ìƒì„¸ ë¡œê·¸
            if settings.should_log_performance():
                chunk_quality = "ìµœì " if self.min_chunk_size <= buffer_length <= self.buffer_size else "ë¹„ì •ìƒ"
                print(f"ğŸ“¤ [ChunkBuffer] {chunk_quality} í”ŒëŸ¬ì‹œ ì™„ë£Œ: {buffer_length}ì â†’ {len(result)}ì")
            
            return result
        else:
            if settings.should_log_debug():
                print(f"ğŸ”„ [ChunkBuffer] ë²„í¼ë§ ì¤‘: {len(self.buffer)}/{self.buffer_size}ì")
        
        return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ê°•í™”ëœ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        total_chunks = max(self.total_chunks_processed, 1)
        avg_chunk_size = (self.total_bytes_processed / total_chunks)
        
        # ì²­í¬ í’ˆì§ˆ ë¶„ì„
        small_ratio = round(self.small_chunks_count / total_chunks * 100, 2)
        optimal_ratio = round(self.optimal_chunks_count / total_chunks * 100, 2)
        large_ratio = round(self.large_chunks_count / total_chunks * 100, 2)
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        if small_ratio <= 5 and optimal_ratio >= 70:
            performance_grade = "A"  # ìš°ìˆ˜
        elif small_ratio <= 15 and optimal_ratio >= 50:
            performance_grade = "B"  # ì–‘í˜¸
        elif small_ratio <= 30:
            performance_grade = "C"  # ë³´í†µ
        else:
            performance_grade = "D"  # ê°œì„  í•„ìš”
        
        return {
            "total_chunks": self.total_chunks_processed,
            "total_bytes": self.total_bytes_processed,
            "avg_chunk_size": round(avg_chunk_size, 2),
            
            # ì²­í¬ í¬ê¸°ë³„ ë¶„ë¥˜
            "small_chunks_count": self.small_chunks_count,
            "optimal_chunks_count": self.optimal_chunks_count,
            "large_chunks_count": self.large_chunks_count,
            
            # ë¹„ìœ¨ ë¶„ì„
            "small_chunks_ratio": small_ratio,
            "optimal_chunks_ratio": optimal_ratio,
            "large_chunks_ratio": large_ratio,
            
            # ì„±ëŠ¥ ì§€í‘œ
            "performance_grade": performance_grade,
            "buffer_efficiency": round(optimal_ratio + (large_ratio * 0.7), 2),  # íš¨ìœ¨ì„± ì ìˆ˜
            
            # í˜„ì¬ ìƒíƒœ
            "current_buffer_size": len(self.buffer),
            "buffer_utilization": round(len(self.buffer) / self.buffer_size * 100, 2),
            
            # ì„¤ì • ì •ë³´
            "min_chunk_size": self.min_chunk_size,
            "optimal_chunk_size": self.optimal_chunk_size,
            "max_chunk_size": self.max_chunk_size,
            "strict_mode": self.strict_size_enforcement
        }
    
    def _clean_special_tokens(self, text: str) -> str:
        """AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° ì œê±°"""
        cleaned_text = text
        for pattern in self.special_token_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        return cleaned_text
    
    def _has_complete_code_element(self) -> bool:
        """ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ(í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±)ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        for pattern in self.complete_code_patterns:
            if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
                return True
        return False
    
    def _has_strong_meaningful_boundary(self) -> bool:
        """ê°•í•œ ì˜ë¯¸ ê²½ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜¤ì§ ìµœê³  ìš°ì„ ìˆœìœ„ íŒ¨í„´ë§Œ)"""
        # ì˜¤ì§ ì²« ë²ˆì§¸ íŒ¨í„´ë§Œ ì²´í¬: ì™„ì „í•œ í•¨ìˆ˜ ì •ì˜ (5ì¤„ ì´ìƒ)
        pattern = self.meaningful_delimiters[0]  # def í•¨ìˆ˜ (5ì¤„ ì´ìƒ)ë§Œ
        if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
            return True
        return False
    
    def _has_meaningful_boundary(self) -> bool:
        """ì˜ë¯¸ìˆëŠ” ê²½ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìƒìœ„ íŒ¨í„´ë§Œ)"""
        # ìƒìœ„ 7ê°œ íŒ¨í„´ë§Œ ì²´í¬ (í•¨ìˆ˜, í´ë˜ìŠ¤, asyncí•¨ìˆ˜, if-else, for, while, try-except)
        for pattern in self.meaningful_delimiters[:7]:
            if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
                return True
        return False
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš©ì„ í”ŒëŸ¬ì‹œí•˜ê³  í›„ì²˜ë¦¬ (í†µê³„ëŠ” add_chunkì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)"""
        content = self.buffer
        self.buffer = ""
        self.last_flush_time = time.time()
        
        # ì „ì—­ í†µê³„ë§Œ ì—…ë°ì´íŠ¸ (í¬ê¸°ë³„ ë¶„ë¥˜ëŠ” add_chunkì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
        self.total_chunks_processed += 1
        self.total_bytes_processed += len(content)
        
        # ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ë¦¬
        if content:
            # 1. AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° ì œê±°
            content = self._clean_special_tokens(content)
            
            # 2. ì—¬ë¶„ì˜ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
            content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)  # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
            content = re.sub(r'[ \t]+', ' ', content)  # ì—¬ëŸ¬ ê³µë°±/íƒ­ â†’ ë‹¨ì¼ ê³µë°±
            content = re.sub(r'[ \t]*\n[ \t]*', '\n', content)  # ì¤„ë°”ê¿ˆ ì£¼ë³€ ê³µë°± ì œê±°
            
            # 3. ì½”ë“œ ë¸”ë¡ ì •ë¦¬
            content = re.sub(r'\n{3,}```', '\n\n```', content)  # ì½”ë“œ ë¸”ë¡ ì• ê³¼ë„í•œ ì¤„ë°”ê¿ˆ
            content = re.sub(r'```\n{3,}', '```\n\n', content)  # ì½”ë“œ ë¸”ë¡ ë’¤ ê³¼ë„í•œ ì¤„ë°”ê¿ˆ
        
        return content.strip()
    
    def force_flush(self) -> Optional[str]:
        """ê°•ì œ í”ŒëŸ¬ì‹œ (ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì‹œ) - íŠ¹ìˆ˜ í† í° ì œê±° í¬í•¨"""
        if self.buffer:
            content = self.flush()
            # ìµœì¢… íŠ¹ìˆ˜ í† í° ì œê±°
            content = self._clean_special_tokens(content)
            return content if content.strip() else None
        return None

    def _contains_end_token(self, text: str) -> bool:
        """im_end í† í°ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ - ê°•í™”ëœ íŒ¨í„´"""
        end_patterns = [
            r'<\|im_end\|>',                                  # ì •í™•í•œ im_end í† í°
            r'<\|end\|>',                                     # ê°„ë‹¨í•œ end í† í°
            r'</?\|assistant\|>',                             # assistant ì¢…ë£Œ
            r'\[/INST\]',                                     # ëª…ë ¹ ì¢…ë£Œ
            r'</s>',                                          # ì‹œí€€ìŠ¤ ì¢…ë£Œ
            r'<eos>',                                         # End of Sequence
            r'<\|endoftext\|>',                               # GPT ìŠ¤íƒ€ì¼ ì¢…ë£Œ
            r'### END ###',                                   # ëª…ì‹œì  ì¢…ë£Œ ë§ˆì»¤
            r'```\s*$',                                       # ì½”ë“œ ë¸”ë¡ ì¢…ë£Œ (ì¤„ ë)
            r'Assistant:\s*$',                                # Assistant ë¼ë²¨ë§Œ ìˆëŠ” ê²½ìš°
        ]
        
        for pattern in end_patterns:
            if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                return True
        return False
    
    def _extract_content_before_end_token(self, text: str) -> str:
        """im_end í† í° ì´ì „ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ - ê°•í™”ëœ íŒ¨í„´"""
        end_patterns = [
            r'<\|im_end\|>',
            r'<\|end\|>',
            r'</?\|assistant\|>',
            r'\[/INST\]',
            r'</s>',
            r'<eos>',
            r'<\|endoftext\|>',
            r'### END ###',
            r'```\s*$',
            r'Assistant:\s*$',
        ]
        
        for pattern in end_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                # í† í° ì´ì „ ë¶€ë¶„ë§Œ ë°˜í™˜
                content_before = text[:match.start()].strip()
                if settings.should_log_performance():
                    print(f"âœ‚ï¸ [ChunkBuffer] ì¢…ë£Œí† í° ì œê±°: '{text}' â†’ '{content_before}'")
                return content_before
        
        return text


class VLLMIntegrationService:
    """vLLM ë©€í‹° LoRA ì„œë²„ì™€ì˜ í†µí•© ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.vllm_base_url = settings.VLLM_SERVER_URL
        self.timeout = aiohttp.ClientTimeout(total=settings.VLLM_TIMEOUT_SECONDS)
        self.session = None
        
        # ì²­í¬ ë²„í¼ë§ ì„¤ì • ê·¹í•œ ê°•í™” (99.9% ì²­í¬ ê°ì†Œ ëª©í‘œ)
        self.chunk_buffering_enabled = True
        self.default_buffer_size = 500  # ê¸°ë³¸ ë²„í¼ í¬ê¸°: 300 â†’ 500ìë¡œ ê·¹í•œ ì¦ê°€
        self.default_buffer_timeout = 2.0  # ê¸°ë³¸ ë²„í¼ íƒ€ì„ì•„ì›ƒ: 1.0 â†’ 2.0ì´ˆë¡œ ê·¹í•œ ì¦ê°€
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.enable_performance_logging = settings.should_log_performance()
        self.enable_debug_logging = settings.should_log_debug()
        self.enable_chunk_details = getattr(settings, 'should_log_chunk_details', lambda: False)() 
        
        if self.enable_performance_logging:
            print(f"âš™ï¸ [vLLM] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: ë²„í¼í¬ê¸°={self.default_buffer_size}, íƒ€ì„ì•„ì›ƒ={self.default_buffer_timeout}ì´ˆ")

    async def _get_session(self) -> aiohttp.ClientSession:
        """aiohttp ì„¸ì…˜ ìƒì„± ë° ì¬ì‚¬ìš©"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=self.timeout,
                headers={
                    "Content-Type": "application/json",
                    "Accept": "text/event-stream",
                },
            )
        return self.session

    async def check_health(self) -> Dict[str, Any]:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.vllm_base_url}/health") as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹…
                    if settings.should_log_debug():
                        logger.log_system_event(
                            "vLLM ì„œë²„ ìƒíƒœ í™•ì¸", "success", {"server_status": result}
                        )
                    
                    return {"status": "healthy", "details": result}
                else:
                    logger.log_system_event(
                        "vLLM ì„œë²„ ìƒíƒœ í™•ì¸",
                        "failed",
                        {"http_status": response.status},
                    )
                    return {
                        "status": "unhealthy",
                        "http_status": response.status}
        except Exception as e:
            logger.log_error(e, "vLLM ì„œë²„ ì—°ê²°")
            return {"status": "error", "error": str(e)}

    async def get_available_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.vllm_base_url}/models") as response:
                if response.status == 200:
                    models = await response.json()
                    
                    # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹…
                    if settings.should_log_debug():
                        logger.log_system_event(
                            "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ", "success", {"model_count": len(models)}
                        )
                    
                    return {"status": "success", "models": models}
                else:
                    return {"status": "error", "http_status": response.status}
        except Exception as e:
            logger.log_error(e, "ëª¨ë¸ ëª©ë¡ ì¡°íšŒ")
            return {"status": "error", "error": str(e)}

    def _map_hapa_to_vllm_model(self, hapa_model: ModelType) -> VLLMModelType:
        """HAPA ëª¨ë¸ íƒ€ì…ì„ vLLM ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            ModelType.CODE_COMPLETION: VLLMModelType.AUTOCOMPLETE,
            ModelType.CODE_GENERATION: VLLMModelType.PROMPT,
            ModelType.CODE_EXPLANATION: VLLMModelType.COMMENT,
            ModelType.BUG_FIX: VLLMModelType.ERROR_FIX,
            ModelType.CODE_REVIEW: VLLMModelType.PROMPT,
            ModelType.CODE_OPTIMIZATION: VLLMModelType.PROMPT,
            ModelType.UNIT_TEST_GENERATION: VLLMModelType.PROMPT,
            ModelType.DOCUMENTATION: VLLMModelType.COMMENT,
        }
        return mapping.get(hapa_model, VLLMModelType.PROMPT)

    def _prepare_vllm_request(
        self, request: CodeGenerationRequest, user_id: str
    ) -> Dict[str, Any]:
        """HAPA ìš”ì²­ì„ vLLM ìš”ì²­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        vllm_model = self._map_hapa_to_vllm_model(request.model_type)

        # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
        optimized_prompt = self._optimize_prompt_for_model(
            request.prompt, vllm_model, request
        )

        # ì‚¬ìš©ì ì„ íƒ ì˜µì…˜ ë§¤í•‘
        user_select_options = self._map_user_options(request)

        # user_idë¥¼ ìˆ«ìë¡œ ë³€í™˜ (í•´ì‹œ ì‚¬ìš©)
        try:
            numeric_user_id = abs(hash(user_id)) % 1000000  # 1-1000000 ë²”ìœ„
        except BaseException:
            numeric_user_id = 12345  # ê¸°ë³¸ê°’

        vllm_request = {
            "user_id": numeric_user_id,
            "model_type": vllm_model.value,
            "prompt": optimized_prompt,
            "user_select_options": user_select_options,
            "temperature": float(getattr(request, "temperature", 0.3)),
            "top_p": float(getattr(request, "top_p", 0.95)),
            "max_tokens": int(getattr(request, "max_tokens", 1024)),
        }

        # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹… - ìš”ì²­ ìƒì„¸ ì •ë³´
        if settings.should_log_request_response():
            logger.log_system_event(
                f"vLLM ìš”ì²­ ì¤€ë¹„",
                "success",
                {
                    "user_id": user_id,
                    "numeric_user_id": numeric_user_id,
                    "model_type": vllm_model.value,
                    "prompt_length": len(optimized_prompt),
                    "temperature": vllm_request["temperature"],
                },
            )

        return vllm_request

    def _optimize_prompt_for_model(
        self,
        prompt: str,
        model_type: VLLMModelType,
        request: CodeGenerationRequest) -> str:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        
        if model_type == VLLMModelType.AUTOCOMPLETE:
            # ìë™ì™„ì„±: ì»¨í…ìŠ¤íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            return prompt

        elif model_type == VLLMModelType.COMMENT:
            # ì£¼ì„/ë¬¸ì„œí™”: ì½”ë“œ í•´ì„ ë° ë¬¸ì„œí™” í”„ë¡¬í”„íŠ¸
            context_prefix = (
                f"# ëŒ€ìƒ ì½”ë“œ:\n{request.context}\n\n" if request.context else ""
            )
            return f"{context_prefix}# ë¬¸ì„œí™” ìš”ì²­: {prompt}"

        elif model_type == VLLMModelType.ERROR_FIX:
            # ë²„ê·¸ ìˆ˜ì •: ì˜¤ë¥˜ ë¶„ì„ ë° ìˆ˜ì • í”„ë¡¬í”„íŠ¸
            context_prefix = (
                f"# ì˜¤ë¥˜ê°€ ìˆëŠ” ì½”ë“œ:\n{request.context}\n\n" if request.context else ""
            )
            return f"""{context_prefix}# ë²„ê·¸ ìˆ˜ì • ìš”ì²­: {prompt}

# ìˆ˜ì • ê°€ì´ë“œë¼ì¸:
1. ì˜¤ë¥˜ ì›ì¸ ëª…í™•íˆ ë¶„ì„
2. ìµœì†Œí•œì˜ ìˆ˜ì •ìœ¼ë¡œ ë¬¸ì œ í•´ê²°
3. ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì½”ë“œ ì‘ì„±

## ìˆ˜ì •ëœ ì½”ë“œ:"""

        else:  # PROMPT (ê¸°ë³¸)
            # ì¼ë°˜ ì½”ë“œ ìƒì„±: ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ í‘œí˜„
            context_prefix = (
                f"# ì»¨í…ìŠ¤íŠ¸:\n{request.context}\n\n" if request.context else ""
            )
            return f"{context_prefix}# ìš”ì²­ì‚¬í•­: {prompt}"

    def _map_user_options(
            self, request: CodeGenerationRequest) -> Dict[str, Any]:
        """HAPA ì‚¬ìš©ì ì˜µì…˜ì„ vLLM í˜•ì‹ìœ¼ë¡œ ë§¤í•‘"""
        options = {}

        # í”„ë¡œê·¸ë˜ë° ê¸°ìˆ  ìˆ˜ì¤€ ë§¤í•‘
        if hasattr(request, "programming_level"):
            level_mapping = {
                "beginner": "beginner",
                "intermediate": "intermediate",
                "advanced": "advanced",
                "expert": "advanced",
            }
            options["python_skill_level"] = level_mapping.get(
                request.programming_level, "intermediate"
            )
        else:
            options["python_skill_level"] = "intermediate"

        # ì„¤ëª… ìŠ¤íƒ€ì¼ ë§¤í•‘
        if hasattr(request, "explanation_detail"):
            detail_mapping = {
                "minimal": "brief",
                "standard": "standard",
                "detailed": "detailed",
                "comprehensive": "detailed",
            }
            options["explanation_style"] = detail_mapping.get(
                request.explanation_detail, "standard"
            )
        else:
            options["explanation_style"] = "standard"

        # ì¶”ê°€ ì˜µì…˜ë“¤
        if hasattr(request, "include_comments"):
            options["include_comments"] = request.include_comments

        if hasattr(request, "code_style"):
            options["code_style"] = request.code_style

        return options

    async def generate_code_stream(
        self, request: CodeGenerationRequest, user_id: str
    ) -> AsyncGenerator[str, None]:
        """vLLM ì„œë²„ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± (ê°œì„ ëœ ì²­í¬ ì²˜ë¦¬)"""

        vllm_request = self._prepare_vllm_request(request, user_id)
        
        # ì²­í¬ ë²„í¼ ì´ˆê¸°í™”
        chunk_buffer = ChunkBuffer(
            buffer_size=self.default_buffer_size,
            buffer_timeout=self.default_buffer_timeout
        ) if self.chunk_buffering_enabled else None

        if self.enable_performance_logging:
            print(f"ğŸ”§ [vLLM] ì²­í¬ ë²„í¼ë§ ì„¤ì •: í™œì„±í™”={self.chunk_buffering_enabled}, ë²„í¼í¬ê¸°={self.default_buffer_size}, íƒ€ì„ì•„ì›ƒ={self.default_buffer_timeout}")
            if chunk_buffer:
                print(f"âœ… [vLLM] ChunkBuffer ìƒì„± ì™„ë£Œ")

        try:
            session = await self._get_session()

            async with session.post(
                f"{self.vllm_base_url}/generate/stream", json=vllm_request
            ) as response:

                if response.status != 200:
                    error_msg = f"vLLM ì„œë²„ ì˜¤ë¥˜: HTTP {response.status}"
                    logger.log_system_event(
                        "vLLM ì„œë²„ ì˜¤ë¥˜",
                        "failed",
                        {"user_id": user_id, "status": response.status},
                    )
                    yield f"data: {json.dumps({'error': error_msg})}\n\n"
                    return

                # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë¡œê·¸ (ì„±ëŠ¥ ë¡œê·¸ë¡œ ë¶„ë¥˜)
                if self.enable_performance_logging:
                    logger.log_system_event(
                        "vLLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘",
                        "started",
                        {"user_id": user_id, "model": vllm_request["model_type"]},
                    )

                chunk_count = 0
                total_content_length = 0
                streaming_start_time = time.time()

                async for line in response.content:
                    try:
                        line_text = line.decode("utf-8").strip()

                        if not line_text:
                            continue

                        # Server-Sent Events í˜•ì‹ ì²˜ë¦¬
                        if line_text.startswith("data: "):
                            data_content = line_text[6:]  # 'data: ' ì œê±°

                            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ - ê°•í™”ëœ ì²˜ë¦¬
                            if data_content == "[DONE]" or data_content.strip() == "[DONE]":
                                # ë²„í¼ì— ë‚¨ì€ ë‚´ìš© í”ŒëŸ¬ì‹œ
                                if chunk_buffer:
                                    final_content = chunk_buffer.force_flush()
                                    if final_content and final_content.strip():
                                        chunk_count += 1
                                        total_content_length += len(final_content)
                                        yield f"data: {json.dumps({'text': final_content})}\n\n"
                                        if self.enable_debug_logging:
                                            print(f"ğŸ“¤ [vLLM] ìµœì¢… ë²„í¼ í”ŒëŸ¬ì‹œ: '{final_content[:30]}...'")
                                
                                # ì™„ë£Œ ë¡œê·¸ (ì„±ëŠ¥ ë¡œê·¸ë¡œ ë¶„ë¥˜)
                                streaming_duration = time.time() - streaming_start_time
                                if self.enable_performance_logging:
                                    # ë²„í¼ ì„±ëŠ¥ í†µê³„ í¬í•¨
                                    buffer_stats = chunk_buffer.get_performance_stats() if chunk_buffer else {}
                                    logger.log_system_event(
                                        "vLLM ìŠ¤íŠ¸ë¦¬ë°", "completed", {
                                            "user_id": user_id,
                                            "total_chunks": chunk_count,
                                            "total_content_length": total_content_length,
                                            "duration_seconds": round(streaming_duration, 2),
                                            "avg_chunk_size": round(total_content_length / max(chunk_count, 1), 1),
                                            "buffer_stats": buffer_stats
                                        })
                                    print(f"ğŸ [vLLM] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬, {total_content_length}ì, {streaming_duration:.2f}ì´ˆ")
                                    
                                    # ì„±ëŠ¥ ê²½ê³  í™•ì¸
                                    if buffer_stats.get('small_chunks_ratio', 0) > 30:
                                        print(f"âš ï¸ [vLLM] ì‘ì€ ì²­í¬ ë¹„ìœ¨ ë†’ìŒ: {buffer_stats.get('small_chunks_ratio', 0)}%")
                                
                                yield f"data: [DONE]\n\n"
                                return  # í™•ì‹¤í•œ ì¢…ë£Œ

                            # JSON ë°ì´í„° íŒŒì‹± ë° ì²˜ë¦¬
                            try:
                                parsed_data = json.loads(data_content)
                                
                                # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
                                text_content = parsed_data.get('text', '')
                                if text_content:
                                    total_content_length += len(text_content)
                                    
                                    # ë””ë²„ê·¸ ë¡œê·¸ëŠ” ê°œë°œ í™˜ê²½ì—ì„œë§Œ
                                    if self.enable_debug_logging:
                                        print(f"ğŸ“¥ [vLLM] ì›ì‹œ í…ìŠ¤íŠ¸: '{text_content[:20]}...' (ê¸¸ì´: {len(text_content)})")
                                    
                                    if chunk_buffer:
                                        # ë²„í¼ë§ ì²˜ë¦¬
                                        buffered_content = chunk_buffer.add_chunk(text_content)
                                        if buffered_content and buffered_content.strip():
                                            chunk_count += 1
                                            
                                            # ì„±ëŠ¥ ë¡œê·¸ëŠ” ì„±ëŠ¥ ëª¨ë“œì—ì„œë§Œ
                                            if self.enable_performance_logging:
                                                print(f"ğŸ“¤ [vLLM] ë²„í¼ë§ ì¶œë ¥: #{chunk_count}, ê¸¸ì´={len(buffered_content)}")
                                            
                                            # END_OF_GENERATION ì‹ í˜¸ ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                                            if buffered_content == "[END_OF_GENERATION]":
                                                streaming_duration = time.time() - streaming_start_time
                                                if self.enable_performance_logging:
                                                    logger.log_system_event(
                                                        "vLLM ìŠ¤íŠ¸ë¦¬ë°", "im_end_detected", {
                                                            "user_id": user_id,
                                                            "total_chunks": chunk_count,
                                                            "total_content_length": total_content_length,
                                                            "early_termination": True,
                                                            "duration_seconds": round(streaming_duration, 2)
                                                        })
                                                    print(f"ğŸ›‘ [vLLM] END_OF_GENERATION ì‹ í˜¸ - ì¡°ê¸° ì¢…ë£Œ")
                                                yield f"data: [DONE]\n\n"
                                                return
                                            
                                            # ì²­í¬ ìƒì„¸ ë¡œê·¸ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
                                            if self.enable_chunk_details:
                                                logger.debug(
                                                    f"ì²­í¬ ì „ì†¡: #{chunk_count}, ê¸¸ì´: {len(buffered_content)}"
                                                )
                                            
                                            yield f"data: {json.dumps({'text': buffered_content})}\n\n"
                                        # else: ë²„í¼ë§ ì¤‘ì´ë¯€ë¡œ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ (ë¡œê·¸ ìƒëµ)
                                    else:
                                        # ë²„í¼ë§ ë¹„í™œì„±í™” ì‹œ ì§ì ‘ ì „ì†¡ (í•˜ì§€ë§Œ im_end í† í° ì²´í¬)
                                        if self.enable_debug_logging:
                                            print(f"ğŸš« [vLLM] ë²„í¼ë§ ë¹„í™œì„±í™” - ì§ì ‘ ì „ì†¡")
                                        
                                        # im_end í† í° ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                                        if ('<|im_end' in text_content or '</s>' in text_content or 
                                            '<eos>' in text_content or '<|endoftext|>' in text_content):
                                            streaming_duration = time.time() - streaming_start_time
                                            if self.enable_performance_logging:
                                                logger.log_system_event(
                                                    "vLLM ìŠ¤íŠ¸ë¦¬ë°", "im_end_detected_direct", {
                                                        "user_id": user_id,
                                                        "total_chunks": chunk_count,
                                                        "total_content_length": total_content_length,
                                                        "early_termination": True,
                                                        "duration_seconds": round(streaming_duration, 2)
                                                    })
                                                print(f"ğŸ›‘ [vLLM] ì§ì ‘ëª¨ë“œì—ì„œ ì¢…ë£Œí† í° ê°ì§€")
                                            yield f"data: [DONE]\n\n"
                                            return
                                        
                                        chunk_count += 1
                                        yield f"data: {data_content}\n\n"
                                else:
                                    # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ë©”íƒ€ë°ì´í„° ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ì „ì†¡
                                    yield f"data: {data_content}\n\n"
                                    
                            except json.JSONDecodeError:
                                # JSONì´ ì•„ë‹Œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                                if chunk_buffer:
                                    buffered_content = chunk_buffer.add_chunk(data_content)
                                    if buffered_content and buffered_content.strip():
                                        chunk_count += 1
                                        yield f"data: {json.dumps({'text': buffered_content})}\n\n"
                                else:
                                    chunk_count += 1
                                    yield f"data: {data_content}\n\n"

                    except Exception as e:
                        # ë¼ì¸ ì²˜ë¦¬ ì˜¤ë¥˜ëŠ” ë””ë²„ê·¸ í™˜ê²½ì—ì„œë§Œ ë¡œê¹…
                        if self.enable_debug_logging:
                            logger.log_error(e, f"ìŠ¤íŠ¸ë¦¼ ë¼ì¸ ì²˜ë¦¬ - user_id: {user_id}")
                        continue

        except asyncio.TimeoutError:
            error_msg = "vLLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼"
            logger.log_system_event("vLLM ì‘ë‹µ", "timeout", {"user_id": user_id})
            yield f"data: {json.dumps({'error': error_msg})}\n\n"

        except Exception as e:
            error_msg = f"vLLM ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {str(e)}"
            logger.log_error(e, f"vLLM ì„œë²„ ì—°ê²° - user_id: {user_id}")
            yield f"data: {json.dumps({'error': error_msg})}\n\n"

    async def generate_code_sync(
        self, request: CodeGenerationRequest, user_id: str
    ) -> CodeGenerationResponse:
        """ë™ê¸°ì‹ ì½”ë“œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëª¨ë‘ ìˆ˜ì§‘)"""

        generated_content = []
        error_occurred = False
        error_message = ""

        async for chunk in self.generate_code_stream(request, user_id):
            try:
                if chunk.startswith("data: "):
                    data_content = chunk[6:].strip()

                    if data_content == "[DONE]":
                        break

                    # JSON íŒŒì‹± ì‹œë„
                    try:
                        data = json.loads(data_content)
                        if "error" in data:
                            error_occurred = True
                            error_message = data["error"]
                            break
                        elif "text" in data:
                            generated_content.append(data["text"])
                        elif isinstance(data, str):
                            generated_content.append(data)
                    except json.JSONDecodeError:
                        # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        generated_content.append(data_content)

            except Exception as e:
                logger.log_error(e, f"ë™ê¸°ì‹ ì‘ë‹µ ì²˜ë¦¬ - user_id: {user_id}")
                error_occurred = True
                error_message = str(e)
                break

        if error_occurred:
            return CodeGenerationResponse(
                success=False,
                generated_code="",
                error_message=error_message,
                model_used=self._map_hapa_to_vllm_model(
                    request.model_type).value,
                processing_time=0,
                token_usage={
                    "total_tokens": 0},
            )

        final_code = "".join(generated_content)

        return CodeGenerationResponse(
            success=True,
            generated_code=final_code,
            model_used=self._map_hapa_to_vllm_model(request.model_type).value,
            processing_time=0,  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° í•„ìš”
            token_usage={"total_tokens": len(final_code.split())},  # ê·¼ì‚¬ì¹˜
        )

    async def close(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()

    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ì„¸ì…˜ ì •ë¦¬"""
        if hasattr(
                self,
                "session") and self.session and not self.session.closed:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°ì—ë§Œ ì •ë¦¬
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(self.session.close())
            except RuntimeError:
                pass


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
vllm_service = VLLMIntegrationService()
