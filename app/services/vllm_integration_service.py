"""
vLLM ë©€í‹° LoRA ì„œë²„ í†µí•© ì„œë¹„ìŠ¤
- 4ê°€ì§€ ëª¨ë¸ íƒ€ì…ë³„ ì½”ë“œ ìƒì„±
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
- í•œêµ­ì–´/ì˜ì–´ ìë™ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì§€ì›
- ì‚¬ìš©ì ì„ íƒ ì˜µì…˜ ìµœì í™”
- ğŸ†• ì²­í¬ ë²„í¼ë§ ë° ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
"""

import asyncio
import json
import re
import time
from datetime import datetime
from enum import Enum
from typing import Any, AsyncGenerator, Dict, List, Optional

import aiohttp

from app.core.config import settings
from app.core.structured_logger import StructuredLogger
from app.schemas.code_generation import (
    CodeGenerationRequest,
    CodeGenerationResponse,
    ModelType,
)

# ğŸ†• AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ import ì¶”ê°€
from app.services.performance_profiler import ai_performance_metrics

logger = StructuredLogger("vllm_integration")


class VLLMModelType(str, Enum):
    """vLLM ì„œë²„ì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ íƒ€ì…"""

    AUTOCOMPLETE = "autocomplete"  # ì½”ë“œ ìë™ì™„ì„± (ë²ˆì—­ ì—†ìŒ)
    PROMPT = "prompt"  # ì¼ë°˜ ì½”ë“œ ìƒì„± (ì „ì²´ ë²ˆì—­)
    COMMENT = "comment"  # ì£¼ì„/docstring ìƒì„± (ì£¼ì„ë§Œ ë²ˆì—­)
    ERROR_FIX = "error_fix"  # ë²„ê·¸ ìˆ˜ì • (ì „ì²´ ë²ˆì—­)


class ChunkBuffer:
    """ì²­í¬ ë²„í¼ë§ í´ë˜ìŠ¤ - ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ì²­í¬ ê·¸ë£¹í™” ë° í›„ì²˜ë¦¬ (ê·¹í•œ ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self, buffer_size: int = 80, buffer_timeout: float = 0.1):
        # ğŸš€ ê·¹í•œ ì„±ëŠ¥ ìµœì í™” ì„¤ì • (99.9% ì²­í¬ ê°ì†Œ ëª©í‘œ: 30-50ê°œ ì²­í¬)
        self.buffer_size = buffer_size  # ê·¹í•œ ê°ì†Œ: 500 â†’ 80ì
        self.buffer_timeout = buffer_timeout  # ê·¹í•œ ê°ì†Œ: 2.0 â†’ 0.1ì´ˆ
        self.min_chunk_size = 200  # ê·¹í•œ ì¦ê°€: 120 â†’ 200ì (ë” í° ì²­í¬ ê°•ì œ)
        self.max_chunk_size = 800  # ê°ì†Œ: 1200 â†’ 800ì
        self.optimal_chunk_size = 400  # ì¦ê°€: 300 â†’ 400ì
        self.buffer = ""
        self.last_flush_time = time.time()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ë“¤
        self.total_chunks_processed = 0
        self.total_bytes_processed = 0
        self.small_chunks_count = 0  # 200ì ë¯¸ë§Œ ì²­í¬ ê°œìˆ˜
        self.large_chunks_count = 0  # 800ì ì´ˆê³¼ ì²­í¬ ê°œìˆ˜
        self.optimal_chunks_count = 0  # 200-400ì ì²­í¬ ê°œìˆ˜
        
        # ğŸ”¥ ê·¹ë„ë¡œ ì—„ê²©í•œ ì²­í¬ ìƒì„± ì •ì±…
        self.force_meaningful_boundaries = True
        self.strict_size_enforcement = True
        self.ultra_strict_mode = True  # ìƒˆë¡œìš´ ê·¹í•œ ëª¨ë“œ
        
        # ğŸ”¥ ê·¹ë„ë¡œ ì—„ê²©í•œ ì˜ë¯¸ êµ¬ë¶„ì íŒ¨í„´ (ì˜¤ì§ ì™„ì „í•œ ì½”ë“œ ë¸”ë¡ë§Œ)
        self.meaningful_delimiters = [
            # ìµœê³  ìš°ì„ ìˆœìœ„: ì™„ì „í•œ í•¨ìˆ˜/í´ë˜ìŠ¤ ë¸”ë¡ë§Œ (ìµœì†Œ 10ì¤„ ì´ìƒ)
            r'def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){10,}',     # í•¨ìˆ˜ ì •ì˜ (10ì¤„ ì´ìƒ)
            r'class\s+\w+[^:]*:\s*\n(?:\s{4}.*\n){10,}',       # í´ë˜ìŠ¤ ì •ì˜ (10ì¤„ ì´ìƒ)
            r'async\s+def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){8,}', # async í•¨ìˆ˜ (8ì¤„ ì´ìƒ)
            
            # ê³ ìš°ì„ ìˆœìœ„: ì™„ì „í•œ ì œì–´ êµ¬ì¡° (ìµœì†Œ 8ì¤„)
            r'if\s+[^:]+:\s*\n(?:\s{4}.*\n){8,}(?:else:\s*\n(?:\s{4}.*\n)*)?', # if-else (8ì¤„ ì´ìƒ)
            r'for\s+[^:]+:\s*\n(?:\s{4}.*\n){6,}',            # for ë£¨í”„ (6ì¤„ ì´ìƒ)
            r'while\s+[^:]+:\s*\n(?:\s{4}.*\n){6,}',          # while ë£¨í”„ (6ì¤„ ì´ìƒ)
            r'try:\s*\n(?:\s{4}.*\n){4,}except[^:]*:\s*\n(?:\s{4}.*\n){4,}', # try-except (ê° 4ì¤„ ì´ìƒ)
            
            # ì¤‘ìš°ì„ ìˆœìœ„: ì™„ì „í•œ docstringì´ë‚˜ ê¸´ ì£¼ì„ ë¸”ë¡ (100ì ì´ìƒ)
            r'"""\s*\n[^"]{100,}\n\s*"""',                    # ê¸´ docstring (100ì ì´ìƒ)
            r"'''\s*\n[^']{100,}\n\s*'''",                    # ê¸´ docstring (100ì ì´ìƒ)
            r'\n\s*#[^\n]{100,}\n',                           # ê¸´ ì£¼ì„ (100ì ì´ìƒ)
        ]
        
        # ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ ê°ì§€ íŒ¨í„´ (ë” ì—„ê²©í•˜ê²Œ)
        self.complete_code_patterns = [
            r'def\s+\w+\([^)]*\):\s*\n(?:\s{4}.*\n){8,}',     # ì™„ì „í•œ í•¨ìˆ˜ (8ì¤„ ì´ìƒ)
            r'class\s+\w+[^:]*:\s*\n(?:\s{4}.*\n){8,}',       # ì™„ì „í•œ í´ë˜ìŠ¤ (8ì¤„ ì´ìƒ)
            r'if\s+[^:]+:\s*\n(?:\s{4}.*\n){4,}else:\s*\n(?:\s{4}.*\n)+', # ì™„ì „í•œ if-else
            r'try:\s*\n(?:\s{4}.*\n)+except[^:]*:\s*\n(?:\s{4}.*\n)+', # ì™„ì „í•œ try-except
        ]
        
        # ğŸ¯ ì‹¤ì œ vLLM stop token íŒ¨í„´ (ì œê±°ìš©)
        self.special_token_patterns = [
            r'\n# --- Generation Complete ---.*$',            # vLLM ì™„ë£Œ ë§ˆì»¤ ë° ì´í›„ ë‚´ìš©
            r'<ï½œfimâ–beginï½œ>.*$',                           # FIM ì‹œì‘ í† í° ë° ì´í›„ ë‚´ìš©
            r'<ï½œfimâ–holeï½œ>.*$',                            # FIM í™€ í† í° ë° ì´í›„ ë‚´ìš©
            r'<ï½œfimâ–endï½œ>.*$',                             # FIM ì¢…ë£Œ í† í° ë° ì´í›„ ë‚´ìš©
            r'<\|endoftext\|>.*$',                            # GPT ì¢…ë£Œ í† í° ë° ì´í›„ ë‚´ìš©
            
            # ë°±ì—…ìš© ì¼ë°˜ì ì¸ í† í°ë“¤
            r'<\|im_end\|>.*$',                               # ChatML ì¢…ë£Œ í† í°
            r'<\|im_start\|>[^|]*\|>',                        # ChatML ì‹œì‘ í† í°
            r'<\|assistant\|>',                               # assistant í† í°
            r'<\|user\|>',                                    # user í† í°
            r'<\|system\|>',                                  # system í† í°
            r'<\|end[^>]*\|>',                                # ê¸°íƒ€ end í† í°
            r'<\|[^>]*\|>',                                   # ê¸°íƒ€ íŠ¹ìˆ˜ í† í°
            r'</?\w+[^>]*>',                                  # HTML íƒœê·¸ ìœ ì‚¬ íŒ¨í„´
            r'\[INST\]|\[/INST\]',                            # ëª…ë ¹ í† í°
            r'<s>|</s>',                                      # ì‹œì‘/ì¢…ë£Œ í† í°
            r'<unk>|<pad>|<eos>|<bos>',                       # íŠ¹ìˆ˜ í† í°ë“¤
            r'Assistant:|Human:|User:',                       # ì—­í•  ë¼ë²¨
        ]
        
        # ğŸš€ ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±° (Specific Pattern Removal)
        self.unwanted_patterns = [
            # ì…”ë±… ë° ìŠ¤í¬ë¦½íŠ¸ í—¤ë”
            r'#!/usr/bin/env python3?.*\n',                   # ì…”ë±… ë¼ì¸
            r'#!/bin/python3?.*\n',                           # ê°„ë‹¨í•œ ì…”ë±…
            r'# -\*- coding: utf-8 -\*-.*\n',                # ì¸ì½”ë”© ì„ ì–¸
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ë° ì£¼ì„ í—¤ë”
            r'# --- File Comment -*\n',                       # íŒŒì¼ ì£¼ì„ í—¤ë”
            r'# Created on\s*:.*\n',                          # ìƒì„± ë‚ ì§œ
            r'# Author\s*:.*\n',                              # ì‘ì„±ì
            r'# @Author\s*:.*\n',                             # @Author í˜•ì‹
            r'# Email\s*:.*\n',                               # ì´ë©”ì¼
            r'# Version\s*:.*\n',                             # ë²„ì „
            r'# Last modified\s*:.*\n',                       # ìˆ˜ì •ì¼
            r'# Description\s*:.*\n',                         # ì„¤ëª…
            
            # ê¸´ êµ¬ë¶„ì„  ë° ì¥ì‹ ì£¼ì„
            r'# -{20,}.*\n',                                  # ê¸´ ëŒ€ì‹œ ë¼ì¸
            r'# ={20,}.*\n',                                  # ê¸´ ë“±í˜¸ ë¼ì¸  
            r'# \*{20,}.*\n',                                 # ê¸´ ë³„í‘œ ë¼ì¸
            r'# _{20,}.*\n',                                  # ê¸´ ì–¸ë”ìŠ¤ì½”ì–´ ë¼ì¸
            
            # ë‚ ì§œ/ì‹œê°„ íŒ¨í„´
            r'\d{4}/\d{1,2}/\d{1,2}.*\d{1,2}:\d{2}:\d{2}',   # ë‚ ì§œì‹œê°„ í˜•ì‹
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',          # ISO ë‚ ì§œì‹œê°„
            
            # HTML/XML íƒœê·¸ ì”ì¬
            r'</c>',                                          # HTML íƒœê·¸ ì”ì¬
            r'<[^>]+>',                                       # ê¸°íƒ€ HTML íƒœê·¸
            
            # ì´ë©”ì¼ ì£¼ì†Œ
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', # ì´ë©”ì¼ íŒ¨í„´
            
            # ë¶ˆí•„ìš”í•œ ë…ìŠ¤íŠ¸ë§ í…œí”Œë¦¿
            r'"""[\s\S]*?"""',                                # ë©€í‹°ë¼ì¸ ë…ìŠ¤íŠ¸ë§ (ì„ íƒì )
            r"'''[\s\S]*?'''",                                # ë©€í‹°ë¼ì¸ ë…ìŠ¤íŠ¸ë§ (ì„ íƒì )
        ]
    
    def add_chunk(self, chunk: str) -> Optional[str]:
        """ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€í•˜ê³  í•„ìš”ì‹œ í”ŒëŸ¬ì‹œ - ê·¹í•œ ì„±ëŠ¥ ìµœì í™”ëœ ë¡œì§"""
        
        # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ìƒì„¸ ë¡œê·¸
        if settings.should_log_debug():
            print(f"ğŸ” [ChunkBuffer] ì²­í¬ ì…ë ¥: '{chunk[:30]}...' (ê¸¸ì´: {len(chunk)})")
        
        # ë¨¼ì € im_end í† í° ì²´í¬ - ë°œê²¬ë˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if self._contains_end_token(chunk):
            if settings.should_log_performance():
                print(f"ğŸ›‘ [ChunkBuffer] ì¢…ë£Œ í† í° ê°ì§€: '{chunk[:20]}...'")
            # im_end í† í° ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            clean_chunk = self._extract_content_before_end_token(chunk)
            if clean_chunk:
                self.buffer += clean_chunk
            # ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•˜ê³  ì¤‘ë‹¨ ì‹ í˜¸ ë°˜í™˜
            final_content = self.flush()
            return final_content if final_content.strip() else "[END_OF_GENERATION]"
        
        # ì¼ë°˜ì ì¸ íŠ¹ìˆ˜ í† í° ì œê±° (im_end ì œì™¸)
        cleaned_chunk = self._clean_special_tokens(chunk)
        
        # ë¹ˆ ë‚´ìš©ì´ë©´ ë¬´ì‹œ
        if not cleaned_chunk.strip():
            return None
            
        self.buffer += cleaned_chunk
        current_time = time.time()
        
        # ğŸ”¥ ê·¹ë„ë¡œ ì—„ê²©í•œ í”ŒëŸ¬ì‹œ ì¡°ê±´ (30-50 ì²­í¬ ëª©í‘œ)
        if self.ultra_strict_mode:
            # ê·¹ë„ë¡œ ì—„ê²©í•œ ëª¨ë“œ: ìµœì†Œ í¬ê¸°ì˜ 3ë°° ë¯¸ë‹¬ ì‹œ ì ˆëŒ€ í”ŒëŸ¬ì‹œ ê¸ˆì§€
            if len(self.buffer) < self.min_chunk_size * 3:  # 600ì ë¯¸ë§Œ
                # ê·¹ë„ë¡œ ì œí•œëœ ì˜ˆì™¸: ì˜¤ì§ ìµœëŒ€ í¬ê¸° 2ë°° ì´ˆê³¼ë‚˜ ê°•ì œ ì¢…ë£Œì‹œë§Œ
                if (len(self.buffer) >= self.max_chunk_size * 2 or  # 1600ì ì´ìƒ
                    self._contains_end_token(self.buffer)):
                    should_flush = True
                else:
                    should_flush = False
            else:
                # ìµœì†Œ í¬ê¸° 3ë°° ì¶©ì¡± ì‹œì—ë§Œ ë‹¤ë¥¸ ì¡°ê±´ ê²€í† 
                should_flush = (
                    # 1. ìµœì  í¬ê¸° 3ë°° ë„ë‹¬ + ì™„ì „í•œ ì½”ë“œ ìš”ì†Œë§Œ
                    (len(self.buffer) >= self.optimal_chunk_size * 3 and  # 1200ì ì´ìƒ
                     self._has_complete_code_element()) or
                    
                    # 2. ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ ì™„ì„± + ìµœì†Œ 600ì ì´ìƒ
                    (len(self.buffer) >= 600 and
                     self._has_complete_code_element() and
                     self._has_strong_meaningful_boundary()) or
                    
                    # 3. ë²„í¼ í¬ê¸° 4ë°° ì´ˆê³¼ (ê°•ì œ í”ŒëŸ¬ì‹œ)
                    len(self.buffer) >= self.buffer_size * 4.0 or  # 320ì ì´ìƒ
                    
                    # 4. ìµœëŒ€ í¬ê¸° 2ë°° ì´ˆê³¼ (ë¬´ì¡°ê±´ í”ŒëŸ¬ì‹œ)
                    len(self.buffer) >= self.max_chunk_size * 2 or  # 1600ì ì´ìƒ
                    
                    # 5. ë§¤ìš° ì—„ê²©í•œ ì‹œê°„ ê¸°ë°˜ ì¡°ê±´ (ê±°ì˜ ë°œìƒ ì•ˆí•¨)
                    (current_time - self.last_flush_time >= self.buffer_timeout * 10.0 and  # 1ì´ˆ ì´ìƒ
                     len(self.buffer) >= self.min_chunk_size * 4 and  # 800ì ì´ìƒ
                     self._has_complete_code_element() and  # ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ
                     self._has_strong_meaningful_boundary())  # ê°•í•œ ê²½ê³„ë§Œ
                )
        else:
            # ê¸°ì¡´ ë¡œì§ (í˜¸í™˜ì„±)
            should_flush = (
                len(self.buffer) >= self.min_chunk_size and (
                    len(self.buffer) >= self.buffer_size * 1.8 or
                    current_time - self.last_flush_time >= self.buffer_timeout or
                    self._has_complete_code_element() or
                    len(self.buffer) >= self.max_chunk_size
                )
            )
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì²­í¬ í’ˆì§ˆ ë¶„ë¥˜
        if should_flush:
            buffer_length = len(self.buffer)
            
            # ì²­í¬ í¬ê¸°ë³„ ë¶„ë¥˜ (ìƒˆë¡œìš´ ê¸°ì¤€)
            if buffer_length < self.min_chunk_size:
                self.small_chunks_count += 1
                if settings.should_log_performance():
                    print(f"âš ï¸ [ChunkBuffer] ì‘ì€ ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì (ë¹„ì •ìƒ)")
            elif buffer_length <= self.optimal_chunk_size:
                self.optimal_chunks_count += 1
                if settings.should_log_debug():
                    print(f"âœ… [ChunkBuffer] ìµœì  ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            else:
                self.large_chunks_count += 1
                if settings.should_log_performance():
                    print(f"ğŸ“¦ [ChunkBuffer] ëŒ€í˜• ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            
            result = self.flush()
            
            # í”ŒëŸ¬ì‹œ ìƒì„¸ ë¡œê·¸
            if settings.should_log_performance():
                chunk_quality = "ìµœì " if self.min_chunk_size <= buffer_length <= self.optimal_chunk_size else "ë¹„ì •ìƒ"
                print(f"ğŸ“¤ [ChunkBuffer] {chunk_quality} í”ŒëŸ¬ì‹œ ì™„ë£Œ: {buffer_length}ì â†’ {len(result)}ì")
            
            return result
        else:
            if settings.should_log_debug():
                print(f"ğŸ”„ [ChunkBuffer] ë²„í¼ë§ ì¤‘: {len(self.buffer)}/{self.buffer_size}ì")
        
        return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ê°•í™”ëœ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        total_chunks = max(self.total_chunks_processed, 1)
        avg_chunk_size = (self.total_bytes_processed / total_chunks)
        
        # ì²­í¬ í’ˆì§ˆ ë¶„ì„
        small_ratio = round(self.small_chunks_count / total_chunks * 100, 2)
        optimal_ratio = round(self.optimal_chunks_count / total_chunks * 100, 2)
        large_ratio = round(self.large_chunks_count / total_chunks * 100, 2)
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        if small_ratio <= 5 and optimal_ratio >= 70:
            performance_grade = "A"  # ìš°ìˆ˜
        elif small_ratio <= 15 and optimal_ratio >= 50:
            performance_grade = "B"  # ì–‘í˜¸
        elif small_ratio <= 30:
            performance_grade = "C"  # ë³´í†µ
        else:
            performance_grade = "D"  # ê°œì„  í•„ìš”
        
        return {
            "total_chunks": self.total_chunks_processed,
            "total_bytes": self.total_bytes_processed,
            "avg_chunk_size": round(avg_chunk_size, 2),
            
            # ì²­í¬ í¬ê¸°ë³„ ë¶„ë¥˜
            "small_chunks_count": self.small_chunks_count,
            "optimal_chunks_count": self.optimal_chunks_count,
            "large_chunks_count": self.large_chunks_count,
            
            # ë¹„ìœ¨ ë¶„ì„
            "small_chunks_ratio": small_ratio,
            "optimal_chunks_ratio": optimal_ratio,
            "large_chunks_ratio": large_ratio,
            
            # ì„±ëŠ¥ ì§€í‘œ
            "performance_grade": performance_grade,
            "buffer_efficiency": round(optimal_ratio + (large_ratio * 0.7), 2),  # íš¨ìœ¨ì„± ì ìˆ˜
            
            # í˜„ì¬ ìƒíƒœ
            "current_buffer_size": len(self.buffer),
            "buffer_utilization": round(len(self.buffer) / self.buffer_size * 100, 2),
            
            # ì„¤ì • ì •ë³´
            "min_chunk_size": self.min_chunk_size,
            "optimal_chunk_size": self.optimal_chunk_size,
            "max_chunk_size": self.max_chunk_size,
            "strict_mode": self.strict_size_enforcement
        }
    
    def _clean_special_tokens(self, text: str) -> str:
        """AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° ì œê±°"""
        cleaned_text = text
        for pattern in self.special_token_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        return cleaned_text
    
    def _remove_unwanted_patterns(self, text: str) -> str:
        """ğŸš€ ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±° (Specific Pattern Removal)"""
        cleaned_text = text
        
        # ì²« ë²ˆì§¸ë¡œ ì¢…ë£Œ ë§ˆì»¤ ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ ì ìš©
        cleaned_text = self._extract_content_before_end_token(cleaned_text)
        
        # ë‘ ë²ˆì§¸ë¡œ ë¶ˆí•„ìš”í•œ íŒ¨í„´ë“¤ ì œê±°
        for pattern in self.unwanted_patterns:
            before_length = len(cleaned_text)
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.MULTILINE | re.IGNORECASE)
            after_length = len(cleaned_text)
            
            # ë¡œê·¸: íŒ¨í„´ì´ ì œê±°ë˜ì—ˆì„ ë•Œë§Œ
            if before_length != after_length and settings.should_log_performance():
                print(f"ğŸ§¹ [íŒ¨í„´ì œê±°] '{pattern[:30]}...' ì œê±°: {before_length-after_length}ì")
        
        # ìµœì¢… ì •ë¦¬: ê³¼ë„í•œ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì œê±°
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)  # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        cleaned_text = re.sub(r'[ \t]+\n', '\n', cleaned_text)  # ì¤„ ë ê³µë°± ì œê±°
        cleaned_text = re.sub(r'\n[ \t]+', '\n', cleaned_text)  # ì¤„ ì‹œì‘ ê³µë°± ì œê±° (ë“¤ì—¬ì“°ê¸° ì œì™¸)
        
        return cleaned_text.strip()
    
    def _has_complete_code_element(self) -> bool:
        """ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ(í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±)ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        for pattern in self.complete_code_patterns:
            if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
                return True
        return False
    
    def _has_strong_meaningful_boundary(self) -> bool:
        """ê°•í•œ ì˜ë¯¸ ê²½ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜¤ì§ ìµœê³  ìš°ì„ ìˆœìœ„ íŒ¨í„´ë§Œ)"""
        # ì˜¤ì§ ì²« ë²ˆì§¸ íŒ¨í„´ë§Œ ì²´í¬: ì™„ì „í•œ í•¨ìˆ˜ ì •ì˜ (5ì¤„ ì´ìƒ)
        pattern = self.meaningful_delimiters[0]  # def í•¨ìˆ˜ (5ì¤„ ì´ìƒ)ë§Œ
        if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
            return True
        return False
    
    def _has_meaningful_boundary(self) -> bool:
        """ì˜ë¯¸ìˆëŠ” ê²½ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ìƒìœ„ íŒ¨í„´ë§Œ)"""
        # ìƒìœ„ 7ê°œ íŒ¨í„´ë§Œ ì²´í¬ (í•¨ìˆ˜, í´ë˜ìŠ¤, asyncí•¨ìˆ˜, if-else, for, while, try-except)
        for pattern in self.meaningful_delimiters[:7]:
            if re.search(pattern, self.buffer, re.MULTILINE | re.DOTALL):
                return True
        return False
    
    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš©ì„ í”ŒëŸ¬ì‹œí•˜ê³  í›„ì²˜ë¦¬ (í†µê³„ëŠ” add_chunkì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)"""
        content = self.buffer
        self.buffer = ""
        self.last_flush_time = time.time()
        
        # ì „ì—­ í†µê³„ë§Œ ì—…ë°ì´íŠ¸ (í¬ê¸°ë³„ ë¶„ë¥˜ëŠ” add_chunkì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
        self.total_chunks_processed += 1
        self.total_bytes_processed += len(content)
        
        # ğŸš€ ê°•í™”ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ (2ë‹¨ê³„ ë°©ë²• ì ìš©)
        if content:
            # 1ë‹¨ê³„: ì¢…ë£Œ ë§ˆì»¤ ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ + ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
            content = self._remove_unwanted_patterns(content)
            
            # 2ë‹¨ê³„: AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° ì œê±° (ë°±ì—…)
            content = self._clean_special_tokens(content)
            
            # 3ë‹¨ê³„: ì½”ë“œ ë¸”ë¡ ì •ë¦¬
            content = re.sub(r'\n{3,}```', '\n\n```', content)  # ì½”ë“œ ë¸”ë¡ ì• ê³¼ë„í•œ ì¤„ë°”ê¿ˆ
            content = re.sub(r'```\n{3,}', '```\n\n', content)  # ì½”ë“œ ë¸”ë¡ ë’¤ ê³¼ë„í•œ ì¤„ë°”ê¿ˆ
        
        return content.strip()
    
    def force_flush(self) -> Optional[str]:
        """ê°•ì œ í”ŒëŸ¬ì‹œ (ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì‹œ) - ğŸš€ 2ë‹¨ê³„ ì •ë¦¬ ì ìš©"""
        if self.buffer:
            content = self.flush()
            # ğŸš€ ì¶”ê°€ì ì¸ ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±° (ë” ê°•ë ¥í•œ ì •ë¦¬)
            content = self._remove_unwanted_patterns(content)
            content = self._clean_special_tokens(content)
            return content if content.strip() else None
        return None

    def _contains_end_token(self, text: str) -> bool:
        """ì‹¤ì œ vLLM stop token í™•ì¸ - FIM í† í° í¬í•¨"""
        # ğŸ¯ ì‹¤ì œ vLLMì—ì„œ ì‚¬ìš©í•˜ëŠ” stop tokenë“¤
        end_patterns = [
            r'\n# --- Generation Complete ---',               # vLLM ì™„ë£Œ ë§ˆì»¤
            r'<ï½œfimâ–beginï½œ>',                              # FIM ì‹œì‘ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<ï½œfimâ–holeï½œ>',                               # FIM í™€ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<ï½œfimâ–endï½œ>',                                # FIM ì¢…ë£Œ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<\|endoftext\|>',                               # GPT ìŠ¤íƒ€ì¼ ì¢…ë£Œ (ì˜ì–´ |)
            
            # ë°±ì—…ìš© ì¼ë°˜ì ì¸ ì¢…ë£Œ íŒ¨í„´ë“¤
            r'<\|im_end\|>',                                  # ChatML ì¢…ë£Œ
            r'</s>',                                          # ì‹œí€€ìŠ¤ ì¢…ë£Œ
            r'<eos>',                                         # End of Sequence
            r'\[DONE\]',                                      # ì»¤ìŠ¤í…€ ì™„ë£Œ ì‹ í˜¸
        ]
        
        for pattern in end_patterns:
            if re.search(pattern, text, re.MULTILINE):
                return True
        return False
    
    def _extract_content_before_end_token(self, text: str) -> str:
        """ğŸš€ ì¢…ë£Œ ë§ˆì»¤ ê¸°ë°˜ íŠ¸ë ì¼€ì´ì…˜ (Stop Marker Truncation)"""
        # ğŸ¯ ì‹¤ì œ vLLMì—ì„œ ì‚¬ìš©í•˜ëŠ” stop tokenë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        end_patterns = [
            r'\n# --- Generation Complete ---',               # vLLM ì™„ë£Œ ë§ˆì»¤
            r'<ï½œfimâ–beginï½œ>',                              # FIM ì‹œì‘ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<ï½œfimâ–holeï½œ>',                               # FIM í™€ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<ï½œfimâ–endï½œ>',                                # FIM ì¢…ë£Œ í† í° (ì¼ë³¸ì–´ ï½œ)
            r'<\|endoftext\|>',                               # GPT ìŠ¤íƒ€ì¼ ì¢…ë£Œ (ì˜ì–´ |)
            
            # ë°±ì—…ìš© ì¼ë°˜ì ì¸ ì¢…ë£Œ íŒ¨í„´ë“¤
            r'<\|im_end\|>',                                  # ChatML ì¢…ë£Œ
            r'</s>',                                          # ì‹œí€€ìŠ¤ ì¢…ë£Œ
            r'<eos>',                                         # End of Sequence
            r'\[DONE\]',                                      # ì»¤ìŠ¤í…€ ì™„ë£Œ ì‹ í˜¸
            
            # ğŸš€ ì¶”ê°€ íŠ¸ë ì¼€ì´ì…˜ íŒ¨í„´ë“¤ (ë¶ˆí•„ìš”í•œ ë‚´ìš© ì°¨ë‹¨)
            r'#!/usr/bin/env python',                         # ì…”ë±… ì‹œì‘ì ì—ì„œ ì°¨ë‹¨
            r'# --- File Comment',                            # íŒŒì¼ ì£¼ì„ ì‹œì‘ì ì—ì„œ ì°¨ë‹¨
            r'# Created on\s*:',                              # ë©”íƒ€ë°ì´í„° ì‹œì‘ì ì—ì„œ ì°¨ë‹¨
            r'# Author\s*:',                                  # ì‘ì„±ì ì •ë³´ ì‹œì‘ì ì—ì„œ ì°¨ë‹¨
            r'# @Author',                                     # @Author ì‹œì‘ì ì—ì„œ ì°¨ë‹¨
            r'@\w+\.\w+',                                     # ì´ë©”ì¼ ì‹œì‘ì ì—ì„œ ì°¨ë‹¨ (ì˜ˆ: jiaoyu_li@deepseeks.com)
            r'</c>',                                          # HTML íƒœê·¸ ì”ì¬ì—ì„œ ì°¨ë‹¨
        ]
        
        # ê°€ì¥ ë¨¼ì € ë°œê²¬ë˜ëŠ” íŒ¨í„´ì—ì„œ íŠ¸ë ì¼€ì´ì…˜
        earliest_match = None
        earliest_position = len(text)
        earliest_pattern = None
        
        for pattern in end_patterns:
            match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
            if match and match.start() < earliest_position:
                earliest_position = match.start()
                earliest_match = match
                earliest_pattern = pattern
        
        if earliest_match:
            # ê°€ì¥ ë¹ ë¥¸ ì¢…ë£Œ ë§ˆì»¤ ì´ì „ ë¶€ë¶„ë§Œ ë°˜í™˜
            content_before = text[:earliest_position].strip()
            if settings.should_log_performance():
                print(f"âœ‚ï¸ [íŠ¸ë ì¼€ì´ì…˜] '{earliest_pattern}' ê°ì§€ â†’ ì°¨ë‹¨: '{content_before[:50]}...'")
            return content_before
        
        return text


class VLLMIntegrationService:
    """vLLM ë©€í‹° LoRA ì„œë²„ì™€ì˜ í†µí•© ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.vllm_base_url = settings.VLLM_SERVER_URL
        self.timeout = aiohttp.ClientTimeout(total=settings.VLLM_TIMEOUT_SECONDS)
        self.session = None
        
        # ğŸš€ ì²­í¬ ë²„í¼ë§ ì„¤ì • ê·¹í•œ ê°•í™” (30-50 ì²­í¬ ëª©í‘œ)
        self.chunk_buffering_enabled = True
        self.default_buffer_size = 80  # ê·¹í•œ ê°ì†Œ: 500 â†’ 80ì
        self.default_buffer_timeout = 0.1  # ê·¹í•œ ê°ì†Œ: 2.0 â†’ 0.1ì´ˆ
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.enable_performance_logging = settings.should_log_performance()
        self.enable_debug_logging = settings.should_log_debug()
        self.enable_chunk_details = getattr(settings, 'should_log_chunk_details', lambda: False)() 
        
        if self.enable_performance_logging:
            print(f"âš™ï¸ [vLLM] ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: ë²„í¼í¬ê¸°={self.default_buffer_size}, íƒ€ì„ì•„ì›ƒ={self.default_buffer_timeout}ì´ˆ")

    async def _get_session(self) -> aiohttp.ClientSession:
        """aiohttp ì„¸ì…˜ ìƒì„± ë° ì¬ì‚¬ìš©"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=self.timeout,
                headers={
                    "Content-Type": "application/json",
                    "Accept": "text/event-stream",
                },
            )
        return self.session

    async def check_health(self) -> Dict[str, Any]:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.vllm_base_url}/health") as response:
                if response.status == 200:
                    result = await response.json()
                    
                    # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹…
                    if settings.should_log_debug():
                        logger.log_system_event(
                            "vLLM ì„œë²„ ìƒíƒœ í™•ì¸", "success", {"server_status": result}
                        )
                    
                    return {"status": "healthy", "details": result}
                else:
                    logger.log_system_event(
                        "vLLM ì„œë²„ ìƒíƒœ í™•ì¸",
                        "failed",
                        {"http_status": response.status},
                    )
                    return {
                        "status": "unhealthy",
                        "http_status": response.status}
        except Exception as e:
            logger.log_error(e, "vLLM ì„œë²„ ì—°ê²°")
            return {"status": "error", "error": str(e)}

    async def get_available_models(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.vllm_base_url}/models") as response:
                if response.status == 200:
                    models = await response.json()
                    
                    # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹…
                    if settings.should_log_debug():
                        logger.log_system_event(
                            "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ", "success", {"model_count": len(models)}
                        )
                    
                    return {"status": "success", "models": models}
                else:
                    return {"status": "error", "http_status": response.status}
        except Exception as e:
            logger.log_error(e, "ëª¨ë¸ ëª©ë¡ ì¡°íšŒ")
            return {"status": "error", "error": str(e)}

    def _map_hapa_to_vllm_model(self, hapa_model: ModelType) -> VLLMModelType:
        """HAPA ëª¨ë¸ íƒ€ì…ì„ vLLM ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘"""
        mapping = {
            ModelType.CODE_COMPLETION: VLLMModelType.AUTOCOMPLETE,
            ModelType.CODE_GENERATION: VLLMModelType.PROMPT,
            ModelType.CODE_EXPLANATION: VLLMModelType.COMMENT,
            ModelType.BUG_FIX: VLLMModelType.ERROR_FIX,
            ModelType.CODE_REVIEW: VLLMModelType.PROMPT,
            ModelType.CODE_OPTIMIZATION: VLLMModelType.PROMPT,
            ModelType.UNIT_TEST_GENERATION: VLLMModelType.PROMPT,
            ModelType.DOCUMENTATION: VLLMModelType.COMMENT,
        }
        return mapping.get(hapa_model, VLLMModelType.PROMPT)

    def _prepare_vllm_request(
        self, request: CodeGenerationRequest, user_id: str
    ) -> Dict[str, Any]:
        """HAPA ìš”ì²­ì„ vLLM ìš”ì²­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ - ê·¹í•œ ì„±ëŠ¥ ìµœì í™”"""
        vllm_model = self._map_hapa_to_vllm_model(request.model_type)

        # ğŸš€ ìš”ì²­ ë³µì¡ë„ ë¶„ì„ ë° ë™ì  íŒŒë¼ë¯¸í„° ìµœì í™”
        complexity_analysis = self._analyze_request_complexity(request.prompt)
        optimized_params = self._get_optimized_parameters(complexity_analysis, vllm_model)
        
        # ğŸš€ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ìµœì í™” (ê°„ê²°ì„± ê°•ì œ)
        optimized_prompt = self._optimize_prompt_for_model(
            request.prompt, vllm_model, request, complexity_analysis
        )

        # ì‚¬ìš©ì ì„ íƒ ì˜µì…˜ ë§¤í•‘
        user_select_options = self._map_user_options(request)

        # user_idë¥¼ ìˆ«ìë¡œ ë³€í™˜ (í•´ì‹œ ì‚¬ìš©)
        try:
            numeric_user_id = abs(hash(user_id)) % 1000000  # 1-1000000 ë²”ìœ„
        except BaseException:
            numeric_user_id = 12345  # ê¸°ë³¸ê°’

        # ğŸ¯ ì‹¤ì œ vLLMì—ì„œ ì‚¬ìš©í•˜ëŠ” stop token ì„¤ì •
        stop_tokens = [
            "\n# --- Generation Complete ---",  # vLLM ì™„ë£Œ ë§ˆì»¤
            "<ï½œfimâ–beginï½œ>",                  # FIM ì‹œì‘ í† í° (ì¼ë³¸ì–´ ï½œ)
            "<ï½œfimâ–holeï½œ>",                   # FIM í™€ í† í° (ì¼ë³¸ì–´ ï½œ)
            "<ï½œfimâ–endï½œ>",                    # FIM ì¢…ë£Œ í† í° (ì¼ë³¸ì–´ ï½œ)
            "<|endoftext|>",                    # GPT ìŠ¤íƒ€ì¼ ì¢…ë£Œ í† í° (ì˜ì–´ |)
        ]
        
        # ê°„ë‹¨í•œ ìš”ì²­ì— ëŒ€í•´ì„œëŠ” ë” ì—„ê²©í•œ ì¢…ë£Œ ì¡°ê±´ ì¶”ê°€
        if complexity_analysis["level"] == "simple":
            stop_tokens.extend([
                "\n\n```",       # ì½”ë“œ ë¸”ë¡ í›„ ì¦‰ì‹œ ì¢…ë£Œ
                "\n\n#",         # ì£¼ì„ ì‹œì‘ ì‹œ ì¢…ë£Œ
                "\nprint(",      # ì¶”ê°€ printë¬¸ ë°©ì§€
                "\n# ì„¤ëª…",      # ì„¤ëª… ì‹œì‘ ì‹œ ì¢…ë£Œ
                "\n# ì˜ˆì‹œ",      # ì˜ˆì‹œ ì‹œì‘ ì‹œ ì¢…ë£Œ
            ])

        vllm_request = {
            "user_id": numeric_user_id,
            "model_type": vllm_model.value,
            "prompt": optimized_prompt,
            "user_select_options": user_select_options,
            "temperature": optimized_params["temperature"],
            "top_p": optimized_params["top_p"],
            "max_tokens": optimized_params["max_tokens"],
            "stop": stop_tokens,  # ğŸš€ ì¢…ë£Œ í† í° ì¶”ê°€
        }

        # í™˜ê²½ë³„ ì¡°ê±´ë¶€ ë¡œê¹… - ìš”ì²­ ìƒì„¸ ì •ë³´
        if settings.should_log_request_response():
            logger.log_system_event(
                f"vLLM ìš”ì²­ ì¤€ë¹„ (ìµœì í™”ë¨)",
                "success",
                {
                    "user_id": user_id,
                    "numeric_user_id": numeric_user_id,
                    "model_type": vllm_model.value,
                    "prompt_length": len(optimized_prompt),
                    "complexity": complexity_analysis["level"],
                    "max_tokens": optimized_params["max_tokens"],
                    "temperature": optimized_params["temperature"],
                },
            )

        return vllm_request
    
    def _analyze_request_complexity(self, prompt: str) -> Dict[str, Any]:
        """ìš”ì²­ ë³µì¡ë„ ë¶„ì„ - ê°„ë‹¨/ì¤‘ê°„/ë³µì¡ ë¶„ë¥˜"""
        prompt_lower = prompt.lower()
        
        # ğŸ” ê°„ë‹¨í•œ ìš”ì²­ íŒ¨í„´ ê°ì§€
        simple_patterns = [
            # ì¶œë ¥ ê´€ë ¨
            r'(ì¶œë ¥|print|display).*["\']?\w{1,10}["\']?',  # "jay ì¶œë ¥", "hello world ì¶œë ¥"
            r'["\']?\w{1,10}["\']?.*ì¶œë ¥',                 # "jayë¥¼ ì¶œë ¥"
            r'print\s*\(["\']?\w{1,20}["\']?\)',           # print("jay")
            
            # ë³€ìˆ˜ ì„ ì–¸
            r'^[a-zA-Z_]\w*\s*=\s*["\']?\w{1,20}["\']?$',  # name = "jay"
            
            # ê°„ë‹¨í•œ í•¨ìˆ˜ í˜¸ì¶œ
            r'^\w+\(\)$',                                  # func()
            
            # í•œ ì¤„ ì½”ë“œ
            r'^.{1,50}$',                                  # 50ì ì´í•˜
        ]
        
        # ğŸ” ë³µì¡í•œ ìš”ì²­ íŒ¨í„´ ê°ì§€
        complex_patterns = [
            # í´ë˜ìŠ¤/í•¨ìˆ˜ ì •ì˜
            r'(class|def|async def)',
            r'(algorithm|ì•Œê³ ë¦¬ì¦˜)',
            r'(database|ë°ì´í„°ë² ì´ìŠ¤|db)',
            r'(api|rest|graphql)',
            r'(optimization|ìµœì í™”)',
            r'(machine learning|ë¨¸ì‹ ëŸ¬ë‹|ml)',
            r'(data structure|ìë£Œêµ¬ì¡°)',
            r'(design pattern|ë””ìì¸íŒ¨í„´)',
            
            # ë³µì¡í•œ ê¸°ëŠ¥
            r'(error handling|ì˜ˆì™¸ì²˜ë¦¬)',
            r'(unit test|í…ŒìŠ¤íŠ¸)',
            r'(documentation|ë¬¸ì„œí™”)',
            r'(refactor|ë¦¬íŒ©í† ë§)',
        ]
        
        # ê¸¸ì´ ê¸°ë°˜ ë¶„ì„
        char_count = len(prompt)
        word_count = len(prompt.split())
        
        # íŒ¨í„´ ë§¤ì¹­
        simple_matches = sum(1 for pattern in simple_patterns if re.search(pattern, prompt, re.IGNORECASE))
        complex_matches = sum(1 for pattern in complex_patterns if re.search(pattern, prompt, re.IGNORECASE))
        
        # ë³µì¡ë„ ê²°ì •
        if simple_matches > 0 and char_count <= 50 and complex_matches == 0:
            complexity_level = "simple"
            confidence = 0.9
        elif complex_matches > 0 or char_count > 200 or word_count > 30:
            complexity_level = "complex"
            confidence = 0.8
        else:
            complexity_level = "medium"
            confidence = 0.7
        
        return {
            "level": complexity_level,
            "confidence": confidence,
            "char_count": char_count,
            "word_count": word_count,
            "simple_matches": simple_matches,
            "complex_matches": complex_matches,
            "patterns_detected": []
        }
    
    def _get_optimized_parameters(self, complexity_analysis: Dict[str, Any], model_type: VLLMModelType) -> Dict[str, Any]:
        """ë³µì¡ë„ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        complexity_level = complexity_analysis["level"]
        
        # ğŸš€ ë³µì¡ë„ë³„ ê·¹í•œ ìµœì í™” íŒŒë¼ë¯¸í„°
        if complexity_level == "simple":
            # ê°„ë‹¨í•œ ìš”ì²­: ê·¹í•œ ìµœì í™” (3-5ì´ˆ, 30-50 ì²­í¬ ëª©í‘œ)
            return {
                "max_tokens": 50,      # ê·¹í•œ ê°ì†Œ: 1024 â†’ 50 í† í°
                "temperature": 0.1,    # ê·¹í•œ ê°ì†Œ: 0.3 â†’ 0.1 (ì •í™•ì„± ìš°ì„ )
                "top_p": 0.8,          # ê°ì†Œ: 0.95 â†’ 0.8 (ì§‘ì¤‘ë„ ì¦ê°€)
            }
        elif complexity_level == "medium":
            # ì¤‘ê°„ ë³µì¡ë„: ì ë‹¹í•œ ìµœì í™”
            return {
                "max_tokens": 200,     # í¬ê²Œ ê°ì†Œ: 1024 â†’ 200 í† í°
                "temperature": 0.2,    # ê°ì†Œ: 0.3 â†’ 0.2
                "top_p": 0.85,         # ê°ì†Œ: 0.95 â†’ 0.85
            }
        else:  # complex
            # ë³µì¡í•œ ìš”ì²­: ë³´ìˆ˜ì  ìµœì í™”
            return {
                "max_tokens": 500,     # ì¤‘ê°„ ê°ì†Œ: 1024 â†’ 500 í† í°
                "temperature": 0.25,   # ì•½ê°„ ê°ì†Œ: 0.3 â†’ 0.25
                "top_p": 0.9,          # ì•½ê°„ ê°ì†Œ: 0.95 â†’ 0.9
            }

    def _optimize_prompt_for_model(
        self,
        prompt: str,
        model_type: VLLMModelType,
        request: CodeGenerationRequest,
        complexity_analysis: Dict[str, Any]) -> str:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” - ê°„ê²°ì„± ê°•ì œ"""
        
        complexity_level = complexity_analysis["level"]
        
        # ğŸš€ ê°„ë‹¨í•œ ìš”ì²­ì— ëŒ€í•œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ìµœì í™”
        if complexity_level == "simple":
            # ê°„ë‹¨í•œ ìš”ì²­: ê·¹ë„ë¡œ ê°„ê²°í•œ ì‘ë‹µ ê°•ì œ
            if model_type == VLLMModelType.AUTOCOMPLETE:
                return prompt
            
            # ê°„ë‹¨í•œ ì¶œë ¥ ìš”ì²­ ìµœì í™”
            if re.search(r'(ì¶œë ¥|print)', prompt, re.IGNORECASE):
                # "jay ì¶œë ¥" -> ê°•ì œë¡œ í•œ ì¤„ ì½”ë“œë§Œ ìš”ì²­
                return f"""ë‹¤ìŒ ìš”ì²­ì— ëŒ€í•´ Python ì½”ë“œ í•œ ì¤„ë§Œ ì‘ì„±í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì—†ì´ ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ìš”ì²­: {prompt}

ì¡°ê±´:
- í•œ ì¤„ ì½”ë“œë§Œ ì‘ì„±
- print() í•¨ìˆ˜ ì‚¬ìš©
- ì„¤ëª… ê¸ˆì§€
- ì˜ˆì‹œë‚˜ ì¶”ê°€ ë‚´ìš© ê¸ˆì§€

ì½”ë“œ:"""
            
            else:
                return f"""ë‹¤ìŒ ìš”ì²­ì— ëŒ€í•´ ìµœì†Œí•œì˜ Python ì½”ë“œë§Œ ì‘ì„±í•˜ì„¸ìš”. ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ì½”ë“œë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ìš”ì²­: {prompt}

ì¡°ê±´:
- ìµœëŒ€ 3ì¤„ ì½”ë“œ
- í•„ìˆ˜ ì½”ë“œë§Œ ì‘ì„±
- ì„¤ëª… ìµœì†Œí™”
- ì˜ˆì‹œ ê¸ˆì§€

ì½”ë“œ:"""
        
        # ê¸°ì¡´ ë¡œì§ (ì¤‘ê°„/ë³µì¡í•œ ìš”ì²­)
        if model_type == VLLMModelType.AUTOCOMPLETE:
            # ìë™ì™„ì„±: ì»¨í…ìŠ¤íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            return prompt

        elif model_type == VLLMModelType.COMMENT:
            # ì£¼ì„/ë¬¸ì„œí™”: ì½”ë“œ í•´ì„ ë° ë¬¸ì„œí™” í”„ë¡¬í”„íŠ¸
            context_prefix = (
                f"# ëŒ€ìƒ ì½”ë“œ:\n{request.context}\n\n" if request.context else ""
            )
            return f"{context_prefix}# ë¬¸ì„œí™” ìš”ì²­: {prompt}"

        elif model_type == VLLMModelType.ERROR_FIX:
            # ë²„ê·¸ ìˆ˜ì •: ì˜¤ë¥˜ ë¶„ì„ ë° ìˆ˜ì • í”„ë¡¬í”„íŠ¸
            context_prefix = (
                f"# ì˜¤ë¥˜ê°€ ìˆëŠ” ì½”ë“œ:\n{request.context}\n\n" if request.context else ""
            )
            return f"""{context_prefix}# ë²„ê·¸ ìˆ˜ì • ìš”ì²­: {prompt}

# ìˆ˜ì • ê°€ì´ë“œë¼ì¸:
1. ì˜¤ë¥˜ ì›ì¸ ëª…í™•íˆ ë¶„ì„
2. ìµœì†Œí•œì˜ ìˆ˜ì •ìœ¼ë¡œ ë¬¸ì œ í•´ê²°
3. ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì½”ë“œ ì‘ì„±

## ìˆ˜ì •ëœ ì½”ë“œ:"""

        else:  # PROMPT (ê¸°ë³¸)
            # ì¼ë°˜ ì½”ë“œ ìƒì„±: ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ í‘œí˜„
            if complexity_level == "medium":
                context_prefix = (
                    f"# ì»¨í…ìŠ¤íŠ¸:\n{request.context}\n\n" if request.context else ""
                )
                return f"""{context_prefix}# ìš”ì²­ì‚¬í•­: {prompt}

ì¡°ê±´:
- ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì½”ë“œ ì‘ì„±
- í•„ìˆ˜ ê¸°ëŠ¥ë§Œ êµ¬í˜„
- ê³¼ë„í•œ ì„¤ëª… ê¸ˆì§€

ì½”ë“œ:"""
            else:  # complex
                context_prefix = (
                    f"# ì»¨í…ìŠ¤íŠ¸:\n{request.context}\n\n" if request.context else ""
                )
                return f"{context_prefix}# ìš”ì²­ì‚¬í•­: {prompt}"

    def _map_user_options(
            self, request: CodeGenerationRequest) -> Dict[str, Any]:
        """HAPA ì‚¬ìš©ì ì˜µì…˜ì„ vLLM í˜•ì‹ìœ¼ë¡œ ë§¤í•‘"""
        options = {}

        # í”„ë¡œê·¸ë˜ë° ê¸°ìˆ  ìˆ˜ì¤€ ë§¤í•‘
        if hasattr(request, "programming_level"):
            level_mapping = {
                "beginner": "beginner",
                "intermediate": "intermediate",
                "advanced": "advanced",
                "expert": "advanced",
            }
            options["python_skill_level"] = level_mapping.get(
                request.programming_level, "intermediate"
            )
        else:
            options["python_skill_level"] = "intermediate"

        # ì„¤ëª… ìŠ¤íƒ€ì¼ ë§¤í•‘
        if hasattr(request, "explanation_detail"):
            detail_mapping = {
                "minimal": "brief",
                "standard": "standard",
                "detailed": "detailed",
                "comprehensive": "detailed",
            }
            options["explanation_style"] = detail_mapping.get(
                request.explanation_detail, "standard"
            )
        else:
            options["explanation_style"] = "standard"

        # ì¶”ê°€ ì˜µì…˜ë“¤
        if hasattr(request, "include_comments"):
            options["include_comments"] = request.include_comments

        if hasattr(request, "code_style"):
            options["code_style"] = request.code_style

        return options

    async def generate_code_stream(
        self, request: CodeGenerationRequest, user_id: str
    ) -> AsyncGenerator[str, None]:
        """vLLM ì„œë²„ë¡œë¶€í„° ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± (ê°œì„ ëœ ì²­í¬ ì²˜ë¦¬)"""

        vllm_request = self._prepare_vllm_request(request, user_id)
        
        # ì²­í¬ ë²„í¼ ì´ˆê¸°í™”
        chunk_buffer = ChunkBuffer(
            buffer_size=self.default_buffer_size,
            buffer_timeout=self.default_buffer_timeout
        ) if self.chunk_buffering_enabled else None

        if self.enable_performance_logging:
            print(f"ğŸ”§ [vLLM] ì²­í¬ ë²„í¼ë§ ì„¤ì •: í™œì„±í™”={self.chunk_buffering_enabled}, ë²„í¼í¬ê¸°={self.default_buffer_size}, íƒ€ì„ì•„ì›ƒ={self.default_buffer_timeout}")
            if chunk_buffer:
                print(f"âœ… [vLLM] ChunkBuffer ìƒì„± ì™„ë£Œ")

        try:
            session = await self._get_session()

            async with session.post(
                f"{self.vllm_base_url}/generate/stream", json=vllm_request
            ) as response:

                if response.status != 200:
                    error_msg = f"vLLM ì„œë²„ ì˜¤ë¥˜: HTTP {response.status}"
                    logger.log_system_event(
                        "vLLM ì„œë²„ ì˜¤ë¥˜",
                        "failed",
                        {"user_id": user_id, "status": response.status},
                    )
                    yield f"data: {json.dumps({'error': error_msg})}\n\n"
                    return

                # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë¡œê·¸ (ì„±ëŠ¥ ë¡œê·¸ë¡œ ë¶„ë¥˜)
                if self.enable_performance_logging:
                    logger.log_system_event(
                        "vLLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘",
                        "started",
                        {"user_id": user_id, "model": vllm_request["model_type"]},
                    )

                chunk_count = 0
                total_content_length = 0
                streaming_start_time = time.time()

                async for line in response.content:
                    try:
                        line_text = line.decode("utf-8").strip()

                        if not line_text:
                            continue

                        # Server-Sent Events í˜•ì‹ ì²˜ë¦¬
                        if line_text.startswith("data: "):
                            data_content = line_text[6:]  # 'data: ' ì œê±°

                            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ - ê°•í™”ëœ ì²˜ë¦¬
                            if data_content == "[DONE]" or data_content.strip() == "[DONE]":
                                # ë²„í¼ì— ë‚¨ì€ ë‚´ìš© í”ŒëŸ¬ì‹œ
                                if chunk_buffer:
                                    final_content = chunk_buffer.force_flush()
                                    if final_content and final_content.strip():
                                        chunk_count += 1
                                        total_content_length += len(final_content)
                                        yield f"data: {json.dumps({'text': final_content})}\n\n"
                                        if self.enable_debug_logging:
                                            print(f"ğŸ“¤ [vLLM] ìµœì¢… ë²„í¼ í”ŒëŸ¬ì‹œ: '{final_content[:30]}...'")
                                
                                # ì™„ë£Œ ë¡œê·¸ (ì„±ëŠ¥ ë¡œê·¸ë¡œ ë¶„ë¥˜)
                                streaming_duration = time.time() - streaming_start_time
                                if self.enable_performance_logging:
                                    # ë²„í¼ ì„±ëŠ¥ í†µê³„ í¬í•¨
                                    buffer_stats = chunk_buffer.get_performance_stats() if chunk_buffer else {}
                                    logger.log_system_event(
                                        "vLLM ìŠ¤íŠ¸ë¦¬ë°", "completed", {
                                            "user_id": user_id,
                                            "total_chunks": chunk_count,
                                            "total_content_length": total_content_length,
                                            "duration_seconds": round(streaming_duration, 2),
                                            "avg_chunk_size": round(total_content_length / max(chunk_count, 1), 1),
                                            "buffer_stats": buffer_stats
                                        })
                                    print(f"ğŸ [vLLM] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬, {total_content_length}ì, {streaming_duration:.2f}ì´ˆ")
                                    
                                    # ì„±ëŠ¥ ê²½ê³  í™•ì¸
                                    if buffer_stats.get('small_chunks_ratio', 0) > 30:
                                        print(f"âš ï¸ [vLLM] ì‘ì€ ì²­í¬ ë¹„ìœ¨ ë†’ìŒ: {buffer_stats.get('small_chunks_ratio', 0)}%")
                                
                                yield f"data: [DONE]\n\n"
                                return  # í™•ì‹¤í•œ ì¢…ë£Œ

                            # JSON ë°ì´í„° íŒŒì‹± ë° ì²˜ë¦¬
                            try:
                                parsed_data = json.loads(data_content)
                                
                                # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
                                text_content = parsed_data.get('text', '')
                                if text_content:
                                    total_content_length += len(text_content)
                                    
                                    # ë””ë²„ê·¸ ë¡œê·¸ëŠ” ê°œë°œ í™˜ê²½ì—ì„œë§Œ
                                    if self.enable_debug_logging:
                                        print(f"ğŸ“¥ [vLLM] ì›ì‹œ í…ìŠ¤íŠ¸: '{text_content[:20]}...' (ê¸¸ì´: {len(text_content)})")
                                    
                                    if chunk_buffer:
                                        # ë²„í¼ë§ ì²˜ë¦¬
                                        buffered_content = chunk_buffer.add_chunk(text_content)
                                        if buffered_content and buffered_content.strip():
                                            chunk_count += 1
                                            
                                            # ì„±ëŠ¥ ë¡œê·¸ëŠ” ì„±ëŠ¥ ëª¨ë“œì—ì„œë§Œ
                                            if self.enable_performance_logging:
                                                print(f"ğŸ“¤ [vLLM] ë²„í¼ë§ ì¶œë ¥: #{chunk_count}, ê¸¸ì´={len(buffered_content)}")
                                            
                                            # END_OF_GENERATION ì‹ í˜¸ ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                                            if buffered_content == "[END_OF_GENERATION]":
                                                streaming_duration = time.time() - streaming_start_time
                                                if self.enable_performance_logging:
                                                    logger.log_system_event(
                                                        "vLLM ìŠ¤íŠ¸ë¦¬ë°", "im_end_detected", {
                                                            "user_id": user_id,
                                                            "total_chunks": chunk_count,
                                                            "total_content_length": total_content_length,
                                                            "early_termination": True,
                                                            "duration_seconds": round(streaming_duration, 2)
                                                        })
                                                    print(f"ğŸ›‘ [vLLM] END_OF_GENERATION ì‹ í˜¸ - ì¡°ê¸° ì¢…ë£Œ")
                                                yield f"data: [DONE]\n\n"
                                                return
                                            
                                            # ì²­í¬ ìƒì„¸ ë¡œê·¸ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
                                            if self.enable_chunk_details:
                                                logger.debug(
                                                    f"ì²­í¬ ì „ì†¡: #{chunk_count}, ê¸¸ì´: {len(buffered_content)}"
                                                )
                                            
                                            yield f"data: {json.dumps({'text': buffered_content})}\n\n"
                                        # else: ë²„í¼ë§ ì¤‘ì´ë¯€ë¡œ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ (ë¡œê·¸ ìƒëµ)
                                    else:
                                        # ë²„í¼ë§ ë¹„í™œì„±í™” ì‹œ ì§ì ‘ ì „ì†¡ (í•˜ì§€ë§Œ im_end í† í° ì²´í¬)
                                        if self.enable_debug_logging:
                                            print(f"ğŸš« [vLLM] ë²„í¼ë§ ë¹„í™œì„±í™” - ì§ì ‘ ì „ì†¡")
                                        
                                        # ğŸ¯ ì‹¤ì œ vLLM stop token + íŠ¸ë ì¼€ì´ì…˜ íŒ¨í„´ ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                                        vllm_stop_tokens = [
                                            "\n# --- Generation Complete ---",
                                            "<ï½œfimâ–beginï½œ>",
                                            "<ï½œfimâ–holeï½œ>",
                                            "<ï½œfimâ–endï½œ>",
                                            "<|endoftext|>",
                                            # ğŸš€ ì¶”ê°€ íŠ¸ë ì¼€ì´ì…˜ íŒ¨í„´ë“¤
                                            "#!/usr/bin/env python",
                                            "# --- File Comment",
                                            "# Created on:",
                                            "# Author:",
                                            "# @Author",
                                            "</c>",
                                            "@deepseeks.com",
                                            "_li@",
                                        ]
                                        
                                        detected_stop_token = None
                                        for stop_token in vllm_stop_tokens:
                                            if stop_token in text_content:
                                                detected_stop_token = stop_token
                                                break
                                        
                                        if detected_stop_token:
                                            streaming_duration = time.time() - streaming_start_time
                                            if self.enable_performance_logging:
                                                logger.log_system_event(
                                                    "vLLM ìŠ¤íŠ¸ë¦¬ë°", "vllm_stop_token_detected_direct", {
                                                        "user_id": user_id,
                                                        "total_chunks": chunk_count,
                                                        "total_content_length": total_content_length,
                                                        "early_termination": True,
                                                        "stop_token": detected_stop_token,
                                                        "duration_seconds": round(streaming_duration, 2)
                                                    })
                                                print(f"ğŸ›‘ [vLLM] ì§ì ‘ëª¨ë“œì—ì„œ ì‹¤ì œ stop token ê°ì§€: {detected_stop_token}")
                                            yield f"data: [DONE]\n\n"
                                            return
                                        
                                        chunk_count += 1
                                        yield f"data: {data_content}\n\n"
                                else:
                                    # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ë©”íƒ€ë°ì´í„° ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ì „ì†¡
                                    yield f"data: {data_content}\n\n"
                                    
                            except json.JSONDecodeError:
                                # JSONì´ ì•„ë‹Œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                                if chunk_buffer:
                                    buffered_content = chunk_buffer.add_chunk(data_content)
                                    if buffered_content and buffered_content.strip():
                                        chunk_count += 1
                                        yield f"data: {json.dumps({'text': buffered_content})}\n\n"
                                else:
                                    chunk_count += 1
                                    yield f"data: {data_content}\n\n"

                    except Exception as e:
                        # ë¼ì¸ ì²˜ë¦¬ ì˜¤ë¥˜ëŠ” ë””ë²„ê·¸ í™˜ê²½ì—ì„œë§Œ ë¡œê¹…
                        if self.enable_debug_logging:
                            logger.log_error(e, f"ìŠ¤íŠ¸ë¦¼ ë¼ì¸ ì²˜ë¦¬ - user_id: {user_id}")
                        continue

        except asyncio.TimeoutError:
            error_msg = "vLLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼"
            logger.log_system_event("vLLM ì‘ë‹µ", "timeout", {"user_id": user_id})
            yield f"data: {json.dumps({'error': error_msg})}\n\n"

        except Exception as e:
            error_msg = f"vLLM ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {str(e)}"
            logger.log_error(e, f"vLLM ì„œë²„ ì—°ê²° - user_id: {user_id}")
            yield f"data: {json.dumps({'error': error_msg})}\n\n"

    async def generate_code_sync(
        self, request: CodeGenerationRequest, user_id: str
    ) -> CodeGenerationResponse:
        """ë™ê¸°ì‹ ì½”ë“œ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ëª¨ë‘ ìˆ˜ì§‘)"""

        generated_content = []
        error_occurred = False
        error_message = ""

        async for chunk in self.generate_code_stream(request, user_id):
            try:
                if chunk.startswith("data: "):
                    data_content = chunk[6:].strip()

                    if data_content == "[DONE]":
                        break

                    # JSON íŒŒì‹± ì‹œë„
                    try:
                        data = json.loads(data_content)
                        if "error" in data:
                            error_occurred = True
                            error_message = data["error"]
                            break
                        elif "text" in data:
                            generated_content.append(data["text"])
                        elif isinstance(data, str):
                            generated_content.append(data)
                    except json.JSONDecodeError:
                        # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        generated_content.append(data_content)

            except Exception as e:
                logger.log_error(e, f"ë™ê¸°ì‹ ì‘ë‹µ ì²˜ë¦¬ - user_id: {user_id}")
                error_occurred = True
                error_message = str(e)
                break

        if error_occurred:
            return CodeGenerationResponse(
                success=False,
                generated_code="",
                error_message=error_message,
                model_used=self._map_hapa_to_vllm_model(
                    request.model_type).value,
                processing_time=0,
                token_usage={
                    "total_tokens": 0},
            )

        final_code = "".join(generated_content)

        return CodeGenerationResponse(
            success=True,
            generated_code=final_code,
            model_used=self._map_hapa_to_vllm_model(request.model_type).value,
            processing_time=0,  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° í•„ìš”
            token_usage={"total_tokens": len(final_code.split())},  # ê·¼ì‚¬ì¹˜
        )

    async def close(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()

    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ì„¸ì…˜ ì •ë¦¬"""
        if hasattr(
                self,
                "session") and self.session and not self.session.closed:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°ì—ë§Œ ì •ë¦¬
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(self.session.close())
            except RuntimeError:
                pass


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
vllm_service = VLLMIntegrationService()
