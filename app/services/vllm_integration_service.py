"""
vLLM ë©€í‹° LoRA ì„œë²„ í†µí•© ì„œë¹„ìŠ¤ (ì ì‘í˜• ì‹œìŠ¤í…œ ì—…ê·¸ë ˆì´ë“œ)
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- ì ì‘í˜• ì²­í¬ ë²„í¼ ì‹œìŠ¤í…œ
- ì§€ëŠ¥ì  Stop Token ê°ì§€
- ìš”ì²­ ë³µì¡ë„ë³„ ë™ì  ìµœì í™”
"""

import asyncio
import json
import re
import time
import ast
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple

import aiohttp

from app.core.config import settings

import logging

from app.schemas.code_generation import CodeGenerationRequest, CodeGenerationResponse
from .adaptive_chunk_buffer import AdaptiveChunkBuffer, IntelligentStopTokenDetector, create_adaptive_system

logger = logging.getLogger(__name__)


# ğŸ›¡ï¸ ì½”ë“œ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ - ê¹¨ì§„ ì½”ë“œ ë°©ì§€
class CodeQualityValidator:
    """Python ì½”ë“œ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.validation_enabled = True
        self.strict_mode = True  # ì—„ê²©í•œ ê²€ì¦ ëª¨ë“œ
        
    def validate_code_chunk(self, code: str) -> Dict[str, Any]:
        """ì½”ë“œ ì²­í¬ í’ˆì§ˆ ê²€ì¦"""
        if not self.validation_enabled or not code.strip():
            return {"valid": True, "issues": [], "confidence": 1.0}
            
        issues = []
        confidence = 1.0
        
        # ğŸ” 1. ê¸°ë³¸ êµ¬ë¬¸ ê²€ì¦
        syntax_issues = self._check_basic_syntax(code)
        if syntax_issues:
            issues.extend(syntax_issues)
            confidence -= 0.3
            
        # ğŸ” 2. ê´„í˜¸ ê· í˜• ê²€ì¦
        balance_issues = self._check_bracket_balance(code)
        if balance_issues:
            issues.extend(balance_issues)
            confidence -= 0.2
            
        # ğŸ” 3. ë¬¸ìì—´ ê· í˜• ê²€ì¦
        quote_issues = self._check_quote_balance(code)
        if quote_issues:
            issues.extend(quote_issues)
            confidence -= 0.2
            
        # ğŸ” 4. AST íŒŒì‹± ì‹œë„ (ì™„ì „ì„± ê²€ì¦)
        ast_issues = self._check_ast_validity(code)
        if ast_issues:
            issues.extend(ast_issues)
            confidence -= 0.3
            
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "confidence": max(0.0, confidence),
            "code_length": len(code),
            "line_count": code.count('\n') + 1
        }
    
    def _check_basic_syntax(self, code: str) -> List[str]:
        """ê¸°ë³¸ êµ¬ë¬¸ ì˜¤ë¥˜ ê²€ì‚¬"""
        issues = []
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ë“¤
        suspicious_patterns = [
            (r'print\(["\'][^"\']*["\']["\']', "print() í•¨ìˆ˜ì˜ ì˜ëª»ëœ ë”°ì˜´í‘œ íŒ¨í„´"),
            (r'["\'][^"\']*\([^)]*["\']', "í•¨ìˆ˜ í˜¸ì¶œ ë‚´ë¶€ì˜ ì˜ëª»ëœ ë”°ì˜´í‘œ"),
            (r'\([^)]*\([^)]*["\'][^"\']*$', "ë¯¸ì™„ì„±ëœ ì¤‘ì²© í•¨ìˆ˜ í˜¸ì¶œ"),
            (r'^[^=]*=[^=]*\([^)]*$', "ë¯¸ì™„ì„±ëœ í•¨ìˆ˜ í• ë‹¹"),
        ]
        
        for pattern, issue_desc in suspicious_patterns:
            if re.search(pattern, code):
                issues.append(issue_desc)
                
        return issues
    
    def _check_bracket_balance(self, code: str) -> List[str]:
        """ê´„í˜¸ ê· í˜• ê²€ì‚¬"""
        issues = []
        
        brackets = {'(': ')', '[': ']', '{': '}'}
        stack = []
        
        for char in code:
            if char in brackets:
                stack.append(char)
            elif char in brackets.values():
                if not stack:
                    issues.append("ë‹«ëŠ” ê´„í˜¸ê°€ ì—¬ëŠ” ê´„í˜¸ë³´ë‹¤ ë§ìŒ")
                    break
                last_open = stack.pop()
                if brackets[last_open] != char:
                    issues.append(f"ê´„í˜¸ íƒ€ì… ë¶ˆì¼ì¹˜: {last_open} vs {char}")
                    
        if stack:
            issues.append(f"ë‹«íˆì§€ ì•Šì€ ê´„í˜¸: {len(stack)}ê°œ")
            
        return issues
    
    def _check_quote_balance(self, code: str) -> List[str]:
        """ë”°ì˜´í‘œ ê· í˜• ê²€ì‚¬"""
        issues = []
        
        single_quotes = code.count("'")
        double_quotes = code.count('"')
        
        if single_quotes % 2 != 0:
            issues.append("í™€ìˆ˜ ê°œì˜ ë‹¨ì¼ ë”°ì˜´í‘œ")
        if double_quotes % 2 != 0:
            issues.append("í™€ìˆ˜ ê°œì˜ ì´ì¤‘ ë”°ì˜´í‘œ")
            
        return issues
    
    def _check_ast_validity(self, code: str) -> List[str]:
        """AST íŒŒì‹±ì„ í†µí•œ êµ¬ë¬¸ ì™„ì „ì„± ê²€ì‚¬"""
        issues = []
        
        try:
            ast.parse(code)
        except SyntaxError as e:
            issues.append(f"êµ¬ë¬¸ ì˜¤ë¥˜: {e.msg}")
        except Exception as e:
            issues.append(f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            
        return issues
    
    async def check_health(self) -> Dict[str, Any]:
        """í˜¸í™˜ì„±ì„ ìœ„í•œ check_health ë©”ì„œë“œ (health_checkì˜ í™•ì¥ ë²„ì „)"""
        try:
            is_healthy = await self.health_check()
            return {
                "status": "healthy" if is_healthy else "unhealthy",
                "timestamp": time.time(),
                "details": {
                    "connected": self.is_connected,
                    "total_requests": self.total_requests,
                    "success_rate": self.successful_requests / max(self.total_requests, 1) * 100
                }
            }
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì²´í¬ ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "timestamp": time.time(),
                "error": str(e)
            }
    
    def suggest_fix(self, code: str, issues: List[str]) -> str:
        """ê°„ë‹¨í•œ ìë™ ìˆ˜ì • ì œì•ˆ"""
        fixed_code = code
        
        # ê°„ë‹¨í•œ ìˆ˜ì •ë“¤
        for issue in issues:
            if "ë‹«íˆì§€ ì•Šì€ ê´„í˜¸" in issue:
                # ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°
                open_count = fixed_code.count('(')
                close_count = fixed_code.count(')')
                if open_count > close_count:
                    fixed_code += ')' * (open_count - close_count)
                    
            elif "í™€ìˆ˜ ê°œì˜ ë‹¨ì¼ ë”°ì˜´í‘œ" in issue:
                # ë§ˆì§€ë§‰ì— ë”°ì˜´í‘œ ì¶”ê°€
                if fixed_code.count("'") % 2 != 0:
                    fixed_code += "'"
                    
            elif "í™€ìˆ˜ ê°œì˜ ì´ì¤‘ ë”°ì˜´í‘œ" in issue:
                # ë§ˆì§€ë§‰ì— ë”°ì˜´í‘œ ì¶”ê°€
                if fixed_code.count('"') % 2 != 0:
                    fixed_code += '"'
        
        return fixed_code


# ì „ì—­ ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤
code_validator = CodeQualityValidator()


# ğŸ¯ ì‘ë‹µ ë¶„ë¦¬ ì‹œìŠ¤í…œ - ì„¤ëª…ê³¼ ì½”ë“œ êµ¬ë¶„
class ResponseParser:
    """AI ì‘ë‹µì„ ì„¤ëª…ê³¼ ì½”ë“œë¡œ ë¶„ë¦¬í•˜ëŠ” íŒŒì„œ"""
    
    def __init__(self):
        self.code_patterns = [
            r'```python\s*(.*?)\s*```',  # Python ì½”ë“œ ë¸”ë¡
            r'```\s*(.*?)\s*```',        # ì¼ë°˜ ì½”ë“œ ë¸”ë¡
            r'def\s+\w+.*?(?=\n\n|\Z)',  # í•¨ìˆ˜ ì •ì˜
            r'class\s+\w+.*?(?=\n\n|\Z)', # í´ë˜ìŠ¤ ì •ì˜
            r'print\s*\([^)]*\)',        # print ë¬¸
            r'^\s*[a-zA-Z_]\w*\s*=.*',   # ë³€ìˆ˜ í• ë‹¹
        ]
        
        self.explanation_markers = [
            'ì´ ì½”ë“œëŠ”', 'ì„¤ëª…:', 'ë‹¤ìŒê³¼ ê°™ì´', 'ì‘ë™ ë°©ì‹:',
            'ì£¼ìš” ê¸°ëŠ¥:', 'ì‚¬ìš©ë²•:', 'ì˜ˆì‹œ:', 'ì°¸ê³ :',
            'This code', 'Explanation:', 'How it works:',
            'Usage:', 'Example:', 'Note:'
        ]
    
    def parse_response(self, raw_response: str) -> Dict[str, str]:
        """ì‘ë‹µì„ ì„¤ëª…ê³¼ ì½”ë“œë¡œ ë¶„ë¦¬"""
        if not raw_response or not raw_response.strip():
            return {"explanation": "", "code": ""}
        
        # 1. ì½”ë“œ ë¸”ë¡ íƒì§€ ë° ì¶”ì¶œ
        code_blocks = self._extract_code_blocks(raw_response)
        
        # 2. ì„¤ëª… ë¶€ë¶„ ì¶”ì¶œ
        explanation_text = self._extract_explanation(raw_response, code_blocks)
        
        # 3. ìµœì¢… ì •ë¦¬
        final_code = self._merge_code_blocks(code_blocks)
        final_explanation = self._clean_explanation(explanation_text)
        
        return {
            "explanation": final_explanation,
            "code": final_code,
            "metadata": {
                "code_blocks_found": len(code_blocks),
                "has_explanation": bool(final_explanation),
                "parsing_confidence": self._calculate_confidence(final_explanation, final_code)
            }
        }
    
    def _extract_code_blocks(self, text: str) -> List[str]:
        """ì½”ë“œ ë¸”ë¡ë“¤ì„ ì¶”ì¶œ"""
        code_blocks = []
        
        # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        for pattern in [r'```python\s*(.*?)\s*```', r'```\s*(.*?)\s*```']:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            code_blocks.extend([match.strip() for match in matches if match.strip()])
        
        # 2. ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì¸ë¼ì¸ ì½”ë“œ íŒ¨í„´ ì°¾ê¸°
        if not code_blocks:
            for pattern in self.code_patterns[2:]:  # í•¨ìˆ˜, í´ë˜ìŠ¤, print ë“±
                matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
                code_blocks.extend([match.strip() for match in matches if match.strip()])
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_blocks = []
        for block in code_blocks:
            if block and block not in unique_blocks:
                # ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸)
                if len(block) >= 3:
                    unique_blocks.append(block)
        
        return unique_blocks
    
    def _extract_explanation(self, text: str, code_blocks: List[str]) -> str:
        """ì„¤ëª… ë¶€ë¶„ì„ ì¶”ì¶œ"""
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        explanation_text = text
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        for pattern in [r'```python.*?```', r'```.*?```']:
            explanation_text = re.sub(pattern, '', explanation_text, flags=re.DOTALL | re.IGNORECASE)
        
        # ì¸ë¼ì¸ ì½”ë“œ ì œê±°
        for block in code_blocks:
            explanation_text = explanation_text.replace(block, '')
        
        # ì„¤ëª… ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œ
        lines = explanation_text.split('\n')
        explanation_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ëª…í™•í•œ ì„¤ëª… íŒ¨í„´ í™•ì¸
            is_explanation = any(marker in line for marker in self.explanation_markers)
            
            # ì½”ë“œê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            is_not_code = not any(pattern in line for pattern in ['def ', 'class ', 'import ', 'print(', '='])
            
            if is_explanation or (is_not_code and len(line) > 10):
                explanation_lines.append(line)
        
        return '\n'.join(explanation_lines)
    
    def _merge_code_blocks(self, code_blocks: List[str]) -> str:
        """ì½”ë“œ ë¸”ë¡ë“¤ì„ ë³‘í•©"""
        if not code_blocks:
            return ""
        
        # ì¤‘ë³µ ì œê±°
        unique_blocks = []
        for block in code_blocks:
            if block not in unique_blocks:
                unique_blocks.append(block)
        
        # ë¸”ë¡ì´ í•˜ë‚˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(unique_blocks) == 1:
            return unique_blocks[0]
        
        # ì—¬ëŸ¬ ë¸”ë¡ì´ ìˆìœ¼ë©´ ì ì ˆíˆ ë³‘í•©
        return '\n\n'.join(unique_blocks)
    
    def _clean_explanation(self, explanation: str) -> str:
        """ì„¤ëª… í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not explanation:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ë§ˆì»¤ ì œê±°
        explanation = re.sub(r'^[#*\-=]+\s*', '', explanation, flags=re.MULTILINE)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        explanation = re.sub(r'\n\s*\n\s*\n', '\n\n', explanation)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        explanation = explanation.strip()
        
        return explanation
    
    def _calculate_confidence(self, explanation: str, code: str) -> float:
        """íŒŒì‹± ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.5  # ê¸°ë³¸ê°’
        
        # ì½”ë“œê°€ ìˆìœ¼ë©´ +0.3
        if code and len(code) > 10:
            confidence += 0.3
        
        # ì„¤ëª…ì´ ìˆìœ¼ë©´ +0.2
        if explanation and len(explanation) > 20:
            confidence += 0.2
        
        # ëª…í™•í•œ êµ¬ë¶„ì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
        if explanation and code:
            if any(marker in explanation for marker in self.explanation_markers):
                confidence += 0.1
        
        return min(1.0, confidence)


# ì „ì—­ íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤
response_parser = ResponseParser()


class VLLMIntegrationService:
    """ì ì‘í˜• vLLM í†µí•© ì„œë¹„ìŠ¤"""

    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.base_url = settings.VLLM_SERVER_URL
        self.session: Optional[aiohttp.ClientSession] = None
        self.is_connected = False
        self.connection_retries = 0
        self.max_retries = 3
        
        # ì ì‘í˜• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.adaptive_buffer, self.stop_detector = create_adaptive_system()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.avg_response_time = 0.0
        
        logger.info("vLLM í†µí•© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì ì‘í˜• ëª¨ë“œ)")

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.connect()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.disconnect()

    async def connect(self):
        """vLLM ì„œë²„ ì—°ê²°"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=30)
            connector = aiohttp.TCPConnector(
                ssl=False,  # SSL ì™„ì „ ë¹„í™œì„±í™”
                limit=100,
                limit_per_host=30,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
        
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector
            )
        
        try:
            async with self.session.get(f"{self.base_url}/health") as response:
                if response.status == 200:
                    self.is_connected = True
                    self.connection_retries = 0
                    logger.info("vLLM ì„œë²„ ì—°ê²° ì„±ê³µ")
                else:
                    raise aiohttp.ClientError(f"Health check failed: {response.status}")
        except Exception as e:
            self.is_connected = False
            self.connection_retries += 1
            logger.error(f"vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {self.connection_retries}/{self.max_retries}): {e}")
            
            if self.connection_retries < self.max_retries:
                await asyncio.sleep(2 ** self.connection_retries)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                await self.connect()
            else:
                raise ConnectionError("vLLM ì„œë²„ ì—°ê²° ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

    async def disconnect(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
            self.session = None
        self.is_connected = False
        logger.info("vLLM ì„œë²„ ì—°ê²° ì¢…ë£Œ")

    def _build_enhanced_prompt(self, request: CodeGenerationRequest, user_preferences: Optional[Dict[str, Any]] = None) -> str:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°œì¸í™” ì •ë³´ ë°˜ì˜)"""
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ê³ í’ˆì§ˆ Python ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
1. ì™„ì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì‘ì„±
2. ì ì ˆí•œ ì£¼ì„ê³¼ ë¬¸ì„œí™” í¬í•¨
3. íŒŒì´ì¬ ìµœì„ ì˜ ê´€ë¡€(best practices) ì¤€ìˆ˜
4. ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ì½”ë“œ ì‘ì„±"""

        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        context_section = ""
        if request.context and request.context.strip():
            context_section = f"\n\nê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸:\n```python\n{request.context}\n```"

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì¡°í•©
        base_prompt = f"""{system_prompt}

ì‚¬ìš©ì ìš”ì²­: {request.prompt}{context_section}

Python ì½”ë“œ:
```python"""

        # ì‚¬ìš©ì ê°œì¸í™” ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì ìš©
        if user_preferences:
            try:
                # ëŸ°íƒ€ì„ì—ë§Œ import
                from app.api.endpoints.code_generation import build_personalized_prompt
                personalized_prompt = build_personalized_prompt(base_prompt, user_preferences)
                logger.info(f"ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸ ì ìš©ë¨: skill_level={user_preferences.get('skill_level', 'unknown')}")
                return personalized_prompt
            except ImportError as e:
                logger.warning(f"ê°œì¸í™” ëª¨ë“ˆ import ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")
                return base_prompt
        else:
            return base_prompt

    def _prepare_vllm_payload(self, request: CodeGenerationRequest, complexity, user_id: str, user_preferences: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """vLLM ìš”ì²­ í˜ì´ë¡œë“œ ì¤€ë¹„ (ê°œì¸í™” ì •ë³´ ë°˜ì˜)"""
        
        # ì‚¬ìš©ì ì„ í˜¸ë„ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì¡°ì •
        base_temperature = 0.3
        base_max_tokens = 400
        base_top_p = 0.8
        
        if user_preferences:
            skill_level = user_preferences.get("skill_level", "intermediate")
            code_style = user_preferences.get("code_style", "standard")
            safety_level = user_preferences.get("safety_level", "standard")
            
            # ìŠ¤í‚¬ ë ˆë²¨ì— ë”°ë¥¸ í† í° ìˆ˜ ì¡°ì •
            if skill_level == "beginner":
                base_max_tokens = int(base_max_tokens * 1.5)  # ë” ìƒì„¸í•œ ì„¤ëª…
            elif skill_level == "expert":
                base_max_tokens = int(base_max_tokens * 0.8)  # ê°„ê²°í•œ ì½”ë“œ
            
            # ì½”ë“œ ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ temperature ì¡°ì •
            if code_style == "concise":
                base_temperature = max(base_temperature * 0.8, 0.1)
            elif code_style == "detailed":
                base_temperature = min(base_temperature * 1.2, 0.4)
            
            # ì•ˆì „ì„± ë ˆë²¨ì— ë”°ë¥¸ top_p ì¡°ì •
            if safety_level == "enhanced":
                base_top_p = max(base_top_p * 0.9, 0.7)
            elif safety_level == "minimal":
                base_top_p = min(base_top_p * 1.1, 0.95)
        
        # ë³µì¡ë„ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„° ì¡°ì •
        if complexity.value == 'simple':
            temperature = base_temperature
            max_tokens = base_max_tokens
            top_p = base_top_p
        elif complexity.value == 'medium':
            temperature = min(base_temperature * 1.3, 0.5)
            max_tokens = int(base_max_tokens * 1.5)
            top_p = min(base_top_p * 1.1, 0.9)
        else:  # complex
            temperature = min(base_temperature * 1.6, 0.7)
            max_tokens = int(base_max_tokens * 2.0)
            top_p = min(base_top_p * 1.2, 0.95)
        
        # ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhanced_prompt = self._build_enhanced_prompt(request, user_preferences)
        
        # vLLM ì„œë²„ê°€ ê¸°ëŒ€í•˜ëŠ” ìŠ¤í‚¤ë§ˆì— ë§ì¶° í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "user_id": int(user_id.split("_")[-1]) if user_id and "_" in user_id else 1,  # user_id ì¶”ì¶œ ë˜ëŠ” ê¸°ë³¸ê°’
            "prompt": enhanced_prompt,
            "model_type": self._map_model_type_to_vllm(request.model_type),
            "user_select_options": user_preferences or {},  # í•„ìˆ˜ í•„ë“œ
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
        }
        
        if user_preferences:
            logger.info(f"ê°œì¸í™”ëœ vLLM í˜ì´ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ: skill={user_preferences.get('skill_level')}, style={user_preferences.get('code_style')}, complexity={complexity.value}")
        else:
            logger.debug(f"ê¸°ë³¸ vLLM í˜ì´ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ (ë³µì¡ë„: {complexity.value})")
        
        return payload

    def _map_model_type_to_vllm(self, model_type) -> str:
        """Backend ModelTypeì„ vLLM ì„œë²„ì˜ model_typeìœ¼ë¡œ ë§¤í•‘"""
        from app.schemas.code_generation import ModelType
        
        mapping = {
            ModelType.CODE_GENERATION: "prompt",
            ModelType.CODE_COMPLETION: "autocomplete",
            ModelType.CODE_EXPLANATION: "comment",
            ModelType.BUG_FIX: "error_fix",
            ModelType.CODE_OPTIMIZATION: "prompt",
            ModelType.UNIT_TEST_GENERATION: "prompt",
            ModelType.CODE_REVIEW: "comment",
            ModelType.DOCUMENTATION: "comment"
        }
        
        return mapping.get(model_type, "prompt")

    async def generate_code_streaming(
        self,
        request: CodeGenerationRequest,
        user_id: str,
        user_preferences: Optional[Dict[str, Any]] = None,
        chunk_callback: Optional[callable] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ì ì‘í˜• ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± (ê°œì¸í™” ì •ë³´ ë°˜ì˜)"""
        
        start_time = time.time()
        self.total_requests += 1
        accumulated_content = ""  # ì „ì²´ ì‘ë‹µ ëˆ„ì 
        
        try:
            # ì ì‘í˜• ë²„í¼ ì„¤ì •
            complexity = self.adaptive_buffer.configure_for_request(
                request.prompt, 
                request.context
            )
            
            # ê°œì¸í™”ëœ ìš”ì²­ ì¤€ë¹„
            payload = self._prepare_vllm_payload(request, complexity, user_id, user_preferences)
            
            personalization_info = f"(ê°œì¸í™”: {bool(user_preferences)})" if user_preferences else "(ê¸°ë³¸ ëª¨ë“œ)"
            logger.info(f"êµ¬ì¡°í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì‹œì‘ {personalization_info} (ë³µì¡ë„: {complexity.value})")
            
            if not self.is_connected:
                await self.connect()

            async with self.session.post(
                f"{self.base_url}/generate/stream",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:

                if response.status != 200:
                    error_text = await response.text()
                    raise aiohttp.ClientError(f"vLLM API ì˜¤ë¥˜ {response.status}: {error_text}")

                async for line in response.content:
                    line_text = line.decode('utf-8').strip()

                    if not line_text or not line_text.startswith('data: '):
                            continue

                    if line_text == 'data: [DONE]':
                        # ìµœì¢… ì‘ë‹µ ë¶„ë¦¬ ë° ì „ì†¡
                        if accumulated_content.strip():
                            parsed_response = response_parser.parse_response(accumulated_content)
                            
                            # ì„¤ëª… ì²­í¬ ì „ì†¡
                            if parsed_response["explanation"]:
                                yield {
                                    "type": "explanation",
                                    "content": parsed_response["explanation"],
                                    "is_complete": False,
                                    "metadata": {
                                        "chunk_type": "explanation",
                                        "complexity": complexity.value,
                                        "personalized": bool(user_preferences)
                                    }
                                }
                            
                            # ì½”ë“œ ì²­í¬ ì „ì†¡
                            if parsed_response["code"]:
                                yield {
                                    "type": "code",
                                    "content": parsed_response["code"],
                                    "is_complete": False,
                                    "metadata": {
                                        "chunk_type": "code",
                                        "complexity": complexity.value,
                                        "parsing_confidence": parsed_response.get("metadata", {}).get("parsing_confidence", 0.8),
                                        "personalized": bool(user_preferences),
                                        "user_preferences": user_preferences if user_preferences else {}
                                    }
                                }
                            
                            # ì™„ë£Œ ì‹ í˜¸
                            yield {
                                "type": "done",
                                "content": "",
                                "is_complete": True,
                                "metadata": {
                                    **self.adaptive_buffer.get_metrics(),
                                    **parsed_response.get("metadata", {}),
                                    "personalized": bool(user_preferences),
                                    "personalization_applied": user_preferences if user_preferences else None
                                }
                            }
                        break
                    
                    # JSON íŒŒì‹± ë° ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    try:
                        json_data = json.loads(line_text[6:])  # 'data: ' ì œê±°
                        
                        if 'choices' in json_data and json_data['choices']:
                            choice = json_data['choices'][0]
                            
                            if 'delta' in choice and 'content' in choice['delta']:
                                content = choice['delta']['content']
                                accumulated_content += content  # ì „ì²´ ì‘ë‹µì— ëˆ„ì 
                                
                                # ì ì‘í˜• ë²„í¼ì— ì¶”ê°€
                                ready_chunks = self.adaptive_buffer.add_chunk(content)
                                
                                # ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ (ê°œì¸í™” ë©”íƒ€ë°ì´í„° í¬í•¨)
                                for chunk in ready_chunks:
                                    if chunk.strip():
                                        # Stop token ê°ì§€
                                        should_stop, reason = self.stop_detector.should_stop(
                                            chunk, 
                                            {'request_type': complexity.value}
                                        )
                                        
                                        if should_stop:
                                            logger.info(f"Stop token ê°ì§€: {reason}")
                                            # ì¡°ê¸° ì¢…ë£Œ ì‹œì—ë„ ì‘ë‹µ ë¶„ë¦¬ ì ìš©
                                            if accumulated_content.strip():
                                                parsed_response = response_parser.parse_response(accumulated_content)
                                                
                                                if parsed_response["explanation"]:
                                                    yield {
                                                        "type": "explanation",
                                                        "content": parsed_response["explanation"],
                                                        "is_complete": True,
                                                        "stop_reason": reason,
                                                        "personalized": bool(user_preferences)
                                                    }
                                                
                                                if parsed_response["code"]:
                                                    yield {
                                                        "type": "code",
                                                        "content": parsed_response["code"],
                                                        "is_complete": True,
                                                        "stop_reason": reason,
                                                        "personalized": bool(user_preferences)
                                                    }
                                            return
                                        
                                        # ì¼ë°˜ ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡ (ê°œì¸í™” ë©”íƒ€ë°ì´í„° í¬í•¨)
                                        yield {
                                            "type": "token",
                                            "content": chunk,
                                            "is_complete": False,
                                            "metadata": {
                                                "complexity": complexity.value,
                                                "chunk_size": len(chunk),
                                                "is_preview": True,
                                                "personalized": bool(user_preferences)
                                            }
                                        }
                                        
                                        # ì½œë°± í˜¸ì¶œ
                                        if chunk_callback:
                                            await chunk_callback(chunk)
                                            
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}, ë¼ì¸: {line_text}")
                        continue
                    except Exception as e:
                        logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

            # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            self.successful_requests += 1
            response_time = time.time() - start_time
            self._update_metrics(response_time, True)
            
            personalization_msg = f" (ê°œì¸í™” ì ìš©: {user_preferences.get('skill_level', 'unknown')})" if user_preferences else ""
            logger.info(f"êµ¬ì¡°í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ{personalization_msg} (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")

        except aiohttp.ClientConnectorError as e:
            self.failed_requests += 1
            response_time = time.time() - start_time
            self._update_metrics(response_time, False)
            
            logger.error(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ClientConnectorError): {e}")
            logger.error(f"âŒ ì‹œë„í•œ URL: {self.base_url}/generate/stream")
            logger.error(f"âŒ ì—°ê²° ìƒíƒœ: is_connected={self.is_connected}")
            
            yield {
                "type": "error", 
                "content": f"vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                "is_complete": True,
                "error": f"ClientConnectorError: {str(e)}",
                "personalized": bool(user_preferences)
            }
        except aiohttp.ClientResponseError as e:
            self.failed_requests += 1
            response_time = time.time() - start_time
            self._update_metrics(response_time, False)
            
            logger.error(f"âŒ vLLM API ì‘ë‹µ ì˜¤ë¥˜: {e.status} - {e.message}")
            logger.error(f"âŒ URL: {e.request_info.url}")
            
            yield {
                "type": "error",
                "content": f"vLLM API ì‘ë‹µ ì˜¤ë¥˜: {e.status}",
                "is_complete": True,
                "error": f"ClientResponseError: {str(e)}",
                "personalized": bool(user_preferences)
            }
        except asyncio.TimeoutError as e:
            self.failed_requests += 1
            response_time = time.time() - start_time
            self._update_metrics(response_time, False)
            
            logger.error(f"âŒ vLLM ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ: {e}")
            logger.error(f"âŒ ì‹œë„í•œ URL: {self.base_url}/generate/stream")
            
            yield {
                "type": "error",
                "content": f"vLLM ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ: {str(e)}",
                "is_complete": True,
                "error": f"TimeoutError: {str(e)}",
                "personalized": bool(user_preferences)
            }
        except Exception as e:
            self.failed_requests += 1
            response_time = time.time() - start_time
            self._update_metrics(response_time, False)
            
            logger.error(f"âŒ êµ¬ì¡°í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì˜¤ë¥˜: {type(e).__name__}: {e}")
            logger.error(f"âŒ vLLM URL: {self.base_url}")
            logger.error(f"âŒ ì—°ê²° ìƒíƒœ: is_connected={self.is_connected}")
            
            yield {
                "type": "error",
                "content": f"ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "is_complete": True,
                "error": f"{type(e).__name__}: {str(e)}",
                "personalized": bool(user_preferences)
            }

    async def generate_code_sync(
        self,
        request: CodeGenerationRequest,
        user_id: str,
        user_preferences: Optional[Dict[str, Any]] = None
    ) -> CodeGenerationResponse:
        """ë™ê¸°ì‹ ì½”ë“œ ìƒì„± (ê°œì¸í™” ì •ë³´ ë°˜ì˜)"""
        
        start_time = time.time()
        accumulated_content = ""
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
            async for chunk in self.generate_code_streaming(request, user_id, user_preferences):
                if chunk.get("type") == "token":
                    accumulated_content += chunk.get("content", "")
                elif chunk.get("type") == "done":
                        break
                elif chunk.get("type") == "error":
                    return CodeGenerationResponse(
                        success=False,
                        generated_code="",
                        error_message=chunk.get("content", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"),
                        model_used="vllm",
                        processing_time=time.time() - start_time,
                        token_usage={"total_tokens": 0},
                    )
            
            # ì‘ë‹µ ë¶„ë¦¬
            parsed_response = response_parser.parse_response(accumulated_content)
            
            # ì„±ê³µ ì‘ë‹µ êµ¬ì„±
            response = CodeGenerationResponse(
                success=True,
                generated_code=parsed_response["code"],
                explanation=parsed_response["explanation"],
                model_used="vllm",
                processing_time=time.time() - start_time,
                token_usage={"total_tokens": len(accumulated_content.split())},
                confidence_score=parsed_response.get("metadata", {}).get("parsing_confidence", 0.8)
            )
            
            # ê°œì¸í™” ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if user_preferences:
                if not hasattr(response, 'metadata'):
                    response.metadata = {}
                response.metadata.update({
                    "personalized": True,
                    "user_preferences": user_preferences,
                    "skill_level": user_preferences.get("skill_level", "unknown"),
                    "code_style": user_preferences.get("code_style", "unknown")
                })
            
            return response
            
        except Exception as e:
            logger.error(f"ë™ê¸°ì‹ ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return CodeGenerationResponse(
                success=False,
                generated_code="",
                error_message=f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                model_used="vllm",
                processing_time=time.time() - start_time,
                token_usage={"total_tokens": 0},
            )

    def _update_metrics(self, response_time: float, success: bool):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì´ë™ í‰ê· ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.avg_response_time = (
            self.avg_response_time * (1 - alpha) + response_time * alpha
        )

    def get_service_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        success_rate = (
            self.successful_requests / max(self.total_requests, 1) * 100
        )
        
        return {
            "connected": self.is_connected,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "success_rate": round(success_rate, 2),
            "avg_response_time": round(self.avg_response_time, 2),
            "adaptive_system": {
                "buffer_metrics": self.adaptive_buffer.get_metrics(),
                "current_complexity": self.adaptive_buffer.current_complexity.value if self.adaptive_buffer.current_complexity else None,
            }
        }
    
    async def check_health(self) -> Dict[str, Any]:
        """
        í˜¸í™˜ì„±ì„ ìœ„í•œ check_health ë©”ì„œë“œ
        ê¸°ì¡´ health_check() ë©”ì„œë“œë¥¼ í™•ì¥í•˜ì—¬ ë” ìƒì„¸í•œ ì •ë³´ ì œê³µ
        """
        try:
            # ê¸°ì¡´ health_check() ë©”ì„œë“œ í˜¸ì¶œ
            is_healthy = await self.health_check()
            
            # ìƒì„¸í•œ ìƒíƒœ ì •ë³´ êµ¬ì„±
            status_info = {
                "status": "healthy" if is_healthy else "unhealthy",
                "timestamp": time.time(),
                "details": {
                    "connected": self.is_connected,
                    "base_url": self.base_url,
                    "total_requests": self.total_requests,
                    "successful_requests": self.successful_requests,
                    "failed_requests": self.failed_requests,
                    "connection_retries": self.connection_retries,
                    "max_retries": self.max_retries
                },
                "performance": {
                    "success_rate": (
                        self.successful_requests / max(self.total_requests, 1) * 100
                    ),
                    "avg_response_time": self.avg_response_time
                }
            }
            
            if is_healthy:
                logger.debug("vLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì™„ë£Œ: ì •ìƒ")
            else:
                logger.warning("vLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì™„ë£Œ: ë¹„ì •ìƒ")
                
            return status_info
            
        except Exception as e:
            logger.error(f"vLLM í—¬ìŠ¤ ì²´í¬ ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "timestamp": time.time(),
                "error": str(e),
                "details": {
                    "connected": False,
                    "base_url": self.base_url,
                    "connection_retries": self.connection_retries
                }
            }

    async def health_check(self) -> bool:
        """í—¬ìŠ¤ ì²´í¬"""
        try:
            logger.info(f"ğŸ” vLLM í—¬ìŠ¤ ì²´í¬ ì‹œì‘: {self.base_url}")
            
            if not self.session:
                logger.info("ì„¸ì…˜ì´ ì—†ì–´ì„œ ìƒˆë¡œ ì—°ê²°í•©ë‹ˆë‹¤")
                await self.connect()
            
            health_url = f"{self.base_url}/health"
            logger.info(f"ğŸ” ìš”ì²­ URL: {health_url}")
            
            async with self.session.get(health_url) as response:
                status = response.status
                logger.info(f"ğŸ” vLLM í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ: {status}")
                
                if status == 200:
                    response_text = await response.text()
                    logger.info(f"âœ… vLLM ì„œë²„ ì •ìƒ: {response_text}")
                    return True
                else:
                    logger.warning(f"âŒ vLLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {status}")
                    return False
                    
        except aiohttp.ClientConnectorError as e:
            logger.error(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ClientConnectorError): {e}")
            logger.error(f"âŒ ì‹œë„í•œ URL: {self.base_url}/health")
            return False
        except asyncio.TimeoutError as e:
            logger.error(f"âŒ vLLM ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ vLLM í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return False

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ChunkBuffer = AdaptiveChunkBuffer 

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
vllm_service = VLLMIntegrationService()