import hashlib
import json
import logging
import os
import pickle
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional, Union

import psutil

from app.core.config import settings

logger = logging.getLogger(__name__)


class PersistentCache:
    """ë©”ëª¨ë¦¬ ì œí•œ ë° ê³ ê¸‰ TTL ì •ì±…ì„ ê°€ì§„ ì˜ì†ì  ìºì‹œ ì‹œìŠ¤í…œ"""

    def __init__(
            self,
            cache_dir: str = None,
            max_memory_mb: int = 200):
        # í†µì¼ëœ ë°ì´í„° ê²½ë¡œ ì‚¬ìš©
        if cache_dir is None:
            cache_dir = f"{settings.get_absolute_data_dir}/cache"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.metadata_file = self.cache_dir / "metadata.json"

        # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.max_cache_size = 2000  # ìµœëŒ€ ì—”íŠ¸ë¦¬ ìˆ˜

        # TTL ì •ì±… ì„¤ì •
        self.ttl_policies = {
            "short": 300,  # 5ë¶„ - ìì£¼ ë³€ê²½ë˜ëŠ” ë°ì´í„°
            "medium": 1800,  # 30ë¶„ - ì¼ë°˜ì ì¸ ë°ì´í„°
            "long": 7200,  # 2ì‹œê°„ - ì•ˆì •ì ì¸ ë°ì´í„°
            "extended": 86400,  # 24ì‹œê°„ - ê±°ì˜ ë³€ê²½ë˜ì§€ ì•ŠëŠ” ë°ì´í„°
        }
        self.default_ttl = self.ttl_policies["medium"]

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.metadata = self._load_metadata()
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì¡° ê²€ì¦ ë° ë³´ì •
        self._validate_metadata()

        # ì´ˆê¸°í™” ì‹œ ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
        try:
            self._cleanup_expired()
            self._enforce_memory_limit()
        except Exception as e:
            logger.warning(f"ìºì‹œ ì´ˆê¸°í™” ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨: {e}")

        logger.info(
            f"PersistentCache ì´ˆê¸°í™” ì™„ë£Œ - ë””ë ‰í† ë¦¬: {self.cache_dir}, ë©”ëª¨ë¦¬ ì œí•œ: {max_memory_mb}MB"
        )

    def _load_metadata(self) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        return {
            "entries": {},
            "stats": {
                "hits": 0,
                "misses": 0,
                "total_entries": 0,
                "last_cleanup": time.time(),
            },
        }

    def _validate_metadata(self):
        """ë©”íƒ€ë°ì´í„° êµ¬ì¡° ê²€ì¦ ë° ë³´ì •"""
        try:
            # í•„ìˆ˜ í‚¤ í™•ì¸ ë° ì¶”ê°€
            if "entries" not in self.metadata:
                self.metadata["entries"] = {}
                logger.warning("ë©”íƒ€ë°ì´í„°ì— 'entries' í‚¤ ì¶”ê°€")
            
            if "stats" not in self.metadata:
                self.metadata["stats"] = {
                    "hits": 0,
                    "misses": 0,
                    "total_entries": 0,
                    "last_cleanup": time.time(),
                }
                logger.warning("ë©”íƒ€ë°ì´í„°ì— 'stats' í‚¤ ì¶”ê°€")
            
            # stats í•˜ìœ„ í‚¤ í™•ì¸
            required_stats_keys = ["hits", "misses", "total_entries", "last_cleanup"]
            for key in required_stats_keys:
                if key not in self.metadata["stats"]:
                    self.metadata["stats"][key] = 0 if key != "last_cleanup" else time.time()
                    logger.warning(f"ë©”íƒ€ë°ì´í„° statsì— '{key}' í‚¤ ì¶”ê°€")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self._save_metadata()
            logger.debug("ë©”íƒ€ë°ì´í„° êµ¬ì¡° ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            # ì™„ì „íˆ ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ìƒì„±
            self.metadata = self._load_metadata()

    def _save_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_key_hash(self, key: str) -> str:
        """í‚¤ë¥¼ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜ (ë³´ì•ˆ ê°•í™”: SHA-256 + Salt)"""
        salt = "hapa_cache_salt_2024"
        return hashlib.sha256(f"{salt}:{key}".encode("utf-8")).hexdigest()

    def _get_cache_file_path(self, key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        key_hash = self._generate_key_hash(key)
        return self.cache_dir / f"{key_hash}.cache"

    def set(
            self,
            key: str,
            value: Any,
            ttl: Optional[int] = None,
            policy: str = "medium") -> bool:
        """ìºì‹œì— ê°’ ì €ì¥ (í–¥ìƒëœ TTL ì •ì±… ë° ë©”ëª¨ë¦¬ ì œí•œ ì ìš©)"""
        try:
            # TTL ì •ì±… ì ìš©
            if ttl is None:
                ttl = self.ttl_policies.get(policy, self.default_ttl)

            # ë©”ëª¨ë¦¬ ì œí•œ ì‚¬ì „ ì²´í¬
            if not self._check_memory_before_add(value):
                logger.warning(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {key}")
                return False

            key_hash = self._generate_key_hash(key)
            cache_file = self._get_cache_file_path(key)
            expires_at = time.time() + ttl

            # ê°’ í¬ê¸° ê³„ì‚°
            serialized_value = pickle.dumps(value)
            value_size = len(serialized_value)

            cache_data = {
                "value": value,
                "expires_at": expires_at,
                "created_at": time.time(),
                "last_accessed": time.time(),
                "access_count": 0,
                "size": value_size,
                "ttl_policy": policy,
                "key": key,
            }

            # íŒŒì¼ì— ì €ì¥
            with open(cache_file, "wb") as f:
                pickle.dump(cache_data, f)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.metadata["entries"][key_hash] = {
                "key": key,
                "file_path": str(cache_file),
                "expires_at": expires_at,
                "created_at": cache_data["created_at"],
                "size": value_size,
                "ttl_policy": policy,
            }

            self.metadata["stats"]["total_entries"] = len(
                self.metadata["entries"])
            self._save_metadata()

            # ë©”ëª¨ë¦¬ ì œí•œ ì ìš©
            self._enforce_memory_limit()

            logger.debug(f"ìºì‹œ ì €ì¥ ì™„ë£Œ: {key} (TTL: {ttl}ì´ˆ, Policy: {policy}, Size: {value_size} bytes)")
            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {key} - {e}")
            return False

    def _check_memory_before_add(self, value: Any) -> bool:
        """ë©”ëª¨ë¦¬ ì¶”ê°€ ì „ ìš©ëŸ‰ í™•ì¸"""
        try:
            value_size = len(pickle.dumps(value))
            current_memory = self._calculate_total_memory()

            # ì—¬ìœ  ê³µê°„ì´ ì¶©ë¶„í•œì§€ í™•ì¸ (90% ì„ê³„ê°’)
            if current_memory + value_size > self.max_memory_bytes * 0.9:
                # ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•´ LRU ì •ë¦¬ ì‹œë„
                self._cleanup_lru(target_free_bytes=value_size * 2)

                # ë‹¤ì‹œ í™•ì¸
                current_memory = self._calculate_total_memory()
                if current_memory + value_size > self.max_memory_bytes:
                    return False

            return True
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False

    def _calculate_total_memory(self) -> int:
        """í˜„ì¬ ìºì‹œê°€ ì‚¬ìš©í•˜ëŠ” ì´ ë©”ëª¨ë¦¬ ê³„ì‚°"""
        total_size = sum(
            entry.get("size", 0) for entry in self.metadata["entries"].values()
        )
        return total_size

    def _enforce_memory_limit(self):
        """ë©”ëª¨ë¦¬ ì œí•œ ì ìš© (LRU ê¸°ë°˜ ì •ë¦¬)"""
        try:
            current_memory = self._calculate_total_memory()

            if current_memory > self.max_memory_bytes:
                target_memory = int(self.max_memory_bytes * 0.8)  # 80%ê¹Œì§€ ì¤„ì´ê¸°
                bytes_to_free = current_memory - target_memory

                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ MB ê³„ì‚°
                current_memory_mb = current_memory / 1024 / 1024
                bytes_to_free_mb = bytes_to_free / 1024 / 1024

                logger.warning(f"ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼ ({current_memory_mb:.2f}MB), {bytes_to_free_mb:.2f}MB ì •ë¦¬ ì¤‘...")
                self._cleanup_lru(bytes_to_free)

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì œí•œ ì ìš© ì‹¤íŒ¨: {e}")

    def _cleanup_lru(self, target_free_bytes: int):
        """LRU ê¸°ë°˜ ìºì‹œ ì •ë¦¬"""
        try:
            # ì ‘ê·¼ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
            entries = list(self.metadata["entries"].items())
            entries.sort(key=lambda x: x[1].get("created_at", 0))

            freed_bytes = 0
            removed_count = 0

            for key_hash, entry in entries:
                if freed_bytes >= target_free_bytes:
                    break

                try:
                    freed_bytes += entry.get("size", 0)
                    self.delete(entry["key"])
                    removed_count += 1
                except Exception as e:
                    logger.error(f"LRU ì •ë¦¬ ì¤‘ ì‚­ì œ ì‹¤íŒ¨: {entry['key']} - {e}")

            if removed_count > 0:
                freed_mb = freed_bytes / 1024 / 1024
                logger.info(f"LRU ì •ë¦¬ ì™„ë£Œ: {removed_count}ê°œ í•­ëª©, {freed_mb:.2f}MB í™•ë³´")

        except Exception as e:
            logger.error(f"LRU ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_advanced_stats(self) -> Dict[str, Any]:
        """ê³ ê¸‰ ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        basic_stats = self.get_stats()

        # TTL ì •ì±…ë³„ í†µê³„
        policy_stats = {}
        for policy in self.ttl_policies.keys():
            policy_entries = [
                entry
                for entry in self.metadata["entries"].values()
                if entry.get("ttl_policy") == policy
            ]
            policy_stats[policy] = {
                "count": len(policy_entries),
                "total_size_mb": sum(e.get("size", 0) for e in policy_entries)
                / 1024
                / 1024,
            }

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        current_memory = self._calculate_total_memory()
        memory_usage_percent = (current_memory / self.max_memory_bytes) * 100

        return {
            **basic_stats,
            "memory_management": {
                "current_memory_mb": current_memory /
                1024 /
                1024,
                "max_memory_mb": self.max_memory_bytes /
                1024 /
                1024,
                "memory_usage_percent": memory_usage_percent,
                "memory_status": "HIGH" if memory_usage_percent > 80 else "NORMAL",
            },
            "ttl_policies": self.ttl_policies,
            "policy_statistics": policy_stats,
        }

    def get(self, key: str, default: Any = None) -> Any:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            cache_file = self._get_cache_file_path(key)

            if not cache_file.exists():
                self.metadata["stats"]["misses"] += 1
                self._save_metadata()
                logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {key} (íŒŒì¼ ì—†ìŒ)")
                return default

            # ìºì‹œ ë°ì´í„° ë¡œë“œ
            with open(cache_file, "rb") as f:
                cache_data = pickle.load(f)

            # ë§Œë£Œ í™•ì¸
            if cache_data["expires_at"] < time.time():
                self.delete(key)
                self.metadata["stats"]["misses"] += 1
                self._save_metadata()
                logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {key} (ë§Œë£Œë¨)")
                return default

            # ì ‘ê·¼ íšŸìˆ˜ ì¦ê°€
            cache_data["access_count"] += 1
            cache_data["last_accessed"] = time.time()

            with open(cache_file, "wb") as f:
                pickle.dump(cache_data, f)

            self.metadata["stats"]["hits"] += 1
            self._save_metadata()

            logger.debug(f"ìºì‹œ íˆíŠ¸: {key}")
            return cache_data["value"]

        except Exception as e:
            logger.error(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {key} - {e}")
            self.metadata["stats"]["misses"] += 1
            self._save_metadata()
            return default

    def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ í‚¤ ì‚­ì œ"""
        try:
            cache_file = self._get_cache_file_path(key)
            key_hash = self._generate_key_hash(key)

            # íŒŒì¼ ì‚­ì œ
            if cache_file.exists():
                cache_file.unlink()

            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
            if key_hash in self.metadata["entries"]:
                del self.metadata["entries"][key_hash]
                self.metadata["stats"]["total_entries"] = len(
                    self.metadata["entries"])
                self._save_metadata()

            logger.debug(f"ìºì‹œ ì‚­ì œ ì™„ë£Œ: {key}")
            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {key} - {e}")
            return False

    def exists(self, key: str) -> bool:
        """ìºì‹œ í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        cache_file = self._get_cache_file_path(key)

        if not cache_file.exists():
            return False

        try:
            with open(cache_file, "rb") as f:
                cache_data = pickle.load(f)

            # ë§Œë£Œ í™•ì¸
            if cache_data["expires_at"] < time.time():
                self.delete(key)
                return False

            return True

        except Exception:
            self.delete(key)
            return False

    def clear(self) -> bool:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        try:
            # ëª¨ë“  ìºì‹œ íŒŒì¼ ì‚­ì œ
            for cache_file in self.cache_dir.glob("*.cache"):
                cache_file.unlink()

            # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
            self.metadata = {
                "entries": {},
                "stats": {
                    "hits": 0,
                    "misses": 0,
                    "total_entries": 0,
                    "last_cleanup": time.time(),
                },
            }
            self._save_metadata()

            logger.info("ëª¨ë“  ìºì‹œ ì‚­ì œ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    def _cleanup_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        try:
            current_time = time.time()
            expired_keys = []

            for key_hash, entry in self.metadata["entries"].items():
                if entry["expires_at"] < current_time:
                    expired_keys.append(entry["key"])

            # ë§Œë£Œëœ í‚¤ë“¤ ì‚­ì œ
            for key in expired_keys:
                self.delete(key)

            self.metadata["stats"]["last_cleanup"] = current_time
            self._save_metadata()

            if expired_keys:
                logger.info(f"ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        stats = self.metadata["stats"].copy()

        # íˆíŠ¸ìœ¨ ê³„ì‚°
        total_requests = stats["hits"] + stats["misses"]
        if total_requests > 0:
            stats["hit_rate"] = stats["hits"] / total_requests
        else:
            stats["hit_rate"] = 0.0

        # ìºì‹œ í¬ê¸° ì •ë³´
        total_size = 0
        for entry in self.metadata["entries"].values():
            total_size += entry.get("size", 0)

        stats["total_size_bytes"] = total_size
        stats["total_size_mb"] = total_size / (1024 * 1024)

        return stats

    def get_cache_info(self) -> Dict[str, Any]:
        """ìƒì„¸ ìºì‹œ ì •ë³´ ë°˜í™˜"""
        return {
            "stats": self.get_stats(),
            "settings": {
                "max_cache_size": self.max_cache_size,
                "default_ttl": self.default_ttl,
                "cache_dir": str(self.cache_dir),
            },
            "entries": len(self.metadata["entries"]),
        }


class AdvancedCacheMonitor:
    """ê³ ê¸‰ ìºì‹œ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ"""

    def __init__(self, cache_instance):
        self.cache = cache_instance
        self.monitoring_enabled = True
        self.alert_thresholds = {
            "memory_usage_mb": 500,  # 500MB ì´ˆê³¼ ì‹œ ì•Œë¦¼
            "hit_rate_threshold": 0.6,  # íˆíŠ¸ìœ¨ 60% ë¯¸ë§Œ ì‹œ ì•Œë¦¼
            "max_entries": 2000,  # ì—”íŠ¸ë¦¬ 2000ê°œ ì´ˆê³¼ ì‹œ ì•Œë¦¼
        }
        self.alert_history = []
        self.monitoring_thread = None
        self._start_monitoring()

    def _start_monitoring(self):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘"""
        if self.monitoring_thread is None or not self.monitoring_thread.is_alive():
            self.monitoring_thread = threading.Thread(
                target=self._monitor_loop, daemon=True
            )
            self.monitoring_thread.start()
            logger.info("ìºì‹œ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")

    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„ (30ì´ˆ ê°„ê²©)"""
        while self.monitoring_enabled:
            try:
                self._check_performance()
                time.sleep(30)
            except Exception as e:
                logger.error(f"ìºì‹œ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°

    def _check_performance(self):
        """ì„±ëŠ¥ ì§€í‘œ í™•ì¸ ë° ì•Œë¦¼"""
        try:
            stats = self.cache.get_stats()
        except Exception as e:
            logger.error(f"ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ statsë¡œ fallback
            stats = {
                "hits": 0,
                "misses": 0,
                "total_entries": 0,
                "hit_rate": 0.0,
                "total_size_mb": 0.0
            }

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ì•ˆì „í•œ ì ‘ê·¼)
        memory_mb = stats.get("total_size_mb", 0)
        if memory_mb > self.alert_thresholds["memory_usage_mb"]:
            threshold_mb = self.alert_thresholds['memory_usage_mb']
            self._send_alert(
                "HIGH_MEMORY_USAGE", f"ìºì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {memory_mb:.2f}MBë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (ì„ê³„ê°’: {threshold_mb}MB)")

        # íˆíŠ¸ìœ¨ í™•ì¸ (ì•ˆì „í•œ ì ‘ê·¼)
        hit_rate = stats.get("hit_rate", 0)
        if hit_rate < self.alert_thresholds["hit_rate_threshold"]:
            threshold_rate = self.alert_thresholds['hit_rate_threshold']
            self._send_alert(
                "LOW_HIT_RATE", f"ìºì‹œ íˆíŠ¸ìœ¨ì´ {hit_rate:.2%}ë¡œ ë‚®ìŠµë‹ˆë‹¤ (ì„ê³„ê°’: {threshold_rate:.2%})")

        # ì—”íŠ¸ë¦¬ ìˆ˜ í™•ì¸ (ì•ˆì „í•œ ì ‘ê·¼)
        entry_count = stats.get("total_entries", 0)
        if entry_count > self.alert_thresholds["max_entries"]:
            max_entries = self.alert_thresholds['max_entries']
            self._send_alert(
                "HIGH_ENTRY_COUNT", f"ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜ê°€ {entry_count}ê°œë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (ì„ê³„ê°’: {max_entries}ê°œ)")

        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
        try:
            system_memory = psutil.virtual_memory()
            if system_memory.percent > 85:
                self._send_alert(
                    "HIGH_SYSTEM_MEMORY",
                    f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ {system_memory.percent:.1f}%ì…ë‹ˆë‹¤",
                )
        except Exception as e:
            logger.warning(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")

    def _send_alert(self, alert_type: str, message: str):
        """ì•Œë¦¼ ë°œì†¡ (ë¡œê·¸ ë° íˆìŠ¤í† ë¦¬ ì €ì¥)"""
        alert = {
            "timestamp": datetime.now(),
            "type": alert_type,
            "message": message}

        # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (10ë¶„ ë‚´ ë™ì¼ íƒ€ì… ì•Œë¦¼ ë¬´ì‹œ)
        recent_alerts = [
            a
            for a in self.alert_history
            if a["type"] == alert_type
            and (datetime.now() - a["timestamp"]) < timedelta(minutes=10)
        ]

        if not recent_alerts:
            logger.warning(f"ğŸš¨ [CACHE ALERT] {message}")
            self.alert_history.append(alert)

            # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.alert_history) > 100:
                self.alert_history = self.alert_history[-100:]

    def get_monitoring_status(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ë°˜í™˜"""
        return {
            "monitoring_enabled": self.monitoring_enabled,
            "alert_thresholds": self.alert_thresholds,
            "recent_alerts": self.alert_history[-10:],  # ìµœê·¼ 10ê°œ ì•Œë¦¼
            "thread_alive": (
                self.monitoring_thread.is_alive() if self.monitoring_thread else False
            ),
        }

    def update_thresholds(self, thresholds: Dict[str, Any]):
        """ì•Œë¦¼ ì„ê³„ê°’ ì—…ë°ì´íŠ¸"""
        self.alert_thresholds.update(thresholds)
        logger.info(f"ìºì‹œ ëª¨ë‹ˆí„°ë§ ì„ê³„ê°’ ì—…ë°ì´íŠ¸: {thresholds}")

    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_enabled = False
        logger.info("ìºì‹œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (PersistentCacheë§Œ ì œê³µ - hybrid_cache_service.py ì‚¬ìš© ê¶Œì¥)
persistent_cache = PersistentCache()
cache_monitor = AdvancedCacheMonitor(persistent_cache)

# í¸ì˜ í•¨ìˆ˜ë“¤ì€ ì œê±°ë¨ - hybrid_cache_service.pyì˜ ë‹¨ì¼ ì§„ì…ì  ì‚¬ìš©
