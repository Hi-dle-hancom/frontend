"""
HAPA ë°±ì—”ë“œ ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
Prometheus í˜¸í™˜ ë©”íŠ¸ë¦­ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import time
from typing import Any, Dict, Optional, List
from datetime import datetime, timedelta

import psutil
from fastapi import APIRouter, Response, Query, Depends
from prometheus_client import (
    CONTENT_TYPE_LATEST,
    CollectorRegistry,
    Counter,
    Gauge,
    Histogram,
    generate_latest,
)
from fastapi.security import OAuth2PasswordRequestForm

from app.core.rate_limiter import limiter
from app.core.security import get_api_key, get_current_user
from app.core.structured_logger import StructuredLogger

logger = StructuredLogger("metrics")

router = APIRouter(tags=["metrics"])

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter(
    "hapa_requests_total", "Total number of requests", [
        "method", "endpoint", "status"])

REQUEST_DURATION = Histogram(
    "hapa_request_duration_seconds",
    "Request duration in seconds",
    ["method", "endpoint"],
)

ACTIVE_CONNECTIONS = Gauge(
    "hapa_active_connections",
    "Number of active connections")

CACHE_HITS = Counter("hapa_cache_hits_total", "Total number of cache hits")

CACHE_MISSES = Counter(
    "hapa_cache_misses_total",
    "Total number of cache misses")

AI_MODEL_REQUESTS = Counter(
    "hapa_ai_model_requests_total",
    "Total number of AI model requests",
    ["model_type", "status"],
)

AI_MODEL_RESPONSE_TIME = Histogram(
    "hapa_ai_model_response_time_seconds",
    "AI model response time in seconds",
    ["model_type"],
)

SYSTEM_CPU_USAGE = Gauge(
    "hapa_system_cpu_usage_percent",
    "System CPU usage percentage")

SYSTEM_MEMORY_USAGE = Gauge(
    "hapa_system_memory_usage_bytes", "System memory usage in bytes"
)

SYSTEM_DISK_USAGE = Gauge(
    "hapa_system_disk_usage_bytes",
    "System disk usage in bytes")

ERROR_COUNT = Counter(
    "hapa_errors_total", "Total number of errors", ["error_type", "endpoint"]
)


def update_system_metrics():
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    # CPU ì‚¬ìš©ë¥ 
    cpu_percent = psutil.cpu_percent(interval=1)
    SYSTEM_CPU_USAGE.set(cpu_percent)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    memory = psutil.virtual_memory()
    SYSTEM_MEMORY_USAGE.set(memory.used)

    # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
    disk = psutil.disk_usage("/")
    SYSTEM_DISK_USAGE.set(disk.used)


@router.get("/metrics")
async def get_metrics():
    """
    Prometheus í˜¸í™˜ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        Response: Prometheus ë©”íŠ¸ë¦­ ë°ì´í„°
    """
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    update_system_metrics()

    # Prometheus ë©”íŠ¸ë¦­ ìƒì„±
    metrics_data = generate_latest(REGISTRY)

    return Response(content=metrics_data, media_type=CONTENT_TYPE_LATEST)


@router.get("/health/detailed")
async def get_detailed_health() -> Dict[str, Any]:
    """
    ìƒì„¸í•œ ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        Dict[str, Any]: ìƒì„¸ ìƒíƒœ ì •ë³´
    """
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage("/")

    return {
        "status": "healthy",
        "timestamp": int(time.time()),
        "system": {
            "cpu": {"usage_percent": cpu_percent, "count": psutil.cpu_count()},
            "memory": {
                "total": memory.total,
                "available": memory.available,
                "used": memory.used,
                "percent": memory.percent,
            },
            "disk": {
                "total": disk.total,
                "used": disk.used,
                "free": disk.free,
                "percent": (disk.used / disk.total) * 100,
            },
        },
        "application": {
            "uptime_seconds": time.time() - psutil.Process().create_time(),
            "pid": psutil.Process().pid,
        },
    }


# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
def record_request(method: str, endpoint: str, status: int, duration: float):
    """ìš”ì²­ ë©”íŠ¸ë¦­ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
    REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)


def record_cache_hit():
    """ìºì‹œ íˆíŠ¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    CACHE_HITS.inc()


def record_cache_miss():
    """ìºì‹œ ë¯¸ìŠ¤ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    CACHE_MISSES.inc()


def record_ai_model_request(
        model_type: str,
        status: str,
        response_time: float):
    """AI ëª¨ë¸ ìš”ì²­ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    AI_MODEL_REQUESTS.labels(model_type=model_type, status=status).inc()
    AI_MODEL_RESPONSE_TIME.labels(model_type=model_type).observe(response_time)


def record_error(error_type: str, endpoint: str):
    """ì—ëŸ¬ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    ERROR_COUNT.labels(error_type=error_type, endpoint=endpoint).inc()


def set_active_connections(count: int):
    """í™œì„± ì—°ê²° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    ACTIVE_CONNECTIONS.set(count)


@router.get("/ai-performance", response_model=Dict[str, Any], summary="AI ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ")
@limiter.limit("10/minute")
async def get_ai_performance_metrics(
    time_window_hours: int = Query(default=24, ge=1, le=168, description="ì¡°íšŒ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)"),
    api_key: str = Depends(get_api_key),
    current_user: Dict[str, Any] = Depends(get_current_user),
):
    """
    AI ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§ API
    
    **ì œê³µ ì •ë³´:**
    - ğŸ“Š **ì‘ë‹µ ì‹œê°„ í†µê³„**: í‰ê· , ìµœëŒ€, P95, ì„ê³„ê°’ ì¤€ìˆ˜ìœ¨
    - ğŸš€ **í† í° ìƒì„± ì†ë„**: í‰ê·  tokens/second, ëª¨ë¸ë³„ ì„±ëŠ¥
    - ğŸ¯ **ì„±ê³µë¥  ì¶”ì **: ëª¨ë¸ë³„ ì„±ê³µë¥ , ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
    - âš ï¸ **ì„ê³„ê°’ ìœ„ë°˜**: ì‹¤ì‹œê°„ ì•Œë¦¼, ì„±ëŠ¥ ë“±ê¸‰
    - ğŸ’¡ **ê°œì„  ê¶Œì¥ì‚¬í•­**: AI ê¸°ë°˜ ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
    
    **ì„ê³„ê°’ ê¸°ì¤€:**
    - ì‘ë‹µì‹œê°„: ìš°ìˆ˜(1ì´ˆ), ì–‘í˜¸(2ì´ˆ), í—ˆìš©(5ì´ˆ), ë¶€ì¡±(10ì´ˆ+)
    - í† í°ì†ë„: ìš°ìˆ˜(50+), ì–‘í˜¸(30+), í—ˆìš©(15+), ë¶€ì¡±(5+ tokens/sec)
    - ì„±ê³µë¥ : ìš°ìˆ˜(98%+), ì–‘í˜¸(95%+), í—ˆìš©(90%+), ë¶€ì¡±(80%+)
    """
    try:
        from app.services.performance_profiler import ai_performance_metrics
        from app.services.enhanced_ai_model import enhanced_ai_service
        
        user_id = current_user.get("user_id", "admin")
        
        logger.info(
            f"AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ìš”ì²­",
            extra={
                "user_id": user_id,
                "time_window_hours": time_window_hours,
            }
        )
        
        # ì„±ëŠ¥ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        performance_summary = ai_performance_metrics.get_performance_summary(time_window_hours)
        
        # ë°±ì—”ë“œ ìƒíƒœ ì •ë³´ ì¡°íšŒ
        backend_status = await enhanced_ai_service.get_backend_status()
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            "success": True,
            "time_window": {
                "hours": time_window_hours,
                "start_time": (datetime.now() - timedelta(hours=time_window_hours)).isoformat(),
                "end_time": datetime.now().isoformat()
            },
            "overview": performance_summary["overview"],
            "performance_metrics": {
                "response_time": {
                    "stats": performance_summary["response_time_stats"],
                    "threshold_compliance": performance_summary["threshold_compliance"]["response_time_compliance"],
                    "target": 2.0,
                    "status": _get_metric_status(
                        performance_summary["response_time_stats"].get("avg", 0),
                        "response_time"
                    )
                },
                "token_generation_speed": {
                    "stats": performance_summary["token_speed_stats"],
                    "threshold_compliance": performance_summary["threshold_compliance"]["token_speed_compliance"],
                    "target": 30.0,
                    "status": _get_metric_status(
                        performance_summary["token_speed_stats"].get("avg", 0),
                        "token_speed"
                    )
                },
                "success_rate": {
                    "overall": performance_summary["threshold_compliance"]["success_rate_compliance"],
                    "target": 0.95,
                    "status": _get_metric_status(
                        performance_summary["threshold_compliance"]["success_rate_compliance"],
                        "success_rate"
                    )
                }
            },
            "model_performance": performance_summary["model_performance"],
            "backend_status": {
                "current_backend": backend_status["current_backend"],
                "vllm_available": backend_status["backends"]["vllm"]["available"],
                "backend_type": "vllm_only"
            },
            "alerts": backend_status["performance_metrics"]["alerts"],
            "recommendations": performance_summary["recommendations"],
            "threshold_violations": {
                "total": performance_summary["overview"]["total_violations"],
                "critical": performance_summary["overview"]["critical_violations"]
            }
        }
        
        # ì„±ê³µ ë¡œê¹…
        logger.info(
            f"AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì„±ê³µ",
            extra={
                "user_id": user_id,
                "total_operations": performance_summary["overview"]["total_operations"],
                "total_violations": performance_summary["overview"]["total_violations"],
                "response_time_avg": performance_summary["response_time_stats"].get("avg", 0),
                "token_speed_avg": performance_summary["token_speed_stats"].get("avg", 0),
            }
        )
        
        return response_data
        
    except Exception as e:
        error_msg = f"AI ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        
        logger.error(
            error_msg,
            extra={
                "user_id": user_id,
                "time_window_hours": time_window_hours,
                "exception": str(e),
            }
        )
        
        return {
            "success": False,
            "error_message": "ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "time_window": {
                "hours": time_window_hours,
                "start_time": (datetime.now() - timedelta(hours=time_window_hours)).isoformat(),
                "end_time": datetime.now().isoformat()
            }
        }


def _get_metric_status(value: float, metric_type: str) -> str:
    """ë©”íŠ¸ë¦­ ê°’ì— ë”°ë¥¸ ìƒíƒœ ë°˜í™˜"""
    from app.services.performance_profiler import AIPerformanceThresholds
    
    if metric_type == "response_time":
        if value <= AIPerformanceThresholds.RESPONSE_TIME_EXCELLENT:
            return "excellent"
        elif value <= AIPerformanceThresholds.RESPONSE_TIME_GOOD:
            return "good"
        elif value <= AIPerformanceThresholds.RESPONSE_TIME_ACCEPTABLE:
            return "acceptable"
        else:
            return "poor"
    
    elif metric_type == "token_speed":
        if value >= AIPerformanceThresholds.TOKEN_SPEED_EXCELLENT:
            return "excellent"
        elif value >= AIPerformanceThresholds.TOKEN_SPEED_GOOD:
            return "good"
        elif value >= AIPerformanceThresholds.TOKEN_SPEED_ACCEPTABLE:
            return "acceptable"
        else:
            return "poor"
    
    elif metric_type == "success_rate":
        if value >= AIPerformanceThresholds.SUCCESS_RATE_EXCELLENT:
            return "excellent"
        elif value >= AIPerformanceThresholds.SUCCESS_RATE_GOOD:
            return "good"
        elif value >= AIPerformanceThresholds.SUCCESS_RATE_ACCEPTABLE:
            return "acceptable"
        else:
            return "poor"
    
    return "unknown"


@router.get("/ai-performance/alerts", response_model=Dict[str, Any], summary="AI ì„±ëŠ¥ ì•Œë¦¼ ì¡°íšŒ")
@limiter.limit("30/minute")
async def get_ai_performance_alerts(
    severity: Optional[str] = Query(default=None, description="ì•Œë¦¼ ì‹¬ê°ë„ í•„í„° (critical, warning)"),
    api_key: str = Depends(get_api_key),
    current_user: Dict[str, Any] = Depends(get_current_user),
):
    """
    AI ëª¨ë¸ ì„±ëŠ¥ ì•Œë¦¼ ì¡°íšŒ API
    
    **ì•Œë¦¼ ìœ í˜•:**
    - ğŸ”´ **Critical**: ì‘ë‹µì‹œê°„ 10ì´ˆ+, í† í°ì†ë„ 5 tokens/sec ë¯¸ë§Œ, ì„±ê³µë¥  80% ë¯¸ë§Œ
    - ğŸŸ¡ **Warning**: ì‘ë‹µì‹œê°„ 5ì´ˆ+, í† í°ì†ë„ 15 tokens/sec ë¯¸ë§Œ, ì„±ê³µë¥  90% ë¯¸ë§Œ
    
    **ì•Œë¦¼ ì •ë³´:**
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì„ê³„ê°’ ìœ„ë°˜ 
    - ëª¨ë¸ë³„ ì„±ëŠ¥ ì €í•˜ ê°ì§€
    - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê²½ê³ 
    """
    try:
        from app.services.performance_profiler import ai_performance_metrics
        
        user_id = current_user.get("user_id", "admin")
        
        # ìµœê·¼ 24ì‹œê°„ ìœ„ë°˜ ì‚¬í•­ ì¡°íšŒ
        recent_violations = [
            v for v in ai_performance_metrics.metrics_data["threshold_violations"]
            if (datetime.now() - v["timestamp"]).total_seconds() < 86400  # 24ì‹œê°„
        ]
        
        # ì‹¬ê°ë„ í•„í„°ë§
        filtered_alerts = []
        for violation_record in recent_violations:
            for violation in violation_record["violations"]:
                if severity is None or violation["severity"] == severity:
                    filtered_alerts.append({
                        "timestamp": violation_record["timestamp"].isoformat(),
                        "model": violation_record["model"],
                        "operation": violation_record["operation"],
                        "severity": violation["severity"],
                        "type": violation["type"],
                        "value": violation["value"],
                        "threshold": violation["threshold"],
                        "message": violation["message"]
                    })
        
        # ìµœì‹ ìˆœ ì •ë ¬
        filtered_alerts.sort(key=lambda x: x["timestamp"], reverse=True)
        
        return {
            "success": True,
            "total_alerts": len(filtered_alerts),
            "severity_filter": severity,
            "alerts": filtered_alerts[:50],  # ìµœëŒ€ 50ê°œ ì œí•œ
            "summary": {
                "critical_count": len([a for a in filtered_alerts if a["severity"] == "critical"]),
                "warning_count": len([a for a in filtered_alerts if a["severity"] == "warning"]),
                "most_common_issue": _get_most_common_issue(filtered_alerts)
            }
        }
        
    except Exception as e:
        logger.error(f"AI ì„±ëŠ¥ ì•Œë¦¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error_message": "ì„±ëŠ¥ ì•Œë¦¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            "total_alerts": 0,
            "alerts": []
        }


def _get_most_common_issue(alerts: List[Dict[str, Any]]) -> Optional[str]:
    """ê°€ì¥ ë¹ˆë²ˆí•œ ì„±ëŠ¥ ì´ìŠˆ íƒ€ì… ë°˜í™˜"""
    if not alerts:
        return None
    
    issue_counts = {}
    for alert in alerts:
        issue_type = alert["type"]
        issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
    
    most_common = max(issue_counts.items(), key=lambda x: x[1])
    return most_common[0] if most_common[1] > 0 else None
