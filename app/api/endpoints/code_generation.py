"""
AI ê¸°ë°˜ ì½”ë“œ ìƒì„± API ì—”ë“œí¬ì¸íŠ¸
- vLLM ë©€í‹° LoRA ì„œë²„ì™€ í†µí•©
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
- 4ê°€ì§€ ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™”
- í•œêµ­ì–´/ì˜ì–´ ìë™ ë²ˆì—­ íŒŒì´í”„ë¼ì¸
"""

import json
from datetime import datetime
from typing import Any, Dict, Optional

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse

from app.core.rate_limiter import limiter
from app.core.security import get_api_key, get_current_user
from app.core.structured_logger import StructuredLogger
from app.schemas.code_generation import (
    CodeGenerationRequest,
    CodeGenerationResponse,
    ModelType,
)
from app.services.error_handling_service import error_handling_service
from app.services.vllm_integration_service import vllm_service

router = APIRouter(prefix="/code", tags=["Code Generation"])
logger = StructuredLogger("code_generation_api")


@router.get("/models", summary="ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ëª©ë¡")
@limiter.limit("30/minute")
async def get_available_models(api_key: str = Depends(get_api_key)):
    """
    vLLM ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    **ì§€ì›í•˜ëŠ” ëª¨ë¸ íƒ€ì…:**
    - `autocomplete`: ì½”ë“œ ìë™ì™„ì„± (ë²ˆì—­ ì—†ìŒ, ì˜ì–´ ì…ë ¥ ê¶Œì¥)
    - `prompt`: ì¼ë°˜ ì½”ë“œ ìƒì„± (ì „ì²´ ë²ˆì—­)
    - `comment`: ì£¼ì„/docstring ìƒì„± (ì£¼ì„ë§Œ ë²ˆì—­)
    - `error_fix`: ë²„ê·¸ ìˆ˜ì • (ì „ì²´ ë²ˆì—­)
    """
    try:
        # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        health_status = await vllm_service.check_health()
        if health_status["status"] != "healthy":
            logger.warning(
                "vLLM ì„œë²„ ìƒíƒœ ë¶ˆì•ˆì •", extra={"health_status": health_status}
            )

        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¡°íšŒ
        models_info = await vllm_service.get_available_models()

        # HAPA ëª¨ë¸ íƒ€ì…ê³¼ ë§¤í•‘ ì •ë³´ ì¶”ê°€
        hapa_model_mapping = {
            "autocomplete": {
                "hapa_types": ["CODE_COMPLETION"],
                "description": "ì½”ë“œ ìë™ì™„ì„± (ì˜ì–´ ì…ë ¥ ê¶Œì¥)",
                "translation": "ì—†ìŒ",
            },
            "prompt": {
                "hapa_types": [
                    "CODE_GENERATION",
                    "CODE_OPTIMIZATION",
                    "UNIT_TEST_GENERATION",
                ],
                "description": "ì¼ë°˜ ì½”ë“œ ìƒì„±",
                "translation": "ì „ì²´ ë²ˆì—­",
            },
            "comment": {
                "hapa_types": [
                    "CODE_EXPLANATION",
                    "CODE_REVIEW",
                    "DOCUMENTATION"],
                "description": "ì£¼ì„/ë¬¸ì„œ ìƒì„±",
                "translation": "ì£¼ì„ë§Œ ë²ˆì—­",
            },
            "error_fix": {
                "hapa_types": ["BUG_FIX"],
                "description": "ë²„ê·¸ ìˆ˜ì •",
                "translation": "ì „ì²´ ë²ˆì—­",
            },
        }

        result = {
            "vllm_server_status": health_status["status"],
            "available_models": models_info.get("available_models", []),
            "model_mapping": hapa_model_mapping,
            "server_info": models_info,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì„±ê³µ: {len(result['available_models'])}ê°œ")
        return result

    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500, detail="ëª¨ë¸ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.post("/generate/stream", summary="ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„±")
@limiter.limit("20/minute")
async def generate_code_stream(
    request: CodeGenerationRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(get_api_key),
    current_user: Dict[str, Any] = Depends(get_current_user),
):
    """
    vLLM ì„œë²„ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    **ì§€ì› ê¸°ëŠ¥:**
    - ğŸ”„ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Server-Sent Events í˜•ì‹ìœ¼ë¡œ ì ì§„ì  ì‘ë‹µ
    - ğŸŒ **ìë™ ë²ˆì—­**: ëª¨ë¸ë³„ í•œêµ­ì–´â†’ì˜ì–´ ë²ˆì—­ ì „ëµ
    - ğŸ¯ **ëª¨ë¸ ìµœì í™”**: ìš”ì²­ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìµœì í™”
    - ğŸ“Š **ìƒì„¸ ë¡œê¹…**: ìš”ì²­ ì¶”ì  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

    **ì‘ë‹µ í˜•ì‹:**
    - Content-Type: `text/event-stream`
    - ê° ë°ì´í„° ì²­í¬: `data: <json_data>\\n\\n`
    - ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: `data: [DONE]\\n\\n`
    """

    user_id = current_user.get("user_id", "anonymous")

    try:
        # ìš”ì²­ ë¡œê¹…
        logger.info(
            f"ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± ìš”ì²­",
            extra={
                "user_id": user_id,
                "model_type": request.model_type.value,
                "prompt_length": len(request.prompt),
                "has_context": bool(request.context),
            },
        )

        # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        health_status = await vllm_service.check_health()
        if health_status["status"] != "healthy":
            raise HTTPException(
                status_code=503,
                detail=f"vLLM ì„œë²„ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤: {
                    health_status.get(
                        'error',
                        'Unknown error')}",
            )

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def stream_generator():
            try:
                async for chunk in vllm_service.generate_code_stream(request, user_id):
                    yield chunk

            except Exception as e:
                error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                logger.error(error_msg, extra={"user_id": user_id})

                # ì˜¤ë¥˜ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                error_data = json.dumps({"error": error_msg})
                yield f"data: {error_data}\n\n"
                yield f"data: [DONE]\n\n"

        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        background_tasks.add_task(
            _log_generation_usage,
            user_id,
            request.model_type.value,
            "streaming")

        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}", extra={"user_id": user_id})
        raise HTTPException(
            status_code=500, detail="ì½”ë“œ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        )


@router.post(
    "/generate", response_model=CodeGenerationResponse, summary="ë™ê¸°ì‹ ì½”ë“œ ìƒì„±"
)
@limiter.limit("15/minute")
async def generate_code(
    request: CodeGenerationRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(get_api_key),
    current_user: Dict[str, Any] = Depends(get_current_user),
):
    """
    vLLM ì„œë²„ë¥¼ í†µí•´ ë™ê¸°ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    **íŠ¹ì§•:**
    - ì™„ì „í•œ ì‘ë‹µì„ í•œ ë²ˆì— ë°˜í™˜
    - ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì¢…í•©
    - ìƒì„¸í•œ ë©”íƒ€ë°ì´í„° í¬í•¨
    - ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ ì§€ì›
    """

    user_id = current_user.get("user_id", "anonymous")
    start_time = datetime.now()

    try:
        # ìš”ì²­ ë¡œê¹…
        logger.info(
            f"ë™ê¸°ì‹ ì½”ë“œ ìƒì„± ìš”ì²­",
            extra={
                "user_id": user_id,
                "model_type": request.model_type.value,
                "prompt_length": len(request.prompt),
            },
        )

        # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        health_status = await vllm_service.check_health()
        if health_status["status"] != "healthy":
            return CodeGenerationResponse(
                success=False,
                generated_code="",
                error_message=f"vLLM ì„œë²„ ì‚¬ìš© ë¶ˆê°€: {
                    health_status.get(
                        'error',
                        'Unknown error')}",
                model_used="N/A",
                processing_time=0,
                token_usage={
                    "total_tokens": 0},
            )

        # ì½”ë“œ ìƒì„± ì‹¤í–‰
        response = await vllm_service.generate_code_sync(request, user_id)

        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = (datetime.now() - start_time).total_seconds()
        response.processing_time = processing_time

        # ì„±ê³µ ë¡œê¹…
        if response.success:
            logger.info(
                f"ì½”ë“œ ìƒì„± ì„±ê³µ",
                extra={
                    "user_id": user_id,
                    "model_used": response.model_used,
                    "processing_time": processing_time,
                    "output_length": len(response.generated_code),
                },
            )
        else:
            logger.warning(
                f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨",
                extra={
                    "user_id": user_id,
                    "error": response.error_message,
                    "processing_time": processing_time,
                },
            )

        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        background_tasks.add_task(
            _log_generation_usage,
            user_id,
            request.model_type.value,
            "sync",
            response.success,
            processing_time,
        )

        return response

    except Exception as e:
        processing_time = (datetime.now() - start_time).total_seconds()
        error_msg = f"ë™ê¸°ì‹ ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}"

        logger.error(
            error_msg,
            extra={
                "user_id": user_id,
                "processing_time": processing_time,
                "exception": str(e),
            },
        )

        # ì˜¤ë¥˜ ì‘ë‹µ ë°˜í™˜
        return CodeGenerationResponse(
            success=False,
            generated_code="",
            error_message="ì½”ë“œ ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            model_used="N/A",
            processing_time=processing_time,
            token_usage={"total_tokens": 0},
        )


@router.get("/health", summary="vLLM ì„œë²„ ìƒíƒœ í™•ì¸")
async def check_vllm_health(api_key: str = Depends(get_api_key)):
    """
    vLLM ë©€í‹° LoRA ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    **ë°˜í™˜ ì •ë³´:**
    - ì„œë²„ ìƒíƒœ (healthy/unhealthy/error)
    - ì‘ë‹µ ì‹œê°„
    - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    - ì„œë²„ ì„¸ë¶€ ì •ë³´
    """
    try:
        health_status = await vllm_service.check_health()
        models_info = await vllm_service.get_available_models()

        return {
            "vllm_server": health_status,
            "available_models": models_info.get("available_models", []),
            "server_details": models_info,
            "timestamp": datetime.now().isoformat(),
            "integration_status": "active",
        }

    except Exception as e:
        logger.error(f"vLLM ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        return {
            "vllm_server": {"status": "error", "error": str(e)},
            "available_models": [],
            "server_details": {},
            "timestamp": datetime.now().isoformat(),
            "integration_status": "error",
        }


@router.post("/test", summary="vLLM ì—°ë™ í…ŒìŠ¤íŠ¸")
@limiter.limit("5/minute")
async def test_vllm_integration(
    model_type: ModelType = Query(
        ModelType.CODE_GENERATION, description="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ íƒ€ì…"
    ),
    api_key: str = Depends(get_api_key),
    current_user: Dict[str, Any] = Depends(get_current_user),
):
    """
    vLLM ì„œë²„ì™€ì˜ ì—°ë™ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    **í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
    - ê°„ë‹¨í•œ ì½”ë“œ ìƒì„± ìš”ì²­
    - ì‘ë‹µ ì‹œê°„ ì¸¡ì •
    - ì˜¤ë¥˜ ì²˜ë¦¬ ê²€ì¦
    """

    user_id = current_user.get("user_id", "test_user")

    # í…ŒìŠ¤íŠ¸ ìš”ì²­ ìƒì„±
    test_request = CodeGenerationRequest(
        prompt="íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
        model_type=model_type,
        context="",
        max_tokens=100,
        temperature=0.3,
    )

    try:
        start_time = datetime.now()

        # ë™ê¸°ì‹ ìƒì„± í…ŒìŠ¤íŠ¸
        response = await vllm_service.generate_code_sync(test_request, user_id)

        processing_time = (datetime.now() - start_time).total_seconds()

        test_result = {
            "test_status": "success" if response.success else "failed",
            "response_time_seconds": processing_time,
            "model_used": response.model_used,
            "output_length": len(
                response.generated_code) if response.success else 0,
            "error_message": response.error_message if not response.success else None,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info(f"vLLM ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ", extra=test_result)
        return test_result

    except Exception as e:
        error_result = {
            "test_status": "error",
            "error_message": str(e),
            "timestamp": datetime.now().isoformat(),
        }

        logger.error(f"vLLM ì—°ë™ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return error_result


# === ë‚´ë¶€ ë„ìš°ë¯¸ í•¨ìˆ˜ ===


async def _log_generation_usage(
    user_id: str,
    model_type: str,
    request_type: str,
    success: bool = True,
    processing_time: float = 0,
):
    """ì½”ë“œ ìƒì„± ì‚¬ìš©ëŸ‰ ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
    try:
        usage_data = {
            "user_id": user_id,
            "model_type": model_type,
            "request_type": request_type,
            "success": success,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info("ì½”ë“œ ìƒì„± ì‚¬ìš©ëŸ‰ ê¸°ë¡", extra=usage_data)

        # ì¶”í›„ ë¶„ì„ìš© ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

    except Exception as e:
        logger.error(f"ì‚¬ìš©ëŸ‰ ë¡œê¹… ì‹¤íŒ¨: {e}")
