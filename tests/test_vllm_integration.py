"""
vLLM ë©€í‹° LoRA ì„œë²„ í†µí•© í…ŒìŠ¤íŠ¸
- ì—°ê²° í…ŒìŠ¤íŠ¸
- ëª¨ë¸ë³„ ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸
- í˜ì¼ì˜¤ë²„ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import pytest
import asyncio
import json
from unittest.mock import AsyncMock, patch, MagicMock
from typing import AsyncGenerator
import logging

from app.services.vllm_integration_service import vllm_service, VLLMModelType
from app.services.enhanced_ai_model import enhanced_ai_service, AIBackendType
from app.schemas.code_generation import (
    CodeGenerationRequest, 
    CodeGenerationResponse,
    ModelType
)

# í…ŒìŠ¤íŠ¸ìš© ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestVLLMIntegrationService:
    """vLLM í†µí•© ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_health_check_success(self):
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        with patch('aiohttp.ClientSession.get') as mock_get:
            # Mock ì‘ë‹µ ì„¤ì •
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json.return_value = {
                "status": "healthy",
                "timestamp": 1751343490.577505
            }
            mock_get.return_value.__aenter__.return_value = mock_response
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = await vllm_service.check_health()
            
            # ê²€ì¦
            assert result["status"] == "healthy"
            assert "details" in result
            assert result["details"]["status"] == "healthy"
    
    @pytest.mark.asyncio
    async def test_health_check_failure(self):
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸"""
        with patch('aiohttp.ClientSession.get') as mock_get:
            # Mock ì‘ë‹µ ì„¤ì • (ì„œë²„ ì˜¤ë¥˜)
            mock_response = AsyncMock()
            mock_response.status = 500
            mock_get.return_value.__aenter__.return_value = mock_response
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = await vllm_service.check_health()
            
            # ê²€ì¦
            assert result["status"] == "unhealthy"
            assert result["http_status"] == 500
    
    @pytest.mark.asyncio
    async def test_get_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        with patch('aiohttp.ClientSession.get') as mock_get:
            # Mock ì‘ë‹µ ì„¤ì •
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json.return_value = {
                "service": "vLLM Multi-LoRA Server",
                "version": "1.0.0",
                "status": "running",
                "available_models": ["autocomplete", "prompt", "comment", "error_fix"]
            }
            mock_get.return_value.__aenter__.return_value = mock_response
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = await vllm_service.get_available_models()
            
            # ê²€ì¦
            assert "available_models" in result
            assert len(result["available_models"]) == 4
            assert "autocomplete" in result["available_models"]
            assert "prompt" in result["available_models"]
    
    def test_hapa_to_vllm_model_mapping(self):
        """HAPA ëª¨ë¸ íƒ€ì…ì„ vLLM ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            (ModelType.CODE_COMPLETION, VLLMModelType.AUTOCOMPLETE),
            (ModelType.CODE_GENERATION, VLLMModelType.PROMPT),
            (ModelType.BUG_FIX, VLLMModelType.ERROR_FIX),
            (ModelType.CODE_EXPLANATION, VLLMModelType.COMMENT),
        ]
        
        for hapa_model, expected_vllm_model in test_cases:
            result = vllm_service._map_hapa_to_vllm_model(hapa_model)
            assert result == expected_vllm_model
    
    def test_prepare_vllm_request(self):
        """vLLM ìš”ì²­ ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ìš”ì²­ ìƒì„±
        request = CodeGenerationRequest(
            prompt="íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜",
            model_type=ModelType.CODE_GENERATION,
            context="",
            temperature=0.3,
            max_tokens=1024
        )
        
        user_id = "test_user_123"
        
        # ìš”ì²­ ì¤€ë¹„
        vllm_request = vllm_service._prepare_vllm_request(request, user_id)
        
        # ê²€ì¦
        assert vllm_request["model_type"] == "prompt"
        assert isinstance(vllm_request["user_id"], int)
        assert vllm_request["temperature"] == 0.3
        assert vllm_request["max_tokens"] == 1024
        assert "user_select_options" in vllm_request
    
    def test_optimize_prompt_for_model(self):
        """ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        request = CodeGenerationRequest(
            prompt="Hello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜",
            context="ê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸"
        )
        
        # ê° ëª¨ë¸ íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸
        test_cases = [
            (VLLMModelType.AUTOCOMPLETE, "ê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸\nHello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜"),
            (VLLMModelType.COMMENT, "# ë‹¤ìŒ ì½”ë“œì— ëŒ€í•œ ìƒì„¸í•œ ì£¼ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\nê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸\n\n# ìš”ì²­ì‚¬í•­: Hello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜"),
            (VLLMModelType.ERROR_FIX, "# ë‹¤ìŒ ì½”ë“œì— ë²„ê·¸ê°€ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ ì°¾ì•„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\n# ë¬¸ì œ ì„¤ëª…: Hello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜\n\nê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸"),
            (VLLMModelType.PROMPT, "# ì»¨í…ìŠ¤íŠ¸:\nê¸°ì¡´ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸\n\n# ìš”ì²­ì‚¬í•­: Hello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜"),
        ]
        
        for model_type, expected_prefix in test_cases:
            result = vllm_service._optimize_prompt_for_model(
                request.prompt, model_type, request
            )
            assert result.startswith(expected_prefix.split('\n')[0])
    
    @pytest.mark.asyncio
    async def test_generate_code_sync_success(self):
        """ë™ê¸°ì‹ ì½”ë“œ ìƒì„± ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Mock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async def mock_stream_generator():
            yield "data: def hello_world():\n\n"
            yield "data:     print('Hello, World!')\n\n"
            yield "data: [DONE]\n\n"
        
        with patch.object(vllm_service, 'generate_code_stream', return_value=mock_stream_generator()):
            request = CodeGenerationRequest(
                prompt="Hello World í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤˜",
                model_type=ModelType.CODE_GENERATION
            )
            
            result = await vllm_service.generate_code_sync(request, "test_user")
            
            # ê²€ì¦
            assert result.success is True
            assert "hello_world" in result.generated_code
            assert result.model_used == "prompt"

class TestEnhancedAIModelService:
    """Enhanced AI ëª¨ë¸ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_initialization(self):
        """Enhanced AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        # Mock vLLM ì„œë¹„ìŠ¤
        with patch.object(enhanced_ai_service, '_check_vllm_health', return_value=True), \
             patch.object(enhanced_ai_service, '_check_legacy_health', return_value=True):
            
            await enhanced_ai_service.initialize()
            
            # ê²€ì¦
            assert enhanced_ai_service.vllm_available is True
            assert enhanced_ai_service.legacy_available is True
            assert enhanced_ai_service.current_backend == AIBackendType.VLLM
    
    @pytest.mark.asyncio
    async def test_backend_failover(self):
        """ë°±ì—”ë“œ í˜ì¼ì˜¤ë²„ í…ŒìŠ¤íŠ¸"""
        # vLLM ì‹¤íŒ¨, Legacy ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤
        enhanced_ai_service.vllm_available = False
        enhanced_ai_service.legacy_available = True
        
        request = CodeGenerationRequest(
            prompt="í…ŒìŠ¤íŠ¸ ì½”ë“œ",
            model_type=ModelType.CODE_GENERATION
        )
        
        with patch.object(enhanced_ai_service, '_generate_with_legacy') as mock_legacy:
            mock_legacy.return_value = CodeGenerationResponse(
                success=True,
                generated_code="def test(): pass",
                model_used="legacy_ai_model",
                processing_time=1.0,
                token_usage={"total_tokens": 10}
            )
            
            result = await enhanced_ai_service.generate_code(request, "test_user")
            
            # ê²€ì¦
            assert result.success is True
            assert result.model_used == "legacy_ai_model"
            mock_legacy.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_performance_stats_update(self):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
        # ì´ˆê¸° ìƒíƒœ
        initial_stats = enhanced_ai_service.performance_stats.copy()
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        enhanced_ai_service._update_performance_stats("vllm", 2.5, True)
        enhanced_ai_service._update_performance_stats("vllm", 1.5, True)
        
        # ê²€ì¦
        vllm_stats = enhanced_ai_service.performance_stats["vllm"]
        assert vllm_stats["requests"] == 2
        assert vllm_stats["successes"] == 2
        assert vllm_stats["avg_response_time"] > 0
    
    @pytest.mark.asyncio
    async def test_backend_switching(self):
        """ë°±ì—”ë“œ ìˆ˜ë™ ì „í™˜ í…ŒìŠ¤íŠ¸"""
        # vLLM ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœ ì„¤ì •
        enhanced_ai_service.vllm_available = True
        enhanced_ai_service.legacy_available = True
        
        # Legacyë¡œ ì „í™˜
        result = await enhanced_ai_service.switch_backend(AIBackendType.LEGACY)
        
        # ê²€ì¦
        assert result is True
        assert enhanced_ai_service.current_backend == AIBackendType.LEGACY
        
        # vLLMìœ¼ë¡œ ë‹¤ì‹œ ì „í™˜
        result = await enhanced_ai_service.switch_backend(AIBackendType.VLLM)
        
        # ê²€ì¦
        assert result is True
        assert enhanced_ai_service.current_backend == AIBackendType.VLLM

class TestAPIIntegration:
    """API í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_streaming_endpoint_mock(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ Mock í…ŒìŠ¤íŠ¸"""
        from app.api.endpoints.code_generation import generate_code_stream
        from fastapi import Request
        from unittest.mock import Mock
        
        # Mock ê°ì²´ ìƒì„±
        mock_request = CodeGenerationRequest(
            prompt="í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„±",
            model_type=ModelType.CODE_GENERATION
        )
        
        mock_background_tasks = Mock()
        mock_api_key = "test_api_key"
        mock_current_user = {"user_id": "test_user"}
        
        # Mock vLLM ì„œë¹„ìŠ¤
        async def mock_stream():
            yield "data: def test():\n\n"
            yield "data:     pass\n\n"
            yield "data: [DONE]\n\n"
        
        with patch('app.api.endpoints.code_generation.vllm_service') as mock_vllm:
            mock_vllm.check_health.return_value = {"status": "healthy"}
            mock_vllm.generate_code_stream.return_value = mock_stream()
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ì‹¤ì œ í˜¸ì¶œì€ FastAPI í…ŒìŠ¤íŠ¸ì—ì„œ)
            # ì—¬ê¸°ì„œëŠ” Mock ë™ì‘ í™•ì¸
            health_check = await mock_vllm.check_health()
            assert health_check["status"] == "healthy"
    
    def test_model_type_validation(self):
        """ëª¨ë¸ íƒ€ì… ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        # ìœ íš¨í•œ ëª¨ë¸ íƒ€ì…
        valid_request = CodeGenerationRequest(
            prompt="í…ŒìŠ¤íŠ¸",
            model_type=ModelType.CODE_GENERATION
        )
        assert valid_request.model_type == ModelType.CODE_GENERATION
        
        # í”„ë¡¬í”„íŠ¸ ê²€ì¦
        with pytest.raises(ValueError):
            CodeGenerationRequest(
                prompt="",  # ë¹ˆ í”„ë¡¬í”„íŠ¸
                model_type=ModelType.CODE_GENERATION
            )

class TestErrorHandling:
    """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_vllm_connection_error(self):
        """vLLM ì—°ê²° ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        with patch('aiohttp.ClientSession.get', side_effect=Exception("Connection failed")):
            result = await vllm_service.check_health()
            
            # ê²€ì¦
            assert result["status"] == "error"
            assert "Connection failed" in result["error"]
    
    @pytest.mark.asyncio
    async def test_malformed_response_handling(self):
        """ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        async def mock_malformed_stream():
            yield "invalid_data_format"
            yield "data: invalid_json_{"
            yield "data: [DONE]\n\n"
        
        with patch.object(vllm_service, 'generate_code_stream', return_value=mock_malformed_stream()):
            request = CodeGenerationRequest(
                prompt="í…ŒìŠ¤íŠ¸",
                model_type=ModelType.CODE_GENERATION
            )
            
            result = await vllm_service.generate_code_sync(request, "test_user")
            
            # ì˜¤ë¥˜ ìƒí™©ì—ì„œë„ ì‘ë‹µ ê°ì²´ê°€ ë°˜í™˜ë˜ì–´ì•¼ í•¨
            assert isinstance(result, CodeGenerationResponse)

@pytest.fixture
def event_loop():
    """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„"""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_integration_tests():
    """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸ§ª vLLM í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ì‹¤ì œ vLLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
        health_result = await vllm_service.check_health()
        print(f"âœ… vLLM ì„œë²„ ìƒíƒœ: {health_result['status']}")
        
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸
        models_result = await vllm_service.get_available_models()
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models_result.get('available_models', [])}")
        
        # Enhanced AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        await enhanced_ai_service.initialize()
        backend_status = await enhanced_ai_service.get_backend_status()
        print(f"âœ… í˜„ì¬ ë°±ì—”ë“œ: {backend_status['current_backend']}")
        
        # ê°„ë‹¨í•œ ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸
        request = CodeGenerationRequest(
            prompt="Hello World í•¨ìˆ˜",
            model_type=ModelType.CODE_GENERATION,
            max_tokens=100
        )
        
        response = await enhanced_ai_service.generate_code(request, "integration_test_user")
        print(f"âœ… ì½”ë“œ ìƒì„± ì„±ê³µ: {response.success}")
        
        print("ğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise
    
    finally:
        # ì •ë¦¬
        await enhanced_ai_service.close()
        await vllm_service.close()

if __name__ == "__main__":
    # í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(run_integration_tests()) 