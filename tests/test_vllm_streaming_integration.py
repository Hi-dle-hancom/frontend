"""
VLLM Streaming Integration Test
í…ŒìŠ¤íŠ¸ ëª©ì : VLLM ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì˜ ì™„ì „í•œ ê²€ì¦
- print("Jay") ì½”ë“œ ìƒì„± ìš”ì²­ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ê²€ì¦
- ì²­í¬ ì²˜ë¦¬ ì„±ëŠ¥ ê²€ì¦ (50-100 ì²­í¬, 3-5ì´ˆ ë‚´)
- ìµœì¢… ì¶œë ¥ ì •í™•ì„± ê²€ì¦
- ë°±ì—”ë“œì—ì„œ í”„ë¡ íŠ¸ì—”ë“œê¹Œì§€ ì™„ì „í•œ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° ê²€ì¦
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Optional
from unittest.mock import AsyncMock, patch, MagicMock
import pytest
import logging

from app.services.vllm_integration_service import vllm_service, VLLMModelType
from app.schemas.code_generation import (
    CodeGenerationRequest, 
    CodeGenerationResponse,
    ModelType
)
from app.api.endpoints.code_generation import generate_code_stream
from fastapi import BackgroundTasks, Request
from fastapi.responses import StreamingResponse

# í…ŒìŠ¤íŠ¸ìš© ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StreamingTestResult:
    """ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.chunks: List[Dict[str, Any]] = []
        self.total_chunks: int = 0
        self.processing_time: float = 0.0
        self.final_code: str = ""
        self.error_occurred: bool = False
        self.error_message: str = ""
        self.chunk_sizes: List[int] = []
        self.chunk_timestamps: List[float] = []
        self.chunks_per_second: float = 0.0
        self.average_chunk_size: float = 0.0
        self.streaming_complete: bool = False
        
    def add_chunk(self, chunk_data: Dict[str, Any], timestamp: float):
        """ì²­í¬ ë°ì´í„° ì¶”ê°€"""
        self.chunks.append(chunk_data)
        self.chunk_timestamps.append(timestamp)
        
        if 'text' in chunk_data:
            text = chunk_data['text']
            self.chunk_sizes.append(len(text))
            self.final_code += text
            
        self.total_chunks += 1
        
    def calculate_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if self.chunk_timestamps:
            self.processing_time = self.chunk_timestamps[-1] - self.chunk_timestamps[0]
            self.chunks_per_second = self.total_chunks / max(self.processing_time, 0.001)
            
        if self.chunk_sizes:
            self.average_chunk_size = sum(self.chunk_sizes) / len(self.chunk_sizes)
            
    def is_performance_acceptable(self) -> bool:
        """ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€ í™•ì¸"""
        return (
            50 <= self.total_chunks <= 100 and
            3.0 <= self.processing_time <= 5.0
        )
        
    def contains_expected_output(self) -> bool:
        """ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        return 'print("Jay")' in self.final_code or "print('Jay')" in self.final_code


class TestVLLMStreamingIntegration:
    """VLLM ìŠ¤íŠ¸ë¦¬ë° í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @pytest.fixture
    def test_request(self):
        """í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ìƒì„± ìš”ì²­"""
        return CodeGenerationRequest(
            prompt="íŒŒì´ì¬ìœ¼ë¡œ Jayë¼ëŠ” ë¬¸ìì—´ì„ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”",
            model_type=ModelType.CODE_GENERATION,
            context="",
            temperature=0.3,
            max_tokens=150
        )
    
    @pytest.fixture
    def mock_vllm_stream(self):
        """Mock VLLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        async def mock_stream():
            # ì‹œë®¬ë ˆì´ì…˜ëœ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (50-100 ì²­í¬)
            chunks = [
                {"text": "# Jayë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œ\n"},
                {"text": "def "},
                {"text": "print_jay"},
                {"text": "():\n"},
                {"text": "    "},
                {"text": "\"\"\"Jayë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n"},
                {"text": "    "},
                {"text": "print"},
                {"text": "("},
                {"text": "\"Jay\""},
                {"text": ")\n\n"},
                {"text": "# í•¨ìˆ˜ í˜¸ì¶œ\n"},
                {"text": "print_jay()"}
            ]
            
            # ì¶©ë¶„í•œ ì²­í¬ ìƒì„± (ëª©í‘œ: 50-100ê°œ)
            extended_chunks = []
            for i, chunk in enumerate(chunks):
                # ê° ì²­í¬ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
                text = chunk["text"]
                if len(text) > 3:
                    # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
                    for j in range(0, len(text), 2):
                        extended_chunks.append({"text": text[j:j+2]})
                else:
                    extended_chunks.append(chunk)
            
            # ëª©í‘œ ì²­í¬ ìˆ˜ì— ë§ê²Œ ì¡°ì •
            while len(extended_chunks) < 50:
                extended_chunks.append({"text": " "})
            
            # SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
            for i, chunk in enumerate(extended_chunks[:75]):  # 75ê°œ ì²­í¬ë¡œ ì œí•œ
                yield f"data: {json.dumps(chunk)}\n\n"
                await asyncio.sleep(0.05)  # 50ms ê°„ê²© (realistic streaming)
                
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
            yield f"data: [DONE]\n\n"
            
        return mock_stream
    
    @pytest.mark.asyncio
    async def test_vllm_streaming_basic_functionality(self, test_request, mock_vllm_stream):
        """ê¸°ë³¸ VLLM ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª ê¸°ë³¸ VLLM ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        result = StreamingTestResult()
        start_time = time.time()
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_vllm_stream()):
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                chunk_count = 0
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    current_time = time.time()
                    
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            result.streaming_complete = True
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            result.add_chunk(parsed_data, current_time)
                            chunk_count += 1
                            
                            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                            if chunk_count % 10 == 0:
                                logger.info(f"ì²˜ë¦¬ëœ ì²­í¬: {chunk_count}ê°œ")
                                
                        except json.JSONDecodeError:
                            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {data_content}")
                            
                result.calculate_stats()
                
                # ê²€ì¦
                assert result.streaming_complete, "ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ"
                assert result.total_chunks >= 50, f"ì²­í¬ ìˆ˜ ë¶€ì¡±: {result.total_chunks} < 50"
                assert result.total_chunks <= 100, f"ì²­í¬ ìˆ˜ ì´ˆê³¼: {result.total_chunks} > 100"
                assert 3.0 <= result.processing_time <= 5.0, f"ì²˜ë¦¬ ì‹œê°„ ë²”ìœ„ ì´ˆê³¼: {result.processing_time}"
                
                logger.info(f"âœ… ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ í†µê³¼: {result.total_chunks}ê°œ ì²­í¬, {result.processing_time:.2f}ì´ˆ")
                
        except Exception as e:
            result.error_occurred = True
            result.error_message = str(e)
            logger.error(f"âŒ ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_streaming_output_accuracy(self, test_request, mock_vllm_stream):
        """ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì •í™•ì„± í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì •í™•ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        result = StreamingTestResult()
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_vllm_stream()):
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            result.add_chunk(parsed_data, time.time())
                        except json.JSONDecodeError:
                            continue
                
                result.calculate_stats()
                
                # ì¶œë ¥ ì •í™•ì„± ê²€ì¦
                assert result.contains_expected_output(), f"ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ ì—†ìŒ: {result.final_code}"
                assert "print" in result.final_code.lower(), "print ë¬¸ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ"
                assert "jay" in result.final_code.lower(), "Jay ë¬¸ìì—´ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ"
                
                logger.info(f"âœ… ì¶œë ¥ ì •í™•ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
                logger.info(f"ìƒì„±ëœ ì½”ë“œ: {result.final_code}")
                
        except Exception as e:
            logger.error(f"âŒ ì¶œë ¥ ì •í™•ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_streaming_performance_metrics(self, test_request, mock_vllm_stream):
        """ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        logger.info("âš¡ ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        result = StreamingTestResult()
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_vllm_stream()):
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            result.add_chunk(parsed_data, time.time())
                        except json.JSONDecodeError:
                            continue
                
                result.calculate_stats()
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€ì¦
                assert result.is_performance_acceptable(), (
                    f"ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ì¶©ì¡±: {result.total_chunks}ê°œ ì²­í¬, "
                    f"{result.processing_time:.2f}ì´ˆ"
                )
                
                assert result.chunks_per_second >= 10, (
                    f"ì´ˆë‹¹ ì²­í¬ ì²˜ë¦¬ ì†ë„ ë¶€ì¡±: {result.chunks_per_second:.1f} < 10"
                )
                
                logger.info(f"âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ í†µê³¼")
                logger.info(f"ì„±ëŠ¥ ì§€í‘œ: {result.chunks_per_second:.1f} ì²­í¬/ì´ˆ, "
                           f"í‰ê·  ì²­í¬ í¬ê¸°: {result.average_chunk_size:.1f}ì")
                
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_api_endpoint_streaming(self, test_request):
        """API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸŒ API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # Mock dependencies
        mock_background_tasks = MagicMock(spec=BackgroundTasks)
        mock_api_key = "test_api_key"
        mock_current_user = {"user_id": "test_user"}
        
        # Mock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async def mock_stream():
            for i in range(60):  # 60ê°œ ì²­í¬
                yield f"data: {json.dumps({'text': f'chunk_{i} '})}\n\n"
                await asyncio.sleep(0.05)
            yield f"data: {json.dumps({'text': 'print(\"Jay\")'})}\n\n"
            yield f"data: [DONE]\n\n"
        
        try:
            with patch.object(vllm_service, 'check_health', return_value={"status": "healthy"}):
                with patch.object(vllm_service, 'generate_code_stream', return_value=mock_stream()):
                    
                    # API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
                    response = await generate_code_stream(
                        request=test_request,
                        background_tasks=mock_background_tasks,
                        api_key=mock_api_key,
                        current_user=mock_current_user
                    )
                    
                    # ì‘ë‹µ íƒ€ì… ê²€ì¦
                    assert isinstance(response, StreamingResponse), "StreamingResponseê°€ ì•„ë‹˜"
                    assert response.media_type == "text/event-stream", "ë¯¸ë””ì–´ íƒ€ì… ë¶ˆì¼ì¹˜"
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ë‚´ìš© ê²€ì¦
                    result = StreamingTestResult()
                    start_time = time.time()
                    
                    async for chunk in response.body_iterator:
                        chunk_str = chunk.decode('utf-8')
                        if chunk_str.startswith("data: "):
                            data_content = chunk_str[6:].strip()
                            
                            if data_content == "[DONE]":
                                result.streaming_complete = True
                                break
                                
                            try:
                                parsed_data = json.loads(data_content)
                                result.add_chunk(parsed_data, time.time())
                            except json.JSONDecodeError:
                                continue
                    
                    result.calculate_stats()
                    
                    # ê²€ì¦
                    assert result.streaming_complete, "API ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ"
                    assert result.total_chunks >= 50, f"API ì²­í¬ ìˆ˜ ë¶€ì¡±: {result.total_chunks}"
                    assert "print" in result.final_code, "API ì‘ë‹µì— print ë¬¸ ì—†ìŒ"
                    
                    logger.info(f"âœ… API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ í†µê³¼")
                    
        except Exception as e:
            logger.error(f"âŒ API ì—”ë“œí¬ì¸íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_chunk_buffering_performance(self, test_request):
        """ì²­í¬ ë²„í¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ—‚ï¸ ì²­í¬ ë²„í¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ë§¤ìš° ì‘ì€ ì²­í¬ë“¤ë¡œ ì‹œë®¬ë ˆì´ì…˜
        async def mock_small_chunks_stream():
            text = "def print_jay():\n    print(\"Jay\")\n\nprint_jay()"
            for char in text:
                yield f"data: {json.dumps({'text': char})}\n\n"
                await asyncio.sleep(0.01)
            yield f"data: [DONE]\n\n"
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_small_chunks_stream()):
                result = StreamingTestResult()
                
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            result.add_chunk(parsed_data, time.time())
                        except json.JSONDecodeError:
                            continue
                
                result.calculate_stats()
                
                # ë²„í¼ë§ íš¨ê³¼ ê²€ì¦
                # ì›ë³¸ ë¬¸ì ìˆ˜ë³´ë‹¤ ì ì€ ì²­í¬ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨ (ë²„í¼ë§ìœ¼ë¡œ ì¸í•´)
                original_char_count = len("def print_jay():\n    print(\"Jay\")\n\nprint_jay()")
                
                logger.info(f"ì›ë³¸ ë¬¸ì ìˆ˜: {original_char_count}, ì¶œë ¥ ì²­í¬ ìˆ˜: {result.total_chunks}")
                logger.info(f"ë²„í¼ë§ íš¨ìœ¨: {(1 - result.total_chunks/original_char_count)*100:.1f}%")
                
                # ìµœì¢… ì¶œë ¥ ì •í™•ì„± ê²€ì¦
                assert result.contains_expected_output(), "ë²„í¼ë§ í›„ ì¶œë ¥ ë¶ˆì •í™•"
                
                logger.info(f"âœ… ì²­í¬ ë²„í¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")
                
        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ë²„í¼ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_error_handling_during_streaming(self, test_request):
        """ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸš¨ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
        async def mock_error_stream():
            # ì •ìƒ ì²­í¬ ëª‡ ê°œ ì „ì†¡
            for i in range(10):
                yield f"data: {json.dumps({'text': f'chunk_{i} '})}\n\n"
                await asyncio.sleep(0.01)
            
            # ì˜¤ë¥˜ ë°œìƒ
            yield f"data: {json.dumps({'error': 'Mock streaming error'})}\n\n"
            yield f"data: [DONE]\n\n"
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_error_stream()):
                result = StreamingTestResult()
                
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            if 'error' in parsed_data:
                                result.error_occurred = True
                                result.error_message = parsed_data['error']
                            else:
                                result.add_chunk(parsed_data, time.time())
                        except json.JSONDecodeError:
                            continue
                
                # ì˜¤ë¥˜ ì²˜ë¦¬ ê²€ì¦
                assert result.error_occurred, "ì˜¤ë¥˜ê°€ ì œëŒ€ë¡œ ê°ì§€ë˜ì§€ ì•ŠìŒ"
                assert result.error_message == "Mock streaming error", "ì˜¤ë¥˜ ë©”ì‹œì§€ ë¶ˆì¼ì¹˜"
                assert result.total_chunks == 10, f"ì˜¤ë¥˜ ì „ ì²­í¬ ìˆ˜ ë¶ˆì¼ì¹˜: {result.total_chunks}"
                
                logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼")
                
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_end_to_end_streaming_flow(self, test_request):
        """ì¢…ë‹¨ê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ì¢…ë‹¨ê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì™€ ìœ ì‚¬í•œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
        async def mock_realistic_stream():
            # ì‹œì‘ ë©”íƒ€ë°ì´í„°
            yield f"data: {json.dumps({'type': 'start', 'model': 'prompt'})}\n\n"
            
            # ì½”ë“œ ìƒì„± ì²­í¬ë“¤
            code_parts = [
                "# Jayë¥¼ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ ì½”ë“œ\n",
                "def print_jay():\n",
                "    \"\"\"Jay ë¬¸ìì—´ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
                "    print(\"Jay\")\n",
                "\n",
                "# í•¨ìˆ˜ í˜¸ì¶œ\n",
                "print_jay()\n"
            ]
            
            for part in code_parts:
                # ê° ë¶€ë¶„ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
                words = part.split()
                for word in words:
                    yield f"data: {json.dumps({'text': word + ' '})}\n\n"
                    await asyncio.sleep(0.03)
                yield f"data: {json.dumps({'text': '\n'})}\n\n"
                await asyncio.sleep(0.05)
            
            # ì™„ë£Œ ë©”íƒ€ë°ì´í„°
            yield f"data: {json.dumps({'type': 'done', 'tokens': 45})}\n\n"
            yield f"data: [DONE]\n\n"
        
        try:
            with patch.object(vllm_service, 'generate_code_stream', return_value=mock_realistic_stream()):
                result = StreamingTestResult()
                metadata = {}
                
                async for chunk in vllm_service.generate_code_stream(test_request, "test_user"):
                    if chunk.startswith("data: "):
                        data_content = chunk[6:].strip()
                        
                        if data_content == "[DONE]":
                            result.streaming_complete = True
                            break
                            
                        try:
                            parsed_data = json.loads(data_content)
                            
                            if 'type' in parsed_data:
                                metadata[parsed_data['type']] = parsed_data
                            else:
                                result.add_chunk(parsed_data, time.time())
                                
                        except json.JSONDecodeError:
                            continue
                
                result.calculate_stats()
                
                # ì¢…ë‹¨ê°„ í”Œë¡œìš° ê²€ì¦
                assert result.streaming_complete, "ì¢…ë‹¨ê°„ ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ"
                assert result.total_chunks >= 20, f"ì¢…ë‹¨ê°„ ì²­í¬ ìˆ˜ ë¶€ì¡±: {result.total_chunks}"
                assert result.contains_expected_output(), "ì¢…ë‹¨ê°„ ì¶œë ¥ ë¶ˆì •í™•"
                assert 'start' in metadata, "ì‹œì‘ ë©”íƒ€ë°ì´í„° ì—†ìŒ"
                assert 'done' in metadata, "ì™„ë£Œ ë©”íƒ€ë°ì´í„° ì—†ìŒ"
                
                logger.info(f"âœ… ì¢…ë‹¨ê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼")
                logger.info(f"ìµœì¢… ìƒì„± ì½”ë“œ:\n{result.final_code}")
                
        except Exception as e:
            logger.error(f"âŒ ì¢…ë‹¨ê°„ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise


# ì‹¤í–‰ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
async def run_streaming_tests():
    """ëª¨ë“  ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logger.info("ğŸš€ VLLM ìŠ¤íŠ¸ë¦¬ë° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    test_instance = TestVLLMStreamingIntegration()
    test_request = CodeGenerationRequest(
        prompt="íŒŒì´ì¬ìœ¼ë¡œ Jayë¼ëŠ” ë¬¸ìì—´ì„ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”",
        model_type=ModelType.CODE_GENERATION,
        context="",
        temperature=0.3,
        max_tokens=150
    )
    
    # Mock ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    async def mock_stream():
        chunks = [
            {"text": "def print_jay():\n"},
            {"text": "    print(\"Jay\")\n"},
            {"text": "\nprint_jay()"}
        ]
        
        # ë” ë§ì€ ì²­í¬ ìƒì„±
        extended_chunks = []
        for chunk in chunks:
            text = chunk["text"]
            for i in range(0, len(text), 2):
                extended_chunks.append({"text": text[i:i+2]})
        
        # ëª©í‘œ ì²­í¬ ìˆ˜ì— ë§ê²Œ íŒ¨ë”©
        while len(extended_chunks) < 60:
            extended_chunks.append({"text": " "})
        
        for chunk in extended_chunks:
            yield f"data: {json.dumps(chunk)}\n\n"
            await asyncio.sleep(0.05)
        
        yield f"data: [DONE]\n\n"
    
    try:
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await test_instance.test_vllm_streaming_basic_functionality(test_request, mock_stream)
        await test_instance.test_streaming_output_accuracy(test_request, mock_stream)
        await test_instance.test_streaming_performance_metrics(test_request, mock_stream)
        await test_instance.test_chunk_buffering_performance(test_request)
        await test_instance.test_error_handling_during_streaming(test_request)
        await test_instance.test_end_to_end_streaming_flow(test_request)
        
        logger.info("ğŸ‰ ëª¨ë“  VLLM ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(run_streaming_tests())