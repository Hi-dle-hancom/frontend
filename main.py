"""
HAPA (Hancom AI Python Assistant) Backend
ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
- vLLM ë©€í‹° LoRA ì„œë²„ í†µí•©
- Enhanced AI ëª¨ë¸ ì„œë¹„ìŠ¤ ì§€ì›
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„±
"""

import time
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse

# Core imports
from app.core.config import settings
from app.core.logging_config import setup_logging
from app.core.structured_logger import StructuredLogger
from app.core.settings_manager import get_settings

# API imports
from app.api.api import api_router

# Service imports
from app.services.enhanced_ai_model import enhanced_ai_service
from app.services.vllm_integration_service import vllm_service
from app.middleware.enhanced_logging_middleware import EnhancedLoggingMiddleware
from app.middleware.security_headers import add_security_middleware

# Exception handlers
from app.api.endpoints.error_monitoring import setup_error_handlers

logger = StructuredLogger("main")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬
    - ì‹œì‘ì‹œ: Enhanced AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    - ì¢…ë£Œì‹œ: ì—°ê²° ì •ë¦¬
    """
    # === ì‹œì‘ ë‹¨ê³„ ===
    logger.log_system_event("HAPA ë°±ì—”ë“œ ì‹œì‘", "started", {
        "environment": settings.ENVIRONMENT,
        "vllm_server": settings.VLLM_SERVER_URL,
        "debug_mode": settings.DEBUG
    })

    try:
        # Enhanced AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.log_system_event("Enhanced AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”", "started")
        await enhanced_ai_service.initialize()

        # vLLM ì„œë²„ ì—°ê²° í™•ì¸
        health_status = await vllm_service.check_health()
        if health_status["status"] == "healthy":
            logger.log_system_event("vLLM ì„œë²„ ì—°ê²°", "success", health_status)
        else:
            logger.log_system_event("vLLM ì„œë²„ ì—°ê²°", "failed", health_status)

        # ë°±ì—”ë“œ ìƒíƒœ ì¡°íšŒ
        backend_status = await enhanced_ai_service.get_backend_status()
        logger.log_system_event("AI ë°±ì—”ë“œ ìƒíƒœ", "success", backend_status)

        logger.log_system_event("HAPA ë°±ì—”ë“œ ì´ˆê¸°í™”", "completed", {
            "vllm_available": backend_status["vllm"]["available"],
            "backend_type": backend_status["backend_type"],
            "last_health_check": backend_status["last_health_check"]
        })

    except Exception as e:
        logger.log_error(e, "HAPA ë°±ì—”ë“œ ì´ˆê¸°í™”")
        # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ì„œë²„ëŠ” ì‹œì‘ (graceful degradation)

    yield

    # === ì¢…ë£Œ ë‹¨ê³„ ===
    logger.log_system_event("HAPA ë°±ì—”ë“œ ì¢…ë£Œ", "started")

    try:
        # Enhanced AI ì„œë¹„ìŠ¤ ì •ë¦¬
        await enhanced_ai_service.close()

        # vLLM ì„œë¹„ìŠ¤ ì •ë¦¬
        await vllm_service.close()

        logger.log_system_event("HAPA ë°±ì—”ë“œ ì¢…ë£Œ", "completed")

    except Exception as e:
        logger.log_error(e, "HAPA ë°±ì—”ë“œ ì¢…ë£Œ")


def create_application() -> FastAPI:
    """
    FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± ë° ì„¤ì •
    """
    # ë¡œê¹… ì„¤ì •
    setup_logging()

    # FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    app = FastAPI(
        title=settings.PROJECT_NAME,
        version="1.0.0",
        description="""
        ğŸš€ **HAPA (Hancom AI Python Assistant) API**

        **ìƒˆë¡œìš´ ê¸°ëŠ¥:**
        - ğŸ¤– vLLM ë©€í‹° LoRA ì„œë²„ í†µí•©
        - ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„±
        - ğŸŒ í•œêµ­ì–´/ì˜ì–´ ìë™ ë²ˆì—­
        - ğŸ”„ ë“€ì–¼ ë°±ì—”ë“œ (vLLM + Legacy AI)
        - ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

        **ì§€ì› ëª¨ë¸:**
        - `autocomplete`: ì½”ë“œ ìë™ì™„ì„±
        - `prompt`: ì¼ë°˜ ì½”ë“œ ìƒì„±
        - `comment`: ì£¼ì„/ë¬¸ì„œ ìƒì„±
        - `error_fix`: ë²„ê·¸ ìˆ˜ì •
        """,
        openapi_url=(
            f"{settings.API_V1_PREFIX}/openapi.json"
            if settings.DEBUG else None
        ),
        docs_url="/docs" if settings.DEBUG else None,
        redoc_url="/redoc" if settings.DEBUG else None,
        lifespan=lifespan
    )

    # ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    hapa_settings = get_settings()
    security_config = {
        "environment": hapa_settings.environment,
        "rate_limit_requests": hapa_settings.security.rate_limit_requests,
        "enable_csp": True,
        "enable_hsts": hapa_settings.security.ssl_enabled
    }
    add_security_middleware(app, security_config)

    # CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì • (ë³´ì•ˆ ê°•í™”)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=hapa_settings.security.allowed_origins,
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        allow_headers=["Authorization", "Content-Type", "X-API-Key"],
        expose_headers=["X-Process-Time", "X-Rate-Limit-Remaining"]
    )

    # Enhanced ë¡œê¹… ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
    app.add_middleware(EnhancedLoggingMiddleware)

    # Trusted Host ë¯¸ë“¤ì›¨ì–´ (ìš´ì˜í™˜ê²½)
    if not hapa_settings.debug:
        app.add_middleware(
            TrustedHostMiddleware,
            allowed_hosts=hapa_settings.security.allowed_hosts
        )

    # API ë¼ìš°í„° í¬í•¨
    app.include_router(api_router, prefix=settings.API_V1_PREFIX)

    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì„¤ì •
    setup_error_handlers(app)

    # ë£¨íŠ¸ ë ˆë²¨ health ì²´í¬ (ìµœìš°ì„  ë“±ë¡)
    @app.get("/health", tags=["Health"])
    async def health_check():
        """ê°„ë‹¨í•œ í—¬ìŠ¤ ì²´í¬ - ì¸ì¦ ë¶ˆí•„ìš”"""
        return {
            "status": "healthy",
            "timestamp": time.time(),
            "service": "HAPA Backend API",
            "version": "1.0.0"
        }

    # ìƒì„¸ health ì²´í¬
    @app.get("/health/detailed", tags=["Health"])
    async def detailed_health_check():
        """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
        try:
            # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
            vllm_health = await vllm_service.check_health()

            # Enhanced AI ì„œë¹„ìŠ¤ ìƒíƒœ
            backend_status = await enhanced_ai_service.get_backend_status()

            return {
                "status": "healthy",
                "timestamp": time.time(),
                "service": "HAPA Backend API",
                "components": {
                    "vllm_server": {
                        "status": "healthy" if vllm_health["status"] == "healthy" else "degraded",
                        "details": vllm_health
                    },
                    "ai_backend": {
                        "status": "healthy",
                        "current_backend": backend_status["backend_type"],
                        "vllm_available": backend_status["vllm"]["available"]
                    }
                }
            }
        except Exception as e:
            logger.log_error(e, "ìƒì„¸ í—¬ìŠ¤ ì²´í¬")
            return {
                "status": "degraded",
                "timestamp": time.time(),
                "error": str(e)
            }

    # ê¸€ë¡œë²Œ ë¯¸ë“¤ì›¨ì–´ - ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    @app.middleware("http")
    async def add_process_time_header(request: Request, call_next):
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
        response.headers["X-Process-Time"] = str(process_time)
        return response

    # ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - vLLM í†µí•© ìƒíƒœ í‘œì‹œ
    @app.get("/", tags=["Root"])
    async def root():
        """
        HAPA ë°±ì—”ë“œ ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
        vLLM í†µí•© ìƒíƒœ ë° ì„œë¹„ìŠ¤ ì •ë³´ ì œê³µ
        """
        try:
            # ë°±ì—”ë“œ ìƒíƒœ ì¡°íšŒ
            backend_status = await enhanced_ai_service.get_backend_status()

            return {
                "service": "HAPA (Hancom AI Python Assistant)",
                "version": "1.0.0",
                "status": "running",
                "timestamp": time.time(),
                "environment": settings.ENVIRONMENT,
                "ai_backends": {
                    "backend_type": backend_status["backend_type"],
                    "vllm": {
                        "available": backend_status["vllm"]["available"],
                        "server_url": settings.VLLM_SERVER_URL
                    }
                },
                "features": [
                    "ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ ìƒì„±",
                    "í•œêµ­ì–´/ì˜ì–´ ìë™ ë²ˆì—­",
                    "vLLM ë©€í‹° LoRA ëª¨ë¸ ì§€ì›",
                    "ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ",
                    "ê³ ì„±ëŠ¥ AI ì¶”ë¡ "
                ],
                "endpoints": {
                    "docs": "/docs",
                    "health": "/api/v1/code/health",
                    "streaming": "/api/v1/code/generate/stream",
                    "sync": "/api/v1/code/generate"
                }
            }

        except Exception as e:
            logger.log_error(e, "ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸")
            return {
                "service": "HAPA (Hancom AI Python Assistant)",
                "status": "degraded",
                "error": "ë°±ì—”ë“œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨"
            }

    # vLLM í†µí•© ìƒíƒœ ì—”ë“œí¬ì¸íŠ¸
    @app.get("/vllm/status", tags=["vLLM Integration"])
    async def vllm_status():
        """
        vLLM ë©€í‹° LoRA ì„œë²„ í†µí•© ìƒíƒœ ìƒì„¸ ì¡°íšŒ
        """
        try:
            # vLLM ì„œë²„ ìƒíƒœ
            health_status = await vllm_service.check_health()

            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
            models_info = await vllm_service.get_available_models()

            # ë°±ì—”ë“œ ìƒíƒœ
            backend_status = await enhanced_ai_service.get_backend_status()

            return {
                "vllm_integration": {
                    "server_health": health_status,
                    "available_models": models_info.get("available_models", []),
                    "server_details": models_info,
                    "backend_status": backend_status["vllm"],
                    "configuration": {
                        "server_url": settings.VLLM_SERVER_URL,
                        "timeout": settings.VLLM_TIMEOUT_SECONDS,
                        "max_retries": settings.VLLM_MAX_RETRIES,
                        "connection_pool_size": settings.VLLM_CONNECTION_POOL_SIZE
                    }
                },
                "timestamp": time.time()
            }

        except Exception as e:
            logger.log_error(e, "vLLM ìƒíƒœ ì¡°íšŒ")
            return JSONResponse(
                status_code=500,
                content={"error": "vLLM ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨", "details": str(e)}
            )

    return app


app = create_application()

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower(),
    ) 