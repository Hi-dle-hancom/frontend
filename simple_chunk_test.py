#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ChunkBuffer í…ŒìŠ¤íŠ¸
- VLLMIntegrationService ì˜ì¡´ì„± ì—†ì´ ì§ì ‘ í…ŒìŠ¤íŠ¸
"""

import re
import time
from typing import Optional, Dict, Any, List

# ëª¨ì˜ ì„¤ì •
class MockSettings:
    @staticmethod
    def should_log_performance():
        return True
    
    @staticmethod 
    def should_log_debug():
        return True

# ChunkBuffer í´ë˜ìŠ¤ ì§ì ‘ ë³µì‚¬ (ì˜ì¡´ì„± ì—†ì´ í…ŒìŠ¤íŠ¸)
class ChunkBuffer:
    """ì²­í¬ ë²„í¼ë§ í´ë˜ìŠ¤ - ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ì²­í¬ ê·¸ë£¹í™” ë° í›„ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self, buffer_size: int = 300, buffer_timeout: float = 1.0):
        # ì„±ëŠ¥ ìµœì í™”ëœ ì„¤ì • (ë” ì—„ê²©í•œ ê¸°ì¤€)
        self.buffer_size = buffer_size  # ìµœëŒ€ ë²„í¼ í¬ê¸° (200 â†’ 300ìë¡œ ì¦ê°€)
        self.buffer_timeout = buffer_timeout  # ë²„í¼ íƒ€ì„ì•„ì›ƒ (0.5 â†’ 1.0ì´ˆë¡œ ì¦ê°€)
        self.min_chunk_size = 80  # ìµœì†Œ ì²­í¬ í¬ê¸° (50 â†’ 80ìë¡œ ì¦ê°€)
        self.max_chunk_size = 800  # ìµœëŒ€ ì²­í¬ í¬ê¸° (500 â†’ 800ìë¡œ ì¦ê°€)
        self.optimal_chunk_size = 150  # ìµœì  ì²­í¬ í¬ê¸° (ìƒˆë¡œ ì¶”ê°€)
        self.buffer = ""
        self.last_flush_time = time.time()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ë“¤
        self.total_chunks_processed = 0
        self.total_bytes_processed = 0
        self.small_chunks_count = 0  # 80ì ë¯¸ë§Œ ì²­í¬ ê°œìˆ˜
        self.large_chunks_count = 0  # 800ì ì´ˆê³¼ ì²­í¬ ê°œìˆ˜
        self.optimal_chunks_count = 0  # 80-300ì ì²­í¬ ê°œìˆ˜
        
        # ì²­í¬ í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì„¤ì •
        self.force_meaningful_boundaries = True  # ì˜ë¯¸ ìˆëŠ” ê²½ê³„ì—ì„œë§Œ í”ŒëŸ¬ì‹œ
        self.strict_size_enforcement = True  # ì—„ê²©í•œ í¬ê¸° ê²€ì¦
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ êµ¬ë¶„ì íŒ¨í„´
        self.meaningful_delimiters = [
            r'def\s+\w+\([^)]*\):\s*\n',      # í•¨ìˆ˜ ì •ì˜
            r'class\s+\w+[^:]*:\s*\n',        # í´ë˜ìŠ¤ ì •ì˜
            r'[.!?]\s+[A-Z]',                 # ë¬¸ì¥ ê²½ê³„
            r';\s*\n',                        # ì„¸ë¯¸ì½œë¡  í›„ ì¤„ë°”ê¿ˆ
            r'}\s*\n',                        # ì¤‘ê´„í˜¸ ë‹«í˜ í›„ ì¤„ë°”ê¿ˆ
            r'\n\s*\n',                       # ë¹ˆ ì¤„
        ]
        
        # AI ëª¨ë¸ íŠ¹ìˆ˜ í† í° íŒ¨í„´
        self.special_token_patterns = [
            r'<\|im_end\|>.*$',
            r'<\|im_start\|>[^|]*\|>',
            r'<\|assistant\|>',
            r'<\|user\|>',
        ]

    def add_chunk(self, chunk: str) -> Optional[str]:
        """ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€í•˜ê³  í•„ìš”ì‹œ í”ŒëŸ¬ì‹œ"""
        
        # ì¢…ë£Œ í† í° ì²´í¬
        if self._contains_end_token(chunk):
            print(f"ğŸ›‘ ì¢…ë£Œ í† í° ê°ì§€: '{chunk[:20]}...'")
            clean_chunk = self._extract_content_before_end_token(chunk)
            if clean_chunk:
                self.buffer += clean_chunk
            final_content = self.flush()
            return final_content if final_content.strip() else "[END_OF_GENERATION]"
        
        # íŠ¹ìˆ˜ í† í° ì œê±°
        cleaned_chunk = self._clean_special_tokens(chunk)
        
        if not cleaned_chunk.strip():
            return None
            
        self.buffer += cleaned_chunk
        current_time = time.time()
        
        # ì—„ê²©í•œ í”ŒëŸ¬ì‹œ ì¡°ê±´
        if self.strict_size_enforcement:
            if len(self.buffer) < self.min_chunk_size:
                if len(self.buffer) >= self.max_chunk_size or self._contains_end_token(self.buffer):
                    should_flush = True
                else:
                    should_flush = False
            else:
                should_flush = (
                    (len(self.buffer) >= self.optimal_chunk_size and 
                     self._has_meaningful_boundary()) or
                    self._has_complete_code_element() or
                    len(self.buffer) >= self.buffer_size * 2.0 or
                    len(self.buffer) >= self.max_chunk_size or
                    (current_time - self.last_flush_time >= self.buffer_timeout * 2.0 and
                     len(self.buffer) >= self.min_chunk_size * 1.5 and
                     self._has_meaningful_boundary())
                )
        else:
            should_flush = (
                len(self.buffer) >= self.min_chunk_size and (
                    len(self.buffer) >= self.buffer_size * 1.8 or
                    current_time - self.last_flush_time >= self.buffer_timeout or
                    self._has_complete_code_element() or
                    len(self.buffer) >= self.max_chunk_size
                )
            )
        
        # ì²­í¬ í’ˆì§ˆ ë¶„ë¥˜ ë° í”ŒëŸ¬ì‹œ
        if should_flush:
            buffer_length = len(self.buffer)
            
            if buffer_length < self.min_chunk_size:
                self.small_chunks_count += 1
                print(f"âš ï¸ ì‘ì€ ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì (ë¹„ì •ìƒ)")
            elif buffer_length <= self.buffer_size:
                self.optimal_chunks_count += 1
                print(f"âœ… ìµœì  ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            else:
                self.large_chunks_count += 1
                print(f"ğŸ“¦ ëŒ€í˜• ì²­í¬ í”ŒëŸ¬ì‹œ: {buffer_length}ì")
            
            result = self.flush()
            print(f"ğŸ“¤ í”ŒëŸ¬ì‹œ ì™„ë£Œ: {buffer_length}ì â†’ {len(result)}ì")
            return result
        else:
            print(f"ğŸ”„ ë²„í¼ë§ ì¤‘: {len(self.buffer)}/{self.buffer_size}ì")
        
        return None

    def flush(self) -> str:
        """ë²„í¼ ë‚´ìš©ì„ í”ŒëŸ¬ì‹œí•˜ê³  í›„ì²˜ë¦¬"""
        content = self.buffer
        self.buffer = ""
        self.last_flush_time = time.time()
        
        self.total_chunks_processed += 1
        self.total_bytes_processed += len(content)
        
        if content:
            content = self._clean_special_tokens(content)
            content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
            content = re.sub(r'[ \t]+', ' ', content)
            content = re.sub(r'[ \t]*\n[ \t]*', '\n', content)
        
        return content.strip()
    
    def force_flush(self) -> Optional[str]:
        """ê°•ì œ í”ŒëŸ¬ì‹œ"""
        if self.buffer:
            content = self.flush()
            content = self._clean_special_tokens(content)
            return content if content.strip() else None
        return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        total_chunks = max(self.total_chunks_processed, 1)
        avg_chunk_size = (self.total_bytes_processed / total_chunks)
        
        small_ratio = round(self.small_chunks_count / total_chunks * 100, 2)
        optimal_ratio = round(self.optimal_chunks_count / total_chunks * 100, 2)
        large_ratio = round(self.large_chunks_count / total_chunks * 100, 2)
        
        if small_ratio <= 5 and optimal_ratio >= 70:
            performance_grade = "A"
        elif small_ratio <= 15 and optimal_ratio >= 50:
            performance_grade = "B"
        elif small_ratio <= 30:
            performance_grade = "C"
        else:
            performance_grade = "D"
        
        return {
            "total_chunks": self.total_chunks_processed,
            "total_bytes": self.total_bytes_processed,
            "avg_chunk_size": round(avg_chunk_size, 2),
            "small_chunks_count": self.small_chunks_count,
            "optimal_chunks_count": self.optimal_chunks_count,
            "large_chunks_count": self.large_chunks_count,
            "small_chunks_ratio": small_ratio,
            "optimal_chunks_ratio": optimal_ratio,
            "large_chunks_ratio": large_ratio,
            "performance_grade": performance_grade,
            "buffer_efficiency": round(optimal_ratio + (large_ratio * 0.7), 2),
            "min_chunk_size": self.min_chunk_size,
            "optimal_chunk_size": self.optimal_chunk_size,
            "max_chunk_size": self.max_chunk_size,
        }
    
    def _clean_special_tokens(self, text: str) -> str:
        """íŠ¹ìˆ˜ í† í° ì œê±°"""
        cleaned_text = text
        for pattern in self.special_token_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        return cleaned_text
    
    def _has_complete_code_element(self) -> bool:
        """ì™„ì „í•œ ì½”ë“œ ìš”ì†Œ í™•ì¸"""
        return any(re.search(pattern, self.buffer) for pattern in self.meaningful_delimiters[:3])
    
    def _has_meaningful_boundary(self) -> bool:
        """ì˜ë¯¸ ìˆëŠ” ê²½ê³„ í™•ì¸"""
        return any(re.search(pattern, self.buffer) for pattern in self.meaningful_delimiters)
    
    def _contains_end_token(self, text: str) -> bool:
        """ì¢…ë£Œ í† í° í™•ì¸"""
        return '<|im_end|>' in text or '[DONE]' in text
    
    def _extract_content_before_end_token(self, text: str) -> str:
        """ì¢…ë£Œ í† í° ì´ì „ ë‚´ìš© ì¶”ì¶œ"""
        for token in ['<|im_end|>', '[DONE]']:
            if token in text:
                return text.split(token)[0]
        return text

def test_performance_improvement():
    """ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ChunkBuffer ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ì‹œë®¬ë ˆì´ì…˜ í† í°ë“¤ (1000+ ê°œ ì‘ì€ í† í°)
    tokens = []
    for i in range(100):
        tokens.extend([
            "def", " ", "func", str(i), "(", ")", ":", "\n",
            "    ", "print", "(", "\"hello\"", ")", "\n",
            "    ", "return", " ", str(i), "\n\n"
        ])
    
    print(f"ì…ë ¥ í† í° ìˆ˜: {len(tokens)}")
    print(f"í‰ê·  í† í° í¬ê¸°: {sum(len(t) for t in tokens) / len(tokens):.1f}ì")
    
    # ê¸°ì¡´ ì„¤ì • í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ ê¸°ì¡´ ì„¤ì • (ì™„í™” ëª¨ë“œ)")
    old_buffer = ChunkBuffer(buffer_size=200, buffer_timeout=0.5)
    old_buffer.strict_size_enforcement = False
    
    old_chunks = []
    start_time = time.time()
    
    for token in tokens:
        result = old_buffer.add_chunk(token)
        if result:
            old_chunks.append(result)
    
    final_old = old_buffer.force_flush()
    if final_old:
        old_chunks.append(final_old)
    
    old_duration = time.time() - start_time
    old_stats = old_buffer.get_performance_stats()
    
    print(f"ğŸ“Š ê¸°ì¡´ ê²°ê³¼: {len(old_chunks)}ê°œ ì²­í¬, í‰ê·  {old_stats['avg_chunk_size']:.1f}ì")
    
    # ìƒˆë¡œìš´ ì—„ê²©í•œ ì„¤ì •
    print("\n2ï¸âƒ£ ìƒˆë¡œìš´ ì—„ê²©í•œ ì„¤ì •")
    new_buffer = ChunkBuffer(buffer_size=300, buffer_timeout=1.0)
    new_buffer.strict_size_enforcement = True
    
    new_chunks = []
    start_time = time.time()
    
    for token in tokens:
        result = new_buffer.add_chunk(token)
        if result:
            new_chunks.append(result)
    
    final_new = new_buffer.force_flush()
    if final_new:
        new_chunks.append(final_new)
    
    new_duration = time.time() - start_time
    new_stats = new_buffer.get_performance_stats()
    
    print(f"ğŸ“Š ìƒˆë¡œìš´ ê²°ê³¼: {len(new_chunks)}ê°œ ì²­í¬, í‰ê·  {new_stats['avg_chunk_size']:.1f}ì")
    
    # ì„±ëŠ¥ ê°œì„  ë¶„ì„
    reduction = ((len(old_chunks) - len(new_chunks)) / len(old_chunks)) * 100
    size_improvement = new_stats['avg_chunk_size'] / old_stats['avg_chunk_size']
    
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ :")
    print(f"   ì²­í¬ ìˆ˜ ê°ì†Œ: {reduction:.1f}% ({len(old_chunks)} â†’ {len(new_chunks)})")
    print(f"   í‰ê·  í¬ê¸° ì¦ê°€: {size_improvement:.1f}ë°°")
    print(f"   ì„±ëŠ¥ ë“±ê¸‰: {new_stats['performance_grade']}")
    print(f"   ìµœì  ì²­í¬ ë¹„ìœ¨: {new_stats['optimal_chunks_ratio']:.1f}%")
    
    if reduction >= 70:
        print("ğŸ† ëª©í‘œ ë‹¬ì„±: 70% ì´ìƒ ì²­í¬ ê°ì†Œ!")
    else:
        print("âš ï¸ ëª©í‘œ ë¯¸ë‹¬: ì¶”ê°€ ìµœì í™” í•„ìš”")

if __name__ == "__main__":
    test_performance_improvement() 